{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis:\n",
    "## _Data Mining for Meaningful Concepts In Christianity Newsgroups_\n",
    "---\n",
    "\n",
    "Prepared By: Jason Schenck  \n",
    "Date: February 6th 2017  \n",
    "CSC-570 Data Science Essentials\n",
    "\n",
    "\n",
    "<br>\n",
    "<big>Table Of Contents</big>\n",
    "\n",
    "---\n",
    "* **[1 Introduction][Introduction]**\n",
    "   * [1.1][1.1] _Purpose & Data Source_\n",
    "   * [1.2][1.2] _What is a \"Latent Semantic Analysis\"?_\n",
    "   * [1.3][1.3] _Terminology Defined_\n",
    "   * [1.4][1.4] _Process/Procedure & Methodology_\n",
    "\n",
    "\n",
    "* **[2 Data Preparation][Data Preparation]**\n",
    "   * [2.1][2.1] _Data Retrieval_\n",
    "   * [2.2][2.2] _Data Inspection_\n",
    "   * [2.3][2.3] _Defining 'stopwords'_\n",
    "\n",
    "\n",
    "* **[3 Latent Semantic Analysis (LSA)][Latent Semantic Analysis (LSA)]**\n",
    "   * [3.1][3.1] _TF-IDF Vectorization_\n",
    "   * [3.2][3.2] _SVD Modeling with Scikit-Learn_\n",
    "\n",
    "\n",
    "* **[4 Results: Interpration Of Extracted Concepts][Results: Interpration Of Extracted Concepts]**\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "[Introduction]: #1-Introduction\n",
    "[1.1]: #1.1-Purpose-&-Data-Source\n",
    "[1.2]: #1.2-What-is-a-\"Latent-Semantic-Analysis\"?\n",
    "[1.3]: #1.3-Terminology-Defined\n",
    "[1.4]: #1.4-Process/Procedure-&-Methodology\n",
    "[Data Preparation]: #2-Data-Preparation\n",
    "[2.1]: #2.1-Data-Retrieval\n",
    "[2.2]: #2.2-Data-Inspection\n",
    "[2.3]: #2.3-Defining-'stopwords'\n",
    "[Latent Semantic Analysis (LSA)]: #3-Latent-Semantic-Analysis-(LSA)\n",
    "[3.1]: #3.1-TF-IDF-Vectorization\n",
    "[3.2]: #3.2-SVD-Modeling-with-Scikit-Learn\n",
    "[Results: Interpration Of Extracted Concepts]: #4-Results:-Interpration-Of-Extracted-Concepts\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Data Source: </b><a href=\"http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#\">\"Twenty Newsgroups\", Provided By: Scikit-Learn </a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Purpose & Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis I will be performing data mining in an effort to extract a series of meaningful and significant concepts from a public dataset of newsgroup postings on the topic of Christianity.\n",
    "\n",
    "The dataset, titled \"Twenty Newsgroups\" and is officially described as follows:\n",
    ">\"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\"\n",
    "\n",
    "A newsgroup is an online public forum for discussion on a particular topic. The topic that I will be extracting data from will be \"Christianity\" (_soc.religion.christian_). I'm very curious to see what the results of this analysis will be, and in concluding intend to share my opinion on them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 What is a \"Latent Semantic Analysis\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Latent Semantic Analysis (LSA)_ is a technique commonly used in the field of Natural Language Processing (NLP). As a computer scientist, when performing NLP we are concerned with the interactions that that exist between computers and human language. A great portion of this field focuses on the analysis of the relationship between multiple words in a document of text contained in a collection of related documents. This is known as the subfield of _Natural Language Understanding_ and can be thought of more simply as \"teaching computers how to read\". \n",
    "\n",
    "LSA is more formally defined by [_\"An Introduction to Latent Semantic Analysis\" by Landauer, Foltz, & Laham_](http://lsa.colorado.edu/papers/dp1.LSAintro.pdf)\n",
    ">\"Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the\n",
    "contextual-usage meaning of words by statistical computations applied to a large corpus of\n",
    "text (Landauer and Dumais, 1997). The underlying idea is that the aggregate of all the word\n",
    "contexts in which a given word does and does not appear provides a set of mutual\n",
    "constraints that largely determines the similarity of meaning of words and sets of words to\n",
    "each other.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Terminology Defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a vast list of new terminoloy defined by the field of NLP. Below I will briefly define those of significance to LSA which will be used extensively throughout this analysis.\n",
    "\n",
    "* **Bag Of Words (BOW)** - An abstraction model in NLP where we consider each document of text to simply be a \"bag of words\" in the literal sense, such that grammar and conceptual meaning is ignored.\n",
    "* **Term Frequency–Inverse Document Frequency (TF-IDF)** - A mathematical calculation for scoring the importance of a word in a document or a collection. This score value is based on _Zipf's Law_ of power distributions.\n",
    "* **Term** - A single word found in a document of text.\n",
    "* **Document** - A single collection of terms. Defined by the LSA study. In this case, each discussion post by a user will be a document.\n",
    "* **Corpus** - A single collection of related documents.\n",
    "* **Concept** - The final output of an LSA is a list of concepts. These are words, or multiple words together, which were found to have the highest significance across our corpus. They are called concepts, because they represent a meaningful 'conceptualization' that has been extracted from the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Process/Procedure & Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In brief, I will summarize a list of 7 steps representing the overall process required to perform an LSA:\n",
    "\n",
    "1. Collect/Retrieve a dataset containing text of interest. \n",
    "2. Define which text in the dataset will be represented as documents (sentences, discussion board poasts, news articles, ?)\n",
    "3. Using the BOW model, parse by document and store words in a BOW where each bag is a document. Ending result should be a collection of documents of terms.\n",
    "4. Clean the data by removing any non-alphanumeric characters such as HTML or XML tagging. Next, remove words that have very high frequency of repetition across the corpus, but with little to no significance. Due to the nature of 'TF-IDF' which relies on the _inverse_ frequency of significant terms across the corpus, this part of the process is not a straightforward one. Instead, by trial and error remove words with caution and sparingly, then re-test the model. This means steps 1-7 are completed, however you then must test and repeat this step possibly several times until the desired output is achieved. \n",
    "5. Perform TF-IDF Vectorization. This scores the words as terms for each document and across the corpus.\n",
    "6. Matrix decomposition using the SVD algorithm.\n",
    "7. Output a list of concepts extracted. \n",
    "\n",
    "Now we can begin our prepartions for LSA, starting with step 1, importing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports, and dataset download via sk-learn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import re\n",
    "\n",
    "categories = ['soc.religion.christian']\n",
    "dataset = fetch_20newsgroups(subset='all',shuffle=True, random_state=42, categories=categories)\n",
    "corpus = dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many documents (forum posts) are in the dataset\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: sciysg@nusunix1.nus.sg (Yung Shing Gene)\\nSubject: Mission Aviation Fellowship\\nOrganization: National University of Singapore\\nLines: 3\\n\\nHi,\\n\\tDoes anyone know anything about this group and what they\\ndo? Any info would be appreciated. Thanks!\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first document\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor x in range(0,12):\\n   print(corpus[x])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Uncomment this block to inspect a sample of 10 documents *\n",
    "# Print the first 10 documents to inspect the data\n",
    "'''\n",
    "for x in range(0,12):\n",
    "    print(corpus[x])\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**  \n",
    "It appears our data is in plain text with no tagging. However, each post starts with a heading which I've noticed is also variable across the corpus. For example some posts start with a header containing \"From\", \"Subject\", and \"Organization\" while others do not. The following headers are present across the corpus:  \n",
    "* **From:** [ _email@emailaddress.com_ ]\n",
    "* **Subject:** [ _topic_ ]\n",
    "* **Reply-To:** [ _email@emailaddress.com_ ]\n",
    "* **Organization:** [ _Organization Name_ ]\n",
    "* **Lines:** [ _# Lines of post_ ]\n",
    "\n",
    "Also, it appears that a post can be from either a public individual or a member of an organization. In either case, posts can also be both new posts or replies to other's posts. Every header ends with \"Lines:\" which tells us the number of lines of text contained in the post message itself.\n",
    "\n",
    "\n",
    "Post content looks like it could be problemsome for LSA if I don't carefully define the stopset of exclusion words. I found that this part of the process consisted of stopset defining and repetitive model testing in order to fine-tune the results.   \n",
    "\n",
    "One thing that I know we are going to want to exclude regardless are e-mail addresses because these items appear across the entire corpus, and therefore will decrease model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: \n",
      "Subject: tongues (read me!)\n",
      "Lines: 8\n",
      "\n",
      "Persons interested in the tongues question are are invited to\n",
      "peruse an essay of mine, obtainable by sending the message\n",
      " GET TONGUES NOTRANS\n",
      " to  or to\n",
      "    \n",
      "\n",
      " Yours,\n",
      " James Kiefer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using regex, find and remove all e-mail addresses in all documents across the entire corpus\n",
    "corpus = [re.sub(r'(\\s)(\\S+\\@\\S+)(\\s)', r'\\1\\3', corpus[x]) for x in range(len(corpus))]\n",
    "\n",
    "# Check it\n",
    "print(corpus[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from: \n",
      "subject: tongues (read me!)\n",
      "lines: 8\n",
      "\n",
      "persons interested in the tongues question are are invited to\n",
      "peruse an essay of mine, obtainable by sending the message\n",
      " get tongues notrans\n",
      " to  or to\n",
      "    \n",
      "\n",
      " yours,\n",
      " james kiefer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert all text to lower-case\n",
    "postDocs = [x.lower() for x in corpus]\n",
    "\n",
    "# Check it\n",
    "print(postDocs[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Defining 'stopwords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have removed all of the email addresses and formatted the text to lower-case, I will define the stopset. \n",
    "\n",
    "A 'stopset' is a list of 'stopwords' which will be excluded from analysis automatically by scikit-learn's vectorization algorithm. For this LSA, I'm going to use a combination of two pre-built lists for the first attempt: a stopset provided by _Natural Language Toolkit(NTLK)_, and one that I found online called the _Terrier stopset_.\n",
    "\n",
    "In order to combine these two, we store them in a 'set' datastructure and perform a 'union' between them removing duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jasonschenck/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import NTLK stopset\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this cell to add new exclusion words to the stopset before and/or after model testing.\n",
    "# Note: Most of the words below were added over the course of numerous output testing efforts.\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "stopset.update(['mercury','san','christiansen','dozier','athens','josh','0001','jose','lois','perry','department','editorial','etc','0358','542','706','30602','nasa','langley','subject','elizabeth'\n",
    "                'phone','bell','nova','gmi','khan0095','budd','28','bud','nj','wkuvx1','bitnet','easteee','holt','gatech','carol','howard','len','hampton','va','cs','terrance','acad1','sahs','uth','randerso','larc','gov','whitesbsd','nextwork','trol','eeap','apr',\n",
    "                'r2d2','vbv','n4tmi','wbt','wycliffe','ata','hfsi','uk','fidonet','jeff','fenholt','indiana','fisher','microsystems','creps','alvin','netcom','andrew','fil','revdak','jr','velasco',\n",
    "                'virgilio','ac','za','hayesstw','risc1','ucs','lee','nicholas','mandock','randal','overacker','larry','bernard','elizabeth','dean','seanna','unisa','rose','bryan','bnr','jayne','heath','scott','llo','acs','vela','atterlep',\n",
    "                'lines','petch','carlson','caralv','university','georgia','aisun3','reply-to','organization','hulman','hayes','steve','mcovingt','ai','ca','covington','bigelow','eugene','tek','gvg47','chuck','gvg','com','uga','bernadette','rutgers',\n",
    "                'edu','quot','spacing','text','line','none','sans','line','title','word', 'neue','johnsd2','rpi','mls','panix','ebay','group','freenet','carleton','ncr','cso','uxa','uiuc','bjorn','elsegundoca','mit','koberg','gt7122b','oo','la','microsoft','kuhub','cc','ukans',\n",
    "                'fnal','marka','csd','sapienza','lady','posting','rolfe','joe','jon','tom','fred','ling','siew','wee','matt5','lest','bill','wager','oakland','rochester','alan','steele','therefore','todd','aaron','bryce','a888','sledd','stan','pretoria','392','commentary',\n",
    "                'cox','paz','vic','fax','713','703','3729','827','murray','dale','gary','reply','mail','gerry','tx','shall','245','shell','box','univ','aa888','traer','bruce','__','___','601','22102','708','632','trei','eggert','amateur','radio','company','houston','lincoln','408',\n",
    "                '241','9760','02173','617','244','st','203','617','981','2575','subject','really','number','quite','loisc','article','baker','ashley','sj','see'])\n",
    "\n",
    "# Potential bible verse references, originally added and then removed from stopset\n",
    "# '44','31','10','11','31','14','21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the Terrier stopset from file, union with existing stopset\n",
    "terrierstopset = open('terrierstopset.txt', 'r').read()\n",
    "stopset = set(stopset).union(set(terrierstopset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this process, I'll be using the TfidfVectorizer() function from the scikit-learn library. This is the part of the LSA that actually converts the words of text that we have collected in to numerical representations by assigning them TF-IDF scores. \n",
    "> _ The TF-IDF score of a word 'w' is:_  \n",
    "> \n",
    "> $$tf(w) * idf(w)$$\n",
    ">\n",
    "> _where: $$tf(w) =\\frac{\\text{number of times a word appears in the doc}}{\\text{total number of words in the doc}}$$_ \n",
    ">\n",
    "> and : $$idf(w)=  \\left\\{log\\frac{\\text{number of documents}}{\\text{number of documents that contain the word w}}\\right\\}$$\n",
    "\n",
    "When we vectorize, we are essentially defining a lexical analyzer that is built into scikit-learn and therefore must specify some important parameters:  \n",
    "\n",
    "* **stopwords:** set the param to var stopset  \n",
    "<br>\n",
    "* **use idf:** always set to true for LSA  \n",
    "<br>\n",
    "* **ngram range:** 'grams' are words, and the ngram_range specifies to the analyzer the minimum(1) to the maximum(N) grams to consider for contextual relationships. I originally started this analysis with ngram_range=(1,3), however found through serveral rounds of testing and fine-tuning that (2,5) tends to produce the most optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the vectorizer model\n",
    "vectorizer = TfidfVectorizer(stop_words=stopset, use_idf=True, ngram_range=(2, 4),smooth_idf=True)\n",
    "\n",
    "# Fit the corpus data\n",
    "X = vectorizer.fit_transform(postDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 420714)\t0.158640819949\n",
      "  (0, 338335)\t0.158640819949\n",
      "  (0, 152859)\t0.158640819949\n",
      "  (0, 243857)\t0.158640819949\n",
      "  (0, 39327)\t0.158640819949\n",
      "  (0, 141944)\t0.158640819949\n",
      "  (0, 251236)\t0.138487062613\n",
      "  (0, 343183)\t0.158640819949\n",
      "  (0, 175992)\t0.143395091067\n",
      "  (0, 28432)\t0.115840647931\n",
      "  (0, 205145)\t0.131086376814\n",
      "  (0, 28878)\t0.158640819949\n",
      "  (0, 189799)\t0.158640819949\n",
      "  (0, 412636)\t0.138487062613\n",
      "  (0, 31117)\t0.138487062613\n",
      "  (0, 420715)\t0.158640819949\n",
      "  (0, 338336)\t0.158640819949\n",
      "  (0, 152860)\t0.158640819949\n",
      "  (0, 243858)\t0.158640819949\n",
      "  (0, 39328)\t0.158640819949\n",
      "  (0, 141945)\t0.158640819949\n",
      "  (0, 251245)\t0.158640819949\n",
      "  (0, 343184)\t0.158640819949\n",
      "  (0, 175993)\t0.143395091067\n",
      "  (0, 28435)\t0.149722640257\n",
      "  (0, 205149)\t0.158640819949\n",
      "  (0, 28879)\t0.158640819949\n",
      "  (0, 189800)\t0.158640819949\n",
      "  (0, 412639)\t0.143395091067\n",
      "  (0, 420716)\t0.158640819949\n",
      "  (0, 338337)\t0.158640819949\n",
      "  (0, 152861)\t0.158640819949\n",
      "  (0, 243859)\t0.158640819949\n",
      "  (0, 39329)\t0.158640819949\n",
      "  (0, 141946)\t0.158640819949\n",
      "  (0, 251246)\t0.158640819949\n",
      "  (0, 343185)\t0.158640819949\n",
      "  (0, 175994)\t0.149722640257\n",
      "  (0, 28436)\t0.158640819949\n",
      "  (0, 205150)\t0.158640819949\n",
      "  (0, 28880)\t0.158640819949\n",
      "  (0, 189801)\t0.158640819949\n"
     ]
    }
   ],
   "source": [
    "# Tada! This is now the output of the first document in the corpus, in sparse IDF matrix form.\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(997, 420957)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The current shape is (documents, terms)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 SVD Modeling with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single Value Decomposition** (SVD) is the process of taking our corpus of matrices (X), and performing _matrix decomposition_ such that:\n",
    "\n",
    "<big>$$X \\approx USV^{T}$$</big>\n",
    "\n",
    "where...\n",
    "\n",
    "* **X** = Original corpus matrix\n",
    "* **m** = Number of documents contained in X\n",
    "* **n** = Number of terms\n",
    "<br>\n",
    "\n",
    " \n",
    "**_X is decomposed into three matricies called U, S, and T with k-value such that..._**  \n",
    "\n",
    "\n",
    "\n",
    ">* **k** = Number of concepts we want to mine for\n",
    ">\n",
    ">\n",
    ">* **U** = An {'_m x k_'} matrix.  \n",
    ">  * _Rows_ = Documents\n",
    ">  * _Columns_ = Concepts\n",
    ">* **S** = A {'_k x k_'} diagonal matrix. \n",
    ">  * _Elements_ =  Variation captured from each concept.\n",
    ">* **V** = An {'_n x k_'} matrix.\n",
    ">  * _Rows_ = Terms\n",
    ">  * _Columns_ = Concepts\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an advanced mathematical procedure involving linear algebra which will decompose our matrix X into three U,S,& V. The entire process is built-in to scikit-learn as an engine model, all we must do is define the model specifications and let it do the work for us. \n",
    "\n",
    "[**scikit-learn**](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) provides the following documentation on this function:  \n",
    "> \"Dimensionality reduction using truncated SVD (aka LSA).\n",
    "This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently.\n",
    "In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).\n",
    "This estimator supports two algorithms: a fast randomized SVD solver, and a “naive” algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the TruncatedSVD model\n",
    "\n",
    "# Params: n_components=100 for LSA per sk-learn doc, n_iter=5 (default, and should be adjusted during testing) \n",
    "lsa = TruncatedSVD(n_components=100, n_iter=5)\n",
    "\n",
    "# Fit the model\n",
    "lsa.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.54905793e-06,   3.54905793e-06,   3.54905793e-06, ...,\n",
       "         2.19264744e-05,   2.19264744e-05,   2.19264744e-05])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After decomposition, 'lsa.components_[]' represents matrix V'\n",
    "lsa.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "priest priest\n",
      "four years\n",
      "immaculate conception\n",
      "answer priest\n",
      "told priest\n",
      "years old\n",
      "case doctrine\n",
      "apparition deemed\n",
      "apparition deemed true\n",
      "apparition deemed true sealed\n",
      " \n",
      "Concept 1:\n",
      "secretary interior\n",
      "married god\n",
      "god eyes\n",
      "appointee james\n",
      "appointee james watt\n",
      "appointee james watt pentacostal\n",
      "christian think\n",
      "christian think secretary\n",
      "christian think secretary interior\n",
      "days would last\n",
      " \n",
      "Concept 2:\n",
      "grass valley\n",
      "daily verse grass\n",
      "daily verse grass valley\n",
      "verse grass\n",
      "verse grass valley\n",
      "verse grass valley grass\n",
      "grass valley grass\n",
      "grass valley grass valley\n",
      "valley grass\n",
      "valley grass valley\n",
      " \n",
      "Concept 3:\n",
      "married god\n",
      "god eyes\n",
      "married god eyes\n",
      "two people\n",
      "people married god\n",
      "two people married god\n",
      "people married\n",
      "two people married\n",
      "become married god\n",
      "become married god eyes\n",
      " \n",
      "Concept 4:\n",
      "hate sin\n",
      "eternal death\n",
      "original sin\n",
      "christians hell\n",
      "commands us\n",
      "atheists hell\n",
      "love sinner\n",
      "since bible\n",
      "bible problem\n",
      "bible problem view\n",
      " \n",
      "Concept 5:\n",
      "hate sin\n",
      "commands us\n",
      "love sinner\n",
      "sin love\n",
      "hate sin love\n",
      "hate sin love sinner\n",
      "sin love sinner\n",
      "deal sin\n",
      "consistent christianity\n",
      "christianity would think\n",
      " \n",
      "Concept 6:\n",
      "christianity compatible\n",
      "issues christianity\n",
      "christianity compatible check\n",
      "compatible check\n",
      "homosexuality issues\n",
      "homosexuality issues christianity\n",
      "gay christians\n",
      "18 22\n",
      "research center\n",
      "lev 18 22\n",
      " \n",
      "Concept 7:\n",
      "original sin\n",
      "normal humanity\n",
      "god shaped\n",
      "spiritual needs\n",
      "doctrine original\n",
      "doctrine original sin\n",
      "northern research\n",
      "god shaped hole\n",
      "shaped hole\n",
      "mary assumption\n",
      " \n",
      "Concept 8:\n",
      "original sin\n",
      "doctrine original\n",
      "doctrine original sin\n",
      "one enter\n",
      "born water\n",
      "born water spirit\n",
      "water spirit\n",
      "capable comprehending\n",
      "babies supposed\n",
      "babies supposed baptised\n",
      " \n",
      "Concept 9:\n",
      "absolute truth\n",
      "scripture truths\n",
      "truths absolutes\n",
      "absolutes answer\n",
      "truths absolutes answer\n",
      "absolutes scripture\n",
      "always true\n",
      "contradiction terms\n",
      "truth absolute\n",
      "arrogance christians\n",
      " \n",
      "Concept 10:\n",
      "cultural interference\n",
      "body jesus\n",
      "ideological manipulation\n",
      "ideological manipulation cultural\n",
      "ideological manipulation cultural interference\n",
      "manipulation cultural\n",
      "manipulation cultural interference\n",
      "jewish roman\n",
      "phone translators\n",
      "body stolen\n",
      " \n",
      "Concept 11:\n",
      "body jesus\n",
      "jewish roman\n",
      "body stolen\n",
      "authorities would gained\n",
      "authorities would gained lot\n",
      "body jesus even\n",
      "body jesus even though\n",
      "dead body jesus\n",
      "dead body jesus even\n",
      "discredited christians\n",
      " \n",
      "Concept 12:\n",
      "11 51\n",
      "luke 11\n",
      "luke 11 51\n",
      "10 31 explained\n",
      "10 31 explained followers\n",
      "11 51 taking\n",
      "11 51 taking genesis\n",
      "24 44\n",
      "24 44 well\n",
      "24 44 well luke\n",
      " \n",
      "Concept 13:\n",
      "normal humanity\n",
      "mary assumption\n",
      "beyond sanctification\n",
      "beyond sanctification normal\n",
      "beyond sanctification normal humanity\n",
      "greeted mary\n",
      "mary beyond\n",
      "places mary\n",
      "places mary beyond\n",
      "sanctification normal\n",
      " \n",
      "Concept 14:\n",
      "kicked heaven\n",
      "heaven biblical\n",
      "kicked heaven biblical\n",
      "satan kicked\n",
      "satan kicked heaven\n",
      "satan kicked heaven biblical\n",
      "god authority\n",
      "ago satan\n",
      "ago satan angel\n",
      "ago satan angel god\n",
      " \n",
      "Concept 15:\n",
      "original sin\n",
      "never committed\n",
      "sin whole\n",
      "sin whole life\n",
      "whole life\n",
      "fully human\n",
      "mary fully\n",
      "mary fully human\n",
      "catholic dogma\n",
      "god repeat\n",
      " \n",
      "Concept 16:\n",
      "saved faith\n",
      "lukewarm christian\n",
      "faith alone\n",
      "say faith\n",
      "faith without\n",
      "murphy law\n",
      "faith deeds\n",
      "faith without deeds\n",
      "without deeds\n",
      "battling problem\n",
      " \n",
      "Concept 17:\n",
      "murphy law\n",
      "according purpose\n",
      "according purpose murphy\n",
      "according purpose murphy law\n",
      "amplifications murphy\n",
      "amplifications murphy law\n",
      "amplifications murphy law harmonize\n",
      "anything go\n",
      "anything go wrong\n",
      "anything go wrong familiar\n",
      " \n",
      "Concept 18:\n",
      "knows everything\n",
      "catholic doctrine predestination\n",
      "god knows everything\n",
      "god knows everything knows\n",
      "knows everything knows\n",
      "since god knows\n",
      "since god knows everything\n",
      "doctrine predestination\n",
      "everything knows\n",
      "since god\n",
      " \n",
      "Concept 19:\n",
      "go hell\n",
      "believe christian god\n",
      "god judge\n",
      "go heaven\n",
      "christian god\n",
      "believe christian\n",
      "going hell\n",
      "another effect\n",
      "another effect go\n",
      "another effect go hell\n",
      " \n",
      "Concept 20:\n",
      "exist must\n",
      "enduring values\n",
      "views christianity\n",
      "assume god\n",
      "universe exist\n",
      "atheist views\n",
      "atheist views christianity\n",
      "atheist views christianity accepting\n",
      "christianity accepting\n",
      "christianity accepting jeesus\n",
      " \n",
      "Concept 21:\n",
      "black sabbath\n",
      "spirit filled\n",
      "congregations christians\n",
      "filled believers\n",
      "spirit filled believers\n",
      "visit congregations\n",
      "visit congregations christians\n",
      "maybe trying\n",
      "christians happen\n",
      "christians happen homosexuals\n",
      " \n",
      "Concept 22:\n",
      "black sabbath\n",
      "may wrong\n",
      "hell_2 black\n",
      "hell_2 black sabbath\n",
      "may wrong part\n",
      "may wrong part black\n",
      "part black\n",
      "part black sabbath\n",
      "wrong part\n",
      "wrong part black\n",
      " \n",
      "Concept 23:\n",
      "black sabbath\n",
      "gifted one\n",
      "knew rules\n",
      "medieval period\n",
      "may wrong\n",
      "hell_2 black\n",
      "hell_2 black sabbath\n",
      "satanic tounges\n",
      "spirit talking\n",
      "speaking tongues\n",
      " \n",
      "Concept 24:\n",
      "knew rules\n",
      "medieval period\n",
      "aquinas day\n",
      "ancient books\n",
      "former atheists\n",
      "10th cent\n",
      "10th cent aquinas\n",
      "10th cent aquinas flourished\n",
      "anxious cases\n",
      "anxious cases doubt\n",
      " \n",
      "Concept 25:\n",
      "original sin\n",
      "enter heaven\n",
      "fair god\n",
      "doctrine original\n",
      "doctrine original sin\n",
      "cannot enter heaven\n",
      "cannot enter\n",
      "doctrine original sin sun\n",
      "geno doctrine\n",
      "geno doctrine original\n",
      " \n",
      "Concept 26:\n",
      "research center\n",
      "affirmation mcc\n",
      "affirmation mcc churches\n",
      "affirmation mcc churches meet\n",
      "anyone thinks gay\n",
      "anyone thinks gay christianity\n",
      "check dignity\n",
      "check dignity integrity\n",
      "check dignity integrity light\n",
      "christianity compatible check dignity\n",
      " \n",
      "Concept 27:\n",
      "mason beaten\n",
      "parents mason\n",
      "parents mason beaten\n",
      "christian parents\n",
      "fundamentalist christian\n",
      "fundamentalist christian parents\n",
      "strict fundamentalist\n",
      "beaten child\n",
      "beaten face\n",
      "beaten face would\n",
      " \n",
      "Concept 28:\n",
      "work god\n",
      "genocide work\n",
      "genocide work god\n",
      "serbian genocide\n",
      "serbian genocide work\n",
      "serbian genocide work god\n",
      "god hmm\n",
      "serbs work\n",
      "serbs work god\n",
      "serbs work god hmm\n",
      " \n",
      "Concept 29:\n",
      "proof resurection\n",
      "christ captialist\n",
      "following christ captialist\n",
      "obedience gensis\n",
      "strict obedience\n",
      "strict obedience gensis\n",
      "following christ\n",
      "soc religion\n",
      "accordance allow\n",
      "accordance allow witch\n",
      " \n",
      "Concept 30:\n",
      "codex bezae\n",
      "english translation\n",
      "acts apostles\n",
      "differences long\n",
      "greek nt\n",
      "long recension\n",
      "readings included\n",
      "vaticanus siniaticus\n",
      "1440 variant\n",
      "1440 variant readings\n",
      " \n",
      "Concept 31:\n",
      "soc religion\n",
      "active liberals\n",
      "active liberals catholics\n",
      "active liberals catholics new\n",
      "agers athiests\n",
      "agers athiests someone\n",
      "agers athiests someone might\n",
      "apparent primarily\n",
      "apparent primarily active\n",
      "apparent primarily active liberals\n",
      " \n",
      "Concept 32:\n",
      "south africa\n",
      "evidence senses\n",
      "arrogance christians\n",
      "science reason\n",
      "prove anything\n",
      "monash melbourne\n",
      "physical sphere\n",
      "blood transfusion\n",
      "god existence\n",
      "know truth\n",
      " \n",
      "Concept 33:\n",
      "never achieve\n",
      "goal never\n",
      "goal never achieve\n",
      "achieve know\n",
      "achieve know saved\n",
      "achieve know saved faith\n",
      "anything think james\n",
      "anything think james tells\n",
      "anyways christians\n",
      "anyways christians know\n",
      " \n",
      "Concept 34:\n",
      "environmentalism paganism\n",
      "associate research\n",
      "associate research scientist\n",
      "associate research scientist artificial\n",
      "intelligence programs phone 7415\n",
      "michael associate\n",
      "michael associate research\n",
      "michael associate research scientist\n",
      "phone 7415\n",
      "programs phone 7415\n",
      " \n",
      "Concept 35:\n",
      "ezekiel 18\n",
      "share guilt\n",
      "parents responsible\n",
      "teach children\n",
      "let look\n",
      "68 25\n",
      "death came\n",
      "baptism requires\n",
      "baptism requires faith\n",
      "requires faith\n",
      " \n",
      "Concept 36:\n",
      "enter heaven\n",
      "cannot enter heaven\n",
      "cannot enter\n",
      "jesus come\n",
      "asking jesus\n",
      "asking jesus come\n",
      "asking jesus come heart\n",
      "come heart\n",
      "jesus come heart\n",
      "adults since\n",
      " \n",
      "Concept 37:\n",
      "enter heaven\n",
      "south africa\n",
      "virgin mary\n",
      "cannot enter heaven\n",
      "cannot enter\n",
      "jesus come\n",
      "question virgin\n",
      "question virgin mary\n",
      "asking jesus\n",
      "asking jesus come\n",
      " \n",
      "Concept 38:\n",
      "christian practices\n",
      "parallel mormon\n",
      "mormon ceremonies\n",
      "mormon temples\n",
      "practices parallel\n",
      "early christian\n",
      "aspect christian\n",
      "aspect christian worship\n",
      "aspect christian worship involve\n",
      "biblical reason\n",
      " \n",
      "Concept 39:\n",
      "virgin mary\n",
      "question virgin\n",
      "question virgin mary\n",
      "christian practices\n",
      "parallel mormon\n",
      "although bodily\n",
      "although bodily assumption\n",
      "although bodily assumption basis\n",
      "assumption basis\n",
      "assumption basis bible\n",
      " \n",
      "Concept 40:\n",
      "become atheists\n",
      "people become\n",
      "people become atheists\n",
      "darin johnson\n",
      "sex christianity\n",
      "atheist prayer\n",
      "believe god\n",
      "proven wrong\n",
      "gay christians\n",
      "churches remind\n",
      " \n",
      "Concept 41:\n",
      "darin johnson\n",
      "sex christianity\n",
      "gay christians\n",
      "churches remind\n",
      "gay churches\n",
      "gay churches remind\n",
      "christians sex\n",
      "christians sex christianity\n",
      "abstain time\n",
      "abstain time would\n",
      " \n",
      "Concept 42:\n",
      "new christian\n",
      "like ask\n",
      "would like ask\n",
      "questions new\n",
      "questions new christian\n",
      "steven hoskins\n",
      "anyone recommend\n",
      "anyone recommend good\n",
      "anyone recommend good reading\n",
      "ask anyone\n",
      " \n",
      "Concept 43:\n",
      "israeli government\n",
      "although israeli\n",
      "although israeli government\n",
      "although israeli government give\n",
      "appears nothing\n",
      "appears nothing stands\n",
      "appears nothing stands way\n",
      "brothers sisters time\n",
      "brothers sisters time hand\n",
      "days although\n",
      " \n",
      "Concept 44:\n",
      "luke account\n",
      "israeli government\n",
      "believing resurrection\n",
      "christian ever\n",
      "resurrection one\n",
      "although israeli\n",
      "although israeli government\n",
      "although israeli government give\n",
      "appears nothing\n",
      "appears nothing stands\n",
      " \n",
      "Concept 45:\n",
      "especially christianity\n",
      "use drugs\n",
      "christianity nothing\n",
      "christianity nothing drug\n",
      "christians inject\n",
      "drugs escape\n",
      "drugs escape reality\n",
      "escape reality\n",
      "especially christianity nothing\n",
      "especially christianity nothing drug\n",
      " \n",
      "Concept 46:\n",
      "foolish foolish\n",
      "revealed truth\n",
      "christian trait\n",
      "temper christian\n",
      "temper christian trait\n",
      "question authority\n",
      "acrid angry\n",
      "acrid angry sarcastic\n",
      "angry sarcastic\n",
      "replies acrid\n",
      " \n",
      "Concept 47:\n",
      "foolish foolish\n",
      "christian trait\n",
      "temper christian\n",
      "temper christian trait\n",
      "acrid angry\n",
      "acrid angry sarcastic\n",
      "angry sarcastic\n",
      "replies acrid\n",
      "replies acrid angry\n",
      "replies acrid angry sarcastic\n",
      " \n",
      "Concept 48:\n",
      "god one set\n",
      "god one set rules\n",
      "one set\n",
      "one set rules\n",
      "set rules\n",
      "god one\n",
      "ceremonial moral\n",
      "14 19\n",
      "moral laws\n",
      "ceremonial moral laws\n",
      " \n",
      "Concept 49:\n",
      "virgin mary\n",
      "biblical support\n",
      "catholic doctrine\n",
      "1950 though\n",
      "1950 though certainly\n",
      "1950 though certainly believed\n",
      "assumed body\n",
      "assumed body soul\n",
      "assumed body soul heavenly\n",
      "assumption blessed virgin\n",
      " \n",
      "Concept 50:\n",
      "death penalty\n",
      "capital punishment\n",
      "homosexual intercourse\n",
      "keep peace\n",
      "death penalty revenge\n",
      "penalty revenge\n",
      "try refute\n",
      "non violent\n",
      "non violent provisions\n",
      "violent provisions\n",
      " \n",
      "Concept 51:\n",
      "catholic liturgy\n",
      "palm sunday\n",
      "new things\n",
      "quality catholic\n",
      "quality catholic liturgy\n",
      "liturgy committee\n",
      "knows happening\n",
      "one knows happening\n",
      "ohio state\n",
      "one knows\n",
      " \n",
      "Concept 52:\n",
      "death penalty\n",
      "spiritual needs\n",
      "capital punishment\n",
      "catholic liturgy\n",
      "cell church\n",
      "palm sunday\n",
      "northern research\n",
      "catholic church\n",
      "new things\n",
      "quality catholic\n",
      " \n",
      "Concept 53:\n",
      "death penalty\n",
      "capital punishment\n",
      "eternal marriage\n",
      "given marriage\n",
      "marry given\n",
      "marry given marriage\n",
      "neither marry\n",
      "neither marry given\n",
      "neither marry given marriage\n",
      "david hammerslag\n",
      " \n",
      "Concept 54:\n",
      "death penalty\n",
      "pa ques\n",
      "capital punishment\n",
      "kulikauskas home\n",
      "original sin\n",
      "believe predestination\n",
      "catholic liturgy\n",
      "easter name\n",
      "easter name new\n",
      "easter name new testament\n",
      " \n",
      "Concept 55:\n",
      "eternal marriage\n",
      "given marriage\n",
      "neither marry\n",
      "marry given\n",
      "marry given marriage\n",
      "children born\n",
      "neither marry given\n",
      "neither marry given marriage\n",
      "born wedlock\n",
      "david hammerslag\n",
      " \n",
      "Concept 56:\n",
      "eternal marriage\n",
      "given marriage\n",
      "marry given\n",
      "marry given marriage\n",
      "neither marry\n",
      "neither marry given\n",
      "neither marry given marriage\n",
      "david hammerslag\n",
      "given marriage luke\n",
      "marriage luke\n",
      " \n",
      "Concept 57:\n",
      "believe predestination\n",
      "dreams oobes\n",
      "cell church\n",
      "pa ques\n",
      "body incidents\n",
      "dreams body\n",
      "dreams body incidents\n",
      "new testament\n",
      "vera shanti\n",
      "shanti noyes\n",
      " \n",
      "Concept 58:\n",
      "source existence\n",
      "especially christianity\n",
      "things exist\n",
      "use drugs\n",
      "oulu fi\n",
      "petri pihko\n",
      "emerge nothing\n",
      "anything exists\n",
      "christianity nothing\n",
      "christianity nothing drug\n",
      " \n",
      "Concept 59:\n",
      "cell church\n",
      "cell church discussion\n",
      "church discussion\n",
      "prophetic warning\n",
      "become atheists\n",
      "god told\n",
      "around saying\n",
      "around saying god\n",
      "around saying god told\n",
      "saying god told\n",
      " \n",
      "Concept 60:\n",
      "capital punishment\n",
      "put hell\n",
      "brain washed\n",
      "indoctrinated parents\n",
      "become christian\n",
      "become christian indoctrinated\n",
      "become christian indoctrinated parents\n",
      "christian indoctrinated\n",
      "christian indoctrinated parents\n",
      "christian indoctrinated parents probably\n",
      " \n",
      "Concept 61:\n",
      "second coming\n",
      "dreams oobes\n",
      "body incidents\n",
      "dreams body\n",
      "dreams body incidents\n",
      "mark 13\n",
      "john baptist\n",
      "repeated lives\n",
      "elijah come\n",
      "michigan hospitals\n",
      " \n",
      "Concept 62:\n",
      "much later\n",
      "david wagner\n",
      "church added\n",
      "canonical books\n",
      "deutero canonical\n",
      "whoah whoah\n",
      "spiritual quality\n",
      "deuterocanonicals esp\n",
      "deuterocanonicals esp sirach\n",
      "esp sirach\n",
      " \n",
      "Concept 63:\n",
      "believe predestination\n",
      "death penalty revenge\n",
      "penalty revenge\n",
      "try refute\n",
      "brain washed\n",
      "indoctrinated parents\n",
      "become christian\n",
      "eternity hell hell\n",
      "hell hell\n",
      "hell theory\n",
      " \n",
      "Concept 64:\n",
      "could choose\n",
      "second coming\n",
      "become atheists\n",
      "death penalty\n",
      "born necessity\n",
      "eternity hell hell\n",
      "hell theory\n",
      "annihilation however\n",
      "annihilation however minority\n",
      "annihilation however minority view\n",
      " \n",
      "Concept 65:\n",
      "second coming\n",
      "dreams oobes\n",
      "nabil ayoub\n",
      "church believe\n",
      "body incidents\n",
      "dreams body\n",
      "dreams body incidents\n",
      "coptic church\n",
      "mark 13\n",
      "could choose\n",
      " \n",
      "Concept 66:\n",
      "capital punishment\n",
      "second coming\n",
      "keep peace\n",
      "south africa\n",
      "mark 13\n",
      "hell hell\n",
      "different language\n",
      "personal view\n",
      "non violent\n",
      "non violent provisions\n",
      " \n",
      "Concept 67:\n",
      "god shaped\n",
      "god shaped hole\n",
      "shaped hole\n",
      "capital punishment\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "god shaped hole accepting\n",
      "hole accepting\n",
      "hole accepting jeesus\n",
      " \n",
      "Concept 68:\n",
      "cell church\n",
      "eternity hell\n",
      "second coming\n",
      "eternity hell hell\n",
      "hell hell\n",
      "hell theory\n",
      "annihilation however\n",
      "annihilation however minority\n",
      "annihilation however minority view\n",
      "denominations know\n",
      " \n",
      "Concept 69:\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "pregnancy rates\n",
      "sex education\n",
      "abstinence education\n",
      "non liberal\n",
      "meaning importance\n",
      "abstinence related\n",
      "abstinence related curricula\n",
      " \n",
      "Concept 70:\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "meaning importance\n",
      "holy spirit\n",
      "life meaning importance\n",
      "want meaning\n",
      "life meaning\n",
      "billion years\n",
      "father son\n",
      " \n",
      "Concept 71:\n",
      "pregnancy rates\n",
      "abstinence education\n",
      "sex education\n",
      "non liberal\n",
      "gifted one\n",
      "abstinence related\n",
      "abstinence related curricula\n",
      "related curricula\n",
      "cell church\n",
      "genesis 15\n",
      " \n",
      "Concept 72:\n",
      "second coming\n",
      "mark 13\n",
      "david koresh\n",
      "david koresh second\n",
      "david koresh second coming\n",
      "koresh second\n",
      "koresh second coming\n",
      "pregnancy rates\n",
      "desiree bradley\n",
      "sex education\n",
      " \n",
      "Concept 73:\n",
      "repeated lives\n",
      "john baptist\n",
      "cell church\n",
      "christianity repeated\n",
      "christianity repeated lives\n",
      "elijah come\n",
      "must first\n",
      "apostles ask\n",
      "apostles ask pharisees\n",
      "apostles ask pharisees say\n",
      " \n",
      "Concept 74:\n",
      "paul conditt\n",
      "much deleted\n",
      "people aids\n",
      "good bad\n",
      "existed prior\n",
      "leopard skin\n",
      "extremely angry\n",
      "rexlex writes\n",
      "true religion\n",
      "athiests hell\n",
      " \n",
      "Concept 75:\n",
      "michigan hospitals\n",
      "capital punishment\n",
      "broken ness\n",
      "husband wife\n",
      "dreams oobes\n",
      "filipp technology\n",
      "filipp technology services\n",
      "filipp technology services michigan\n",
      "hospitals surgery\n",
      "michigan hospitals surgery\n",
      " \n",
      "Concept 76:\n",
      "cell church\n",
      "cell church discussion\n",
      "church discussion\n",
      "dead sea\n",
      "cell churches\n",
      "dead sea scrolls\n",
      "sea scrolls\n",
      "coptic church\n",
      "cardinal ximenez\n",
      "believe going\n",
      " \n",
      "Concept 77:\n",
      "rich man\n",
      "god shaped\n",
      "physical body\n",
      "something effect\n",
      "cell church\n",
      "mr stowell\n",
      "jewish proselytism\n",
      "much deleted\n",
      "beyond sanctification\n",
      "beyond sanctification normal\n",
      " \n",
      "Concept 78:\n",
      "remain christian\n",
      "would remain\n",
      "would remain christian\n",
      "beyond doubt\n",
      "14 19\n",
      "first day\n",
      "much deleted\n",
      "god love\n",
      "knew beyond\n",
      "knew beyond doubt\n",
      " \n",
      "Concept 79:\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "blood transfusion\n",
      "certain without\n",
      "shadow doubt\n",
      "sayeth lord\n",
      "thus sayeth\n",
      "thus sayeth lord\n",
      "private interpretation\n",
      " \n",
      "Concept 80:\n",
      "blood transfusion\n",
      "jewish proselytism\n",
      "empty tomb\n",
      "natural disaster\n",
      "us ability\n",
      "true heaven\n",
      "14 19\n",
      "life sexual\n",
      "life sexual relations\n",
      "blood transfusion necessary\n",
      " \n",
      "Concept 81:\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "jesus christ\n",
      "natural disaster\n",
      "baritone range\n",
      "baritone range jesus\n",
      "baritone range jesus chants\n",
      "bass others\n",
      "bass others directly\n",
      " \n",
      "Concept 82:\n",
      "dead sea\n",
      "dead sea scrolls\n",
      "sea scrolls\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "many people\n",
      "wife husband\n",
      "immaculate conception\n",
      "sheila patterson\n",
      " \n",
      "Concept 83:\n",
      "blood transfusion\n",
      "blood transfusion necessary\n",
      "blood transfusion necessary save\n",
      "could perceived\n",
      "could perceived arrogant\n",
      "doctor believes\n",
      "life child\n",
      "necessary save\n",
      "necessary save life\n",
      "necessary save life child\n",
      " \n",
      "Concept 84:\n",
      "south africa\n",
      "expression mercy\n",
      "cell church\n",
      "christianity would think grateful\n",
      "could someone please\n",
      "could someone please remind\n",
      "grateful reflections\n",
      "grateful reflections oft\n",
      "grateful reflections oft quoted\n",
      "hovingh lpts\n",
      " \n",
      "Concept 85:\n",
      "cell church\n",
      "immaculate conception\n",
      "dead sea\n",
      "genesis 15\n",
      "jewish proselytism\n",
      "free decide\n",
      "bruise heel\n",
      "crush head\n",
      "crush head bruise\n",
      "crush head bruise heel\n",
      " \n",
      "Concept 86:\n",
      "online bible\n",
      "anything laboratory\n",
      "anything laboratory rejecting\n",
      "anything laboratory rejecting facts\n",
      "coming understand\n",
      "coming understand anything\n",
      "coming understand anything laboratory\n",
      "facts lexington\n",
      "facts lexington lewis\n",
      "facts wood\n",
      " \n",
      "Concept 87:\n",
      "natural disaster\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "definition christianity\n",
      "disaster judgement\n",
      "natural disaster judgement\n",
      "jesus name\n",
      "belief jesus\n",
      "portland earthquake\n",
      " \n",
      "Concept 88:\n",
      "spiritual needs\n",
      "genesis 15\n",
      "god love\n",
      "dead sea\n",
      "christ love\n",
      "bruise heel\n",
      "crush head\n",
      "crush head bruise\n",
      "crush head bruise heel\n",
      "head bruise\n",
      " \n",
      "Concept 89:\n",
      "natural disaster\n",
      "disaster judgement\n",
      "natural disaster judgement\n",
      "portland earthquake\n",
      "christian practices\n",
      "parallel mormon\n",
      "book jeremiah\n",
      "quick look\n",
      "south africa\n",
      "canterbury christchurch\n",
      " \n",
      "Concept 90:\n",
      "cell church\n",
      "words book\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "christian practices\n",
      "parallel mormon\n",
      "mormon ceremonies\n",
      "almost certainly\n",
      "michael walker\n",
      " \n",
      "Concept 91:\n",
      "jewish proselytism\n",
      "cell church\n",
      "every language\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "bible available\n",
      "accounts jewish\n",
      "accounts jewish proselytism\n",
      "accounts jewish proselytism new\n",
      " \n",
      "Concept 92:\n",
      "resurrection sunday\n",
      "pagan goddess\n",
      "ted kalivoda\n",
      "daniel segard\n",
      "goddess fertility\n",
      "two minds\n",
      "death penalty\n",
      "would lost\n",
      "free decide\n",
      "argument would compelling\n",
      " \n",
      "Concept 93:\n",
      "immaculate conception\n",
      "god shaped\n",
      "natural disaster\n",
      "jewish proselytism\n",
      "new testament\n",
      "second coming\n",
      "god shaped hole\n",
      "shaped hole\n",
      "would disappear\n",
      "disaster judgement\n",
      " \n",
      "Concept 94:\n",
      "holy spirit\n",
      "jesus name\n",
      "father son\n",
      "jacob esau\n",
      "every language\n",
      "exist three\n",
      "exist three forms\n",
      "water water\n",
      "three forms\n",
      "son holy spirit\n",
      " \n",
      "Concept 95:\n",
      "private interpretation\n",
      "second coming\n",
      "gifted one\n",
      "christian practices\n",
      "parallel mormon\n",
      "body incidents\n",
      "dreams body\n",
      "dreams body incidents\n",
      "god love\n",
      "mormon ceremonies\n",
      " \n",
      "Concept 96:\n",
      "holy spirit\n",
      "father son\n",
      "dead sea\n",
      "dead sea scrolls\n",
      "sea scrolls\n",
      "jacob esau\n",
      "son holy spirit\n",
      "empty tomb\n",
      "son holy\n",
      "father son holy\n",
      " \n",
      "Concept 97:\n",
      "according folly\n",
      "answer fool\n",
      "answer fool according\n",
      "answer fool according folly\n",
      "fool according\n",
      "fool according folly\n",
      "rick granberry\n",
      "catholic church\n",
      "immaculate conception\n",
      "true heaven\n",
      " \n",
      "Concept 98:\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "genesis 15\n",
      "michigan hospitals\n",
      "natural disaster\n",
      "also hold belief\n",
      "also hold\n",
      "local church\n",
      "resurrection sunday\n",
      " \n",
      "Concept 99:\n",
      "private interpretation\n",
      "jesus christ\n",
      "jacob esau\n",
      "sheila patterson\n",
      "would disappear\n",
      "resurrection sunday\n",
      "institute technology\n",
      "possibility error\n",
      "non christians\n",
      "little purposes\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Convert the SVD results from numerical representation, back to their appropriate word text form.\n",
    "# Iterates over the enumeration of matrix components, for each: zips the terms to components, sorts them, then prints. \n",
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print(\"Concept %d:\" % i )\n",
    "    for term in sortedTerms:\n",
    "        print(term[0])\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Results: Interpration Of Extracted Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**  \n",
    "In order to produce the above output, it took several attempts of fine-tuning the stopset list, vectorization parameters, and SVD parameters then re-running the model. During this process I found that stopset word selection can be tricky, because only those terms which repeat the most across the entire corpus should be excluded. If one unknowingly removes a term which is sparsely found in the corpus, then the efficiency of the model is reduced negatively, impacting both performance and the output of concepts. \n",
    "\n",
    "After trying a handful of different variations, I found the following parameters produce the most meaningful extraction of concepts:  \n",
    "- TfidfVectorizer(stop_words=stopset,use_idf=True, **ngram_range=(2, 4**)\n",
    "- TruncatedSVD(n_components=100, **n_iter=5**)\n",
    "\n",
    "Other configurations tested include ngram_range(1,3), (2,2), (2,3), (2,5), (3,3), and (1,4). For ngrams < 2 the results lacked substance and returned only very simple concepts such as: God, sin, hate, and love. As ngrams_range was adjusted the resulting concepts became much more intricate and meaningful. I also ran a few different configurations with different values for n_iter (epochs), and noticed that this significantly affected the runtime efficiency of the model for any values ~n_iter > 30. I tested n_iter=100, while it took well over 3 minutes to complete execution, the resulting concepts did not appear to have improved much, if at all. \n",
    "\n",
    "The exclusion words were updated several times as well with each test ran, and mostly what I found was that removing certain terms, about 4-5 at a time, then re-testing the model proved successful in the long-run. Specifically the output concepts were checked for terms which appeared out of place, and just 'odd', and then added to the stopset.\n",
    "\n",
    "An important observation made, was that of certain numbers that repeated as concept output. This was super tricky to filter for, as some were extremely significant actually representing bible verses that fit perfectly to the concept (ex: lev 18 22), while others were junk such as the following three numbers:'706','542','0358', which is actually the telephone number for the A.I. department at Georgia Tech! (_If you see a number produced as part of a concept, Google that number to find the bible verse. It proves to be very significant._)  \n",
    "\n",
    "**Interesting Findings**  \n",
    "Christianity is a topic that I am not personally very familiar with, which is in part why I chose it for this study. I wanted to see if I could extract concepts that were very clear to even an observer who is unknowledgeable on  the topic such as myself. \n",
    "\n",
    "I performed some research on a few of the more interesting concepts and ended up with some pretty awesome discoveries:\n",
    "\n",
    "- Ideological Manipulation ([Wikipedia](https://en.wikipedia.org/wiki/Dominant_ideology)):\n",
    ">  \"Social control exercised and effected by means of the _ideological manipulation_ of aspects of the common culture of a society — religion and politics, culture and economy, etc. — to explain and justify the status quo to the political advantage of the dominant (ruling) class...\"  \n",
    "\n",
    "- James G. Watt ([Wikipedia](https://en.wikipedia.org/wiki/James_G._Watt)):\n",
    "> \"James Gaius Watt (born January 31, 1938) served as U.S. Secretary of the Interior from 1981 to 1983. Often described as \"anti-environmentalist\", he was one of Ronald Reagan's most controversial cabinet appointments.\"\n",
    ">\n",
    "> \"In 1995, Watt was indicted on 25 counts of felony perjury and obstruction of justice by a federal grand jury, accused of making false statements before the grand jury investigating influence peddling at the Department of Housing and Urban Development, which he had lobbied in the 1980s\"\n",
    "\n",
    "- Speaking In Tongues ([Wikipedia](https://en.wikipedia.org/wiki/Glossolalia)):\n",
    "> \"Glossolalia or speaking in tongues, according to linguists, is the fluid vocalizing of speech-like syllables that lack any readily comprehended meaning, in some cases as part of religious practice in which it is believed to be a divine language unknown to the speaker.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
