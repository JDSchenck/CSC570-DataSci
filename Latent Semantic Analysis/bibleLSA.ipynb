{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis:\n",
    "## _Data Mining for Meaningful Concepts In Christianity Newsgroups_\n",
    "---\n",
    "\n",
    "Prepared By: Jason Schenck  \n",
    "Date: February 6th 2017  \n",
    "CSC-570 Data Science Essentials\n",
    "\n",
    "\n",
    "<br>\n",
    "<big>Table Of Contents</big>\n",
    "\n",
    "---\n",
    "* **[1 Introduction][Introduction]**\n",
    "   * [1.1][1.1] _Purpose & Data Source_\n",
    "   * [1.2][1.2] _What is a \"Latent Semantic Analysis\"?_\n",
    "   * [1.3][1.3] _Terminology Defined_\n",
    "   * [1.4][1.4] _Process/Procedure & Methodology_\n",
    "\n",
    "\n",
    "* **[2 Data Preparation][Data Preparation]**\n",
    "   * [2.1][2.1] _Data Retrieval_\n",
    "   * [2.2][2.2] _Data Inspection_\n",
    "   * [2.3][2.3] _Defining 'stopwords'_\n",
    "\n",
    "\n",
    "* **[3 Latent Semantic Analysis (LSA)][Latent Semantic Analysis (LSA)]**\n",
    "   * [3.1][3.1] _TF-IDF Vectorization_\n",
    "   * [3.2][3.2] _SVD Modeling with Scikit-Learn_\n",
    "\n",
    "\n",
    "* **[4 Results: Interpration Of Extracted Concepts][Results: Interpration Of Extracted Concepts]**\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "[Introduction]: #1-Introduction\n",
    "[1.1]: #1.1-Purpose-&-Data-Source\n",
    "[1.2]: #1.2-What-is-a-\"Latent-Semantic-Analysis\"?\n",
    "[1.3]: #1.3-Terminology-Defined\n",
    "[1.4]: #1.4-Process/Procedure-&-Methodology\n",
    "[Data Preparation]: #2-Data-Preparation\n",
    "[2.1]: #2.1-Data-Retrieval\n",
    "[2.2]: #2.2-Data-Inspection\n",
    "[2.3]: #2.3-Defining-'stopwords'\n",
    "[Latent Semantic Analysis (LSA)]: #3-Latent-Semantic-Analysis-(LSA)\n",
    "[3.1]: #3.1-TF-IDF-Vectorization\n",
    "[3.2]: #3.2-SVD-Modeling-with-Scikit-Learn\n",
    "[Results: Interpration Of Extracted Concepts]: #4-Results:-Interpration-Of-Extracted-Concepts\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Data Source: </b><a href=\"http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#\">\"Twenty Newsgroups\", Provided By: Scikit-Learn </a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Purpose & Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis I will be performing data mining in an effort to extract a series of meaningful and significant concepts from a public dataset of newsgroup postings on the topic of Christianity.\n",
    "\n",
    "The dataset, titled \"Twenty Newsgroups\" and is officially described as follows:\n",
    ">\"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\"\n",
    "\n",
    "A newsgroup is an online public forum for discussion on a particular topic. The topic that I will be extracting data from will be \"Christianity\" (_soc.religion.christian_). I'm very curious to see what the results of this analysis will be, and in concluding intend to share my opinion on them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 What is a \"Latent Semantic Analysis\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Latent Semantic Analysis (LSA)_ is a technique commonly used in the field of Natural Language Processing (NLP). As a computer scientist, when performing NLP we are concerned with the interactions that that exist between computers and human language. A great portion of this field focuses on the analysis of the relationship between multiple words in a document of text contained in a collection of related documents. This is known as the subfield of _Natural Language Understanding_ and can be thought of more simply as \"teaching computers how to read\". \n",
    "\n",
    "LSA is more formally defined by [_\"An Introduction to Latent Semantic Analysis\" by Landauer, Foltz, & Laham_](http://lsa.colorado.edu/papers/dp1.LSAintro.pdf)\n",
    ">\"Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the\n",
    "contextual-usage meaning of words by statistical computations applied to a large corpus of\n",
    "text (Landauer and Dumais, 1997). The underlying idea is that the aggregate of all the word\n",
    "contexts in which a given word does and does not appear provides a set of mutual\n",
    "constraints that largely determines the similarity of meaning of words and sets of words to\n",
    "each other.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Terminology Defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a vast list of new terminoloy defined by the field of NLP. Below I will briefly define those of significance to LSA which will be used extensively throughout this analysis.\n",
    "\n",
    "* **Bag Of Words (BOW)** - An abstraction model in NLP where we consider each document of text to simply be a \"bag of words\" in the literal sense, such that grammar and conceptual meaning is ignored.\n",
    "* **Term Frequency–Inverse Document Frequency (TF-IDF)** - A mathematical calculation for scoring the importance of a word in a document or a collection. This score value is based on _Zipf's Law_ of power distributions.\n",
    "* **Term** - A single word found in a document of text.\n",
    "* **Document** - A single collection of terms. Defined by the LSA study. In this case, each discussion post by a user will be a document.\n",
    "* **Corpus** - A single collection of related documents.\n",
    "* **Concept** - The final output of an LSA is a list of concepts. These are words, or multiple words together, which were found to have the highest significance across our corpus. They are called concepts, because they represent a meaningful 'conceptualization' that has been extracted from the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Process/Procedure & Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In brief, I will summarize a list of 7 steps representing the overall process required to perform an LSA:\n",
    "\n",
    "1. Collect/Retrieve a dataset containing text of interest. \n",
    "2. Define which text in the dataset will be represented as documents (sentences, discussion board poasts, news articles, ?)\n",
    "3. Using the BOW model, parse by document and store words in a BOW where each bag is a document. Ending result should be a collection of documents of terms.\n",
    "4. Clean the data by removing any non-alphanumeric characters such as HTML or XML tagging. Next, remove words that have very high frequency of repetition across the corpus, but with little to no significance. Due to the nature of 'TF-IDF' which relies on the _inverse_ frequency of significant terms across the corpus, this part of the process is not a straightforward one. Instead, by trial and error remove words with caution and sparingly, then re-test the model. This means steps 1-7 are completed, however you then must test and repeat this step possibly several times until the desired output is achieved. \n",
    "5. Perform TF-IDF Vectorization. This scores the words as terms for each document and across the corpus.\n",
    "6. Matrix decomposition using the SVD algorithm.\n",
    "7. Output a list of concepts extracted. \n",
    "\n",
    "Now we can begin our prepartions for LSA, starting with step 1, importing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports, and dataset download via sk-learn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import re\n",
    "\n",
    "categories = ['soc.religion.christian']\n",
    "dataset = fetch_20newsgroups(subset='all',shuffle=True, random_state=42, categories=categories)\n",
    "corpus = dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many documents (forum posts) are in the dataset\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: sciysg@nusunix1.nus.sg (Yung Shing Gene)\\nSubject: Mission Aviation Fellowship\\nOrganization: National University of Singapore\\nLines: 3\\n\\nHi,\\n\\tDoes anyone know anything about this group and what they\\ndo? Any info would be appreciated. Thanks!\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first document\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  \\nfor x in range(0,12):\\n print(corpus[x])\\n \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uncomment this block to inspect a sample of 10 documents *\n",
    "# Print the first 10 documents to inspect the data\n",
    "\n",
    "\"\"\"  \n",
    "for x in range(0,12):\n",
    " print(corpus[x])\n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**  \n",
    "It appears our data is in plain text with no tagging. However, each post starts with a heading which I've noticed is also variable across the corpus. For example some posts start with a header containing \"From\", \"Subject\", and \"Organization\" while others do not. The following headers are present across the corpus:  \n",
    "* **From:** [ _email@emailaddress.com_ ]\n",
    "* **Subject:** [ _topic_ ]\n",
    "* **Reply-To:** [ _email@emailaddress.com_ ]\n",
    "* **Organization:** [ _Organization Name_ ]\n",
    "* **Lines:** [ _# Lines of post_ ]\n",
    "\n",
    "Also, it appears that a post can be from either a public individual or a member of an organization. In either case, posts can also be both new posts or replies to other's posts. Every header ends with \"Lines:\" which tells us the number of lines of text contained in the post message itself.\n",
    "\n",
    "\n",
    "Post content looks like it could be problemsome for LSA if I don't carefully define the stopset of exclusion words. I found that this part of the process consisted of stopset defining and repetitive model testing in order to fine-tune the results.   \n",
    "\n",
    "One thing that I know we are going to want to exclude regardless are e-mail addresses because these items appear across the entire corpus, and therefore will decrease model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: \n",
      "Subject: tongues (read me!)\n",
      "Lines: 8\n",
      "\n",
      "Persons interested in the tongues question are are invited to\n",
      "peruse an essay of mine, obtainable by sending the message\n",
      " GET TONGUES NOTRANS\n",
      " to  or to\n",
      "    \n",
      "\n",
      " Yours,\n",
      " James Kiefer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using regex, find and remove all e-mail addresses in all documents across the entire corpus\n",
    "corpus = [re.sub(r'(\\s)(\\S+\\@\\S+)(\\s)', r'\\1\\3', corpus[x]) for x in range(len(corpus))]\n",
    "\n",
    "# Check it\n",
    "print(corpus[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from: \n",
      "subject: tongues (read me!)\n",
      "lines: 8\n",
      "\n",
      "persons interested in the tongues question are are invited to\n",
      "peruse an essay of mine, obtainable by sending the message\n",
      " get tongues notrans\n",
      " to  or to\n",
      "    \n",
      "\n",
      " yours,\n",
      " james kiefer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert all text to lower-case\n",
    "postDocs = [x.lower() for x in corpus]\n",
    "\n",
    "# Check it\n",
    "print(postDocs[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Defining 'stopwords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have removed all of the email addresses and formatted the text to lower-case, I will define the stopset. \n",
    "\n",
    "A 'stopset' is a list of 'stopwords' which will be excluded from analysis automatically by scikit-learn's vectorization algorithm. For this LSA, I'm going to use a combination of two pre-built lists for the first attempt: a stopset provided by _Natural Language Toolkit(NTLK)_, and one that I found online called the _Terrier stopset_.\n",
    "\n",
    "In order to combine these two, we store them in a 'set' datastructure and perform a 'union' between them removing duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jasonschenck/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import NTLK stopset\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this cell to add new exclusion words to the stopset before and/or after model testing.\n",
    "# Note: Most of the words below were added over the course of numerous output testing efforts.\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "stopset.update(['mercury','san','christiansen','dozier','athens','josh','0001','jose','lois','perry','department','editorial','etc','0358','542','706','30602','nasa','langley','subject','elizabeth'\n",
    "                'phone','bell','nova','gmi','khan0095','budd','28','bud','nj','wkuvx1','bitnet','easteee','holt','gatech','carol','howard','len','hampton','va','cs','terrance','acad1','sahs','uth','randerso','larc','gov','whitesbsd','nextwork','trol','eeap','apr',\n",
    "                'r2d2','vbv','n4tmi','wbt','wycliffe','ata','hfsi','uk','fidonet','jeff','fenholt','indiana','fisher','microsystems','creps','alvin','netcom','andrew','fil','revdak','jr','velasco',\n",
    "                'virgilio','ac','za','hayesstw','risc1','ucs','lee','nicholas','mandock','randal','overacker','larry','bernard','elizabeth','dean','seanna','unisa','rose','bryan','bnr','jayne','heath','scott','llo','acs','vela','atterlep',\n",
    "                'lines','petch','carlson','caralv','university','georgia','aisun3','reply-to','organization','hulman','hayes','steve','mcovingt','ai','ca','covington','bigelow','eugene','tek','gvg47','chuck','gvg','com','uga','bernadette','rutgers',\n",
    "                'edu','quot','spacing','text','line','none','sans','line','title','word', 'neue','johnsd2','rpi','mls','panix','ebay','group','freenet','carleton','ncr','cso','uxa','uiuc','bjorn','elsegundoca','mit','koberg','gt7122b','oo','la','microsoft','kuhub','cc','ukans',\n",
    "                'fnal','marka','csd','sapienza','lady','posting','rolfe','joe','jon','tom','fred','ling','siew','wee','matt5','lest','bill','wager','oakland','rochester','alan','steele','therefore','todd','aaron','bryce','a888','sledd','stan','pretoria','392','commentary',\n",
    "                'cox','paz','vic','fax','713','703','3729','827','murray','dale','gary','reply','mail','gerry','tx','shall','245','shell','box','univ','aa888','traer','bruce','__','___','601','22102','708','632','trei','eggert','amateur','radio','company','houston','lincoln','408',\n",
    "                '241','9760','02173','617','244','st','203','617','981','2575','subject','really','number','quite','loisc','article','baker','ashley','sj','see'])\n",
    "\n",
    "# Potential bible verse references, originally added and then removed from stopset\n",
    "# '44','31','10','11','31','14','21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the Terrier stopset from file, union with existing stopset\n",
    "terrierstopset = open('terrierstopset.txt', 'r').read()\n",
    "stopset = set(stopset).union(set(terrierstopset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this process, I'll be using the TfidfVectorizer() function from the scikit-learn library. This is the part of the LSA that actually converts the words of text that we have collected in to numerical representations by assigning them TF-IDF scores. \n",
    "> _ The TF-IDF score of a word 'w' is:_  \n",
    "> \n",
    "> $$tf(w) * idf(w)$$\n",
    ">\n",
    "> _where: $$tf(w) =\\frac{\\text{number of times a word appears in the doc}}{\\text{total number of words in the doc}}$$_ \n",
    ">\n",
    "> and : $$idf(w)=  \\left\\{log\\frac{\\text{number of documents}}{\\text{number of documents that contain the word w}}\\right\\}$$\n",
    "\n",
    "When we vectorize, we are essentially defining a lexical analyzer that is built into scikit-learn and therefore must specify some important parameters:  \n",
    "\n",
    "* **stopwords:** set the param to var stopset  \n",
    "<br>\n",
    "* **use idf:** always set to true for LSA  \n",
    "<br>\n",
    "* **ngram range:** 'grams' are words, and the ngram_range specifies to the analyzer the minimum(1) to the maximum(N) grams to consider for contextual relationships. I originally started this analysis with ngram_range=(1,3), however found through serveral rounds of testing and fine-tuning that (2,5) tends to produce the most optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the vectorizer model\n",
    "vectorizer = TfidfVectorizer(stop_words=stopset, use_idf=True, ngram_range=(2, 4),smooth_idf=True)\n",
    "\n",
    "# Fit the corpus data\n",
    "X = vectorizer.fit_transform(postDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 420714)\t0.158640819949\n",
      "  (0, 338335)\t0.158640819949\n",
      "  (0, 152859)\t0.158640819949\n",
      "  (0, 243857)\t0.158640819949\n",
      "  (0, 39327)\t0.158640819949\n",
      "  (0, 141944)\t0.158640819949\n",
      "  (0, 251236)\t0.138487062613\n",
      "  (0, 343183)\t0.158640819949\n",
      "  (0, 175992)\t0.143395091067\n",
      "  (0, 28432)\t0.115840647931\n",
      "  (0, 205145)\t0.131086376814\n",
      "  (0, 28878)\t0.158640819949\n",
      "  (0, 189799)\t0.158640819949\n",
      "  (0, 412636)\t0.138487062613\n",
      "  (0, 31117)\t0.138487062613\n",
      "  (0, 420715)\t0.158640819949\n",
      "  (0, 338336)\t0.158640819949\n",
      "  (0, 152860)\t0.158640819949\n",
      "  (0, 243858)\t0.158640819949\n",
      "  (0, 39328)\t0.158640819949\n",
      "  (0, 141945)\t0.158640819949\n",
      "  (0, 251245)\t0.158640819949\n",
      "  (0, 343184)\t0.158640819949\n",
      "  (0, 175993)\t0.143395091067\n",
      "  (0, 28435)\t0.149722640257\n",
      "  (0, 205149)\t0.158640819949\n",
      "  (0, 28879)\t0.158640819949\n",
      "  (0, 189800)\t0.158640819949\n",
      "  (0, 412639)\t0.143395091067\n",
      "  (0, 420716)\t0.158640819949\n",
      "  (0, 338337)\t0.158640819949\n",
      "  (0, 152861)\t0.158640819949\n",
      "  (0, 243859)\t0.158640819949\n",
      "  (0, 39329)\t0.158640819949\n",
      "  (0, 141946)\t0.158640819949\n",
      "  (0, 251246)\t0.158640819949\n",
      "  (0, 343185)\t0.158640819949\n",
      "  (0, 175994)\t0.149722640257\n",
      "  (0, 28436)\t0.158640819949\n",
      "  (0, 205150)\t0.158640819949\n",
      "  (0, 28880)\t0.158640819949\n",
      "  (0, 189801)\t0.158640819949\n"
     ]
    }
   ],
   "source": [
    "# Tada! This is now the output of the first document in the corpus, in sparse IDF matrix form.\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(997, 420957)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The current shape is (documents, terms)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 SVD Modeling with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single Value Decomposition** (SVD) is the process of taking our corpus of matrices (X), and performing _matrix decomposition_ such that:\n",
    "\n",
    "<big>$$X \\approx USV^{T}$$</big>\n",
    "\n",
    "where...\n",
    "\n",
    "* **X** = Original corpus matrix\n",
    "* **m** = Number of documents contained in X\n",
    "* **n** = Number of terms\n",
    "<br>\n",
    "\n",
    " \n",
    "**_X is decomposed into three matricies called U, S, and T with k-value such that..._**  \n",
    "\n",
    "\n",
    "\n",
    ">* **k** = Number of concepts we want to mine for\n",
    ">\n",
    ">\n",
    ">* **U** = An {'_m x k_'} matrix.  \n",
    ">  * _Rows_ = Documents\n",
    ">  * _Columns_ = Concepts\n",
    ">* **S** = A {'_k x k_'} diagonal matrix. \n",
    ">  * _Elements_ =  Variation captured from each concept.\n",
    ">* **V** = An {'_n x k_'} matrix.\n",
    ">  * _Rows_ = Terms\n",
    ">  * _Columns_ = Concepts\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an advanced mathematical procedure involving linear algebra which will decompose our matrix X into three U,S,& V. The entire process is built-in to scikit-learn as an engine model, all we must do is define the model specifications and let it do the work for us. \n",
    "\n",
    "[**scikit-learn**](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) provides the following documentation on this function:  \n",
    "> \"Dimensionality reduction using truncated SVD (aka LSA).\n",
    "This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently.\n",
    "In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).\n",
    "This estimator supports two algorithms: a fast randomized SVD solver, and a “naive” algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the TruncatedSVD model\n",
    "\n",
    "# Params: n_components=100 for LSA per sk-learn doc, n_iter=5 (default, and should be adjusted during testing) \n",
    "lsa = TruncatedSVD(n_components=100, n_iter=5)\n",
    "\n",
    "# Fit the model\n",
    "lsa.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.06724032e-05,   1.06724032e-05,   1.06724032e-05, ...,\n",
       "         3.45592466e-05,   3.45592466e-05,   3.45592466e-05])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After decomposition, 'lsa.components_[]' represents matrix V'\n",
    "lsa.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "priest priest\n",
      "four years\n",
      "immaculate conception\n",
      "answer priest\n",
      "told priest\n",
      "years old\n",
      "case doctrine\n",
      "apparition deemed\n",
      "apparition deemed true\n",
      "apparition deemed true sealed\n",
      " \n",
      "Concept 1:\n",
      "secretary interior\n",
      "married god\n",
      "god eyes\n",
      "appointee james\n",
      "appointee james watt\n",
      "appointee james watt pentacostal\n",
      "christian think\n",
      "christian think secretary\n",
      "christian think secretary interior\n",
      "days would last\n",
      " \n",
      "Concept 2:\n",
      "grass valley\n",
      "daily verse grass\n",
      "daily verse grass valley\n",
      "verse grass\n",
      "verse grass valley\n",
      "verse grass valley grass\n",
      "grass valley grass\n",
      "grass valley grass valley\n",
      "valley grass\n",
      "valley grass valley\n",
      " \n",
      "Concept 3:\n",
      "married god\n",
      "god eyes\n",
      "married god eyes\n",
      "two people\n",
      "people married god\n",
      "two people married god\n",
      "people married\n",
      "two people married\n",
      "become married god\n",
      "become married god eyes\n",
      " \n",
      "Concept 4:\n",
      "hate sin\n",
      "eternal death\n",
      "original sin\n",
      "commands us\n",
      "atheists hell\n",
      "christians hell\n",
      "love sinner\n",
      "since bible\n",
      "bible problem\n",
      "bible problem view\n",
      " \n",
      "Concept 5:\n",
      "hate sin\n",
      "commands us\n",
      "love sinner\n",
      "sin love\n",
      "hate sin love\n",
      "hate sin love sinner\n",
      "sin love sinner\n",
      "deal sin\n",
      "consistent christianity\n",
      "christianity would think\n",
      " \n",
      "Concept 6:\n",
      "christianity compatible\n",
      "issues christianity\n",
      "homosexuality issues\n",
      "christianity compatible check\n",
      "compatible check\n",
      "homosexuality issues christianity\n",
      "gay christians\n",
      "18 22\n",
      "research center\n",
      "lev 18 22\n",
      " \n",
      "Concept 7:\n",
      "original sin\n",
      "normal humanity\n",
      "god shaped\n",
      "northern research\n",
      "doctrine original\n",
      "doctrine original sin\n",
      "spiritual needs\n",
      "god shaped hole\n",
      "shaped hole\n",
      "mary assumption\n",
      " \n",
      "Concept 8:\n",
      "original sin\n",
      "doctrine original\n",
      "doctrine original sin\n",
      "one enter\n",
      "born water\n",
      "born water spirit\n",
      "water spirit\n",
      "capable comprehending\n",
      "babies supposed\n",
      "babies supposed baptised\n",
      " \n",
      "Concept 9:\n",
      "absolute truth\n",
      "scripture truths\n",
      "truths absolutes\n",
      "absolutes answer\n",
      "truths absolutes answer\n",
      "absolutes scripture\n",
      "always true\n",
      "contradiction terms\n",
      "arrogance christians\n",
      "truth absolute\n",
      " \n",
      "Concept 10:\n",
      "cultural interference\n",
      "body jesus\n",
      "ideological manipulation\n",
      "ideological manipulation cultural\n",
      "ideological manipulation cultural interference\n",
      "manipulation cultural\n",
      "manipulation cultural interference\n",
      "phone translators\n",
      "concerned recent\n",
      "concerned recent sil\n",
      " \n",
      "Concept 11:\n",
      "body jesus\n",
      "jewish roman\n",
      "body stolen\n",
      "authorities would gained\n",
      "authorities would gained lot\n",
      "body jesus even\n",
      "body jesus even though\n",
      "dead body jesus\n",
      "dead body jesus even\n",
      "discredited christians\n",
      " \n",
      "Concept 12:\n",
      "11 51\n",
      "luke 11\n",
      "luke 11 51\n",
      "10 31 explained\n",
      "10 31 explained followers\n",
      "11 51 taking\n",
      "11 51 taking genesis\n",
      "24 44\n",
      "24 44 well\n",
      "24 44 well luke\n",
      " \n",
      "Concept 13:\n",
      "normal humanity\n",
      "mary assumption\n",
      "beyond sanctification\n",
      "beyond sanctification normal\n",
      "beyond sanctification normal humanity\n",
      "greeted mary\n",
      "mary beyond\n",
      "places mary\n",
      "places mary beyond\n",
      "sanctification normal\n",
      " \n",
      "Concept 14:\n",
      "kicked heaven\n",
      "heaven biblical\n",
      "kicked heaven biblical\n",
      "satan kicked\n",
      "satan kicked heaven\n",
      "satan kicked heaven biblical\n",
      "god authority\n",
      "satan angel\n",
      "ago satan\n",
      "ago satan angel\n",
      " \n",
      "Concept 15:\n",
      "original sin\n",
      "never committed\n",
      "sin whole\n",
      "sin whole life\n",
      "whole life\n",
      "fully human\n",
      "mary fully\n",
      "mary fully human\n",
      "catholic dogma\n",
      "also never committed\n",
      " \n",
      "Concept 16:\n",
      "saved faith\n",
      "lukewarm christian\n",
      "faith alone\n",
      "faith without\n",
      "say faith\n",
      "faith deeds\n",
      "faith without deeds\n",
      "without deeds\n",
      "battling problem\n",
      "battling problem know\n",
      " \n",
      "Concept 17:\n",
      "murphy law\n",
      "according purpose\n",
      "according purpose murphy\n",
      "according purpose murphy law\n",
      "amplifications murphy\n",
      "amplifications murphy law\n",
      "amplifications murphy law harmonize\n",
      "anything go\n",
      "anything go wrong\n",
      "anything go wrong familiar\n",
      " \n",
      "Concept 18:\n",
      "go hell\n",
      "go heaven\n",
      "believe christian god\n",
      "god judge\n",
      "christian god\n",
      "believe christian\n",
      "going hell\n",
      "another effect\n",
      "another effect go\n",
      "another effect go hell\n",
      " \n",
      "Concept 19:\n",
      "knows everything\n",
      "catholic doctrine predestination\n",
      "god knows everything\n",
      "god knows everything knows\n",
      "knows everything knows\n",
      "since god knows\n",
      "since god knows everything\n",
      "doctrine predestination\n",
      "everything knows\n",
      "since god\n",
      " \n",
      "Concept 20:\n",
      "exist must\n",
      "enduring values\n",
      "views christianity\n",
      "assume god\n",
      "universe exist\n",
      "atheist views\n",
      "atheist views christianity\n",
      "atheist views christianity accepting\n",
      "christianity accepting\n",
      "christianity accepting jeesus\n",
      " \n",
      "Concept 21:\n",
      "black sabbath\n",
      "spirit filled\n",
      "may wrong\n",
      "congregations christians\n",
      "filled believers\n",
      "spirit filled believers\n",
      "visit congregations\n",
      "visit congregations christians\n",
      "maybe trying\n",
      "drug addicts\n",
      " \n",
      "Concept 22:\n",
      "black sabbath\n",
      "may wrong\n",
      "hell_2 black\n",
      "hell_2 black sabbath\n",
      "may wrong part\n",
      "may wrong part black\n",
      "part black\n",
      "part black sabbath\n",
      "wrong part\n",
      "wrong part black\n",
      " \n",
      "Concept 23:\n",
      "gifted one\n",
      "satanic tounges\n",
      "spirit talking\n",
      "modern day\n",
      "witness real\n",
      "speaking tongues\n",
      "different language\n",
      "angelic tongue\n",
      "babel god\n",
      "babel god punished\n",
      " \n",
      "Concept 24:\n",
      "knew rules\n",
      "medieval period\n",
      "aquinas day\n",
      "ancient books\n",
      "former atheists\n",
      "10th cent\n",
      "10th cent aquinas\n",
      "10th cent aquinas flourished\n",
      "anxious cases\n",
      "anxious cases doubt\n",
      " \n",
      "Concept 25:\n",
      "original sin\n",
      "enter heaven\n",
      "fair god\n",
      "doctrine original\n",
      "doctrine original sin\n",
      "cannot enter heaven\n",
      "cannot enter\n",
      "doctrine original sin sun\n",
      "geno doctrine\n",
      "geno doctrine original\n",
      " \n",
      "Concept 26:\n",
      "research center\n",
      "affirmation mcc\n",
      "affirmation mcc churches\n",
      "affirmation mcc churches meet\n",
      "anyone thinks gay\n",
      "anyone thinks gay christianity\n",
      "check dignity\n",
      "check dignity integrity\n",
      "check dignity integrity light\n",
      "christianity compatible check dignity\n",
      " \n",
      "Concept 27:\n",
      "mason beaten\n",
      "parents mason\n",
      "parents mason beaten\n",
      "christian parents\n",
      "fundamentalist christian\n",
      "fundamentalist christian parents\n",
      "strict fundamentalist\n",
      "beaten child\n",
      "beaten face\n",
      "beaten face would\n",
      " \n",
      "Concept 28:\n",
      "work god\n",
      "genocide work\n",
      "genocide work god\n",
      "serbian genocide\n",
      "serbian genocide work\n",
      "serbian genocide work god\n",
      "god hmm\n",
      "serbs work\n",
      "serbs work god\n",
      "serbs work god hmm\n",
      " \n",
      "Concept 29:\n",
      "codex bezae\n",
      "english translation\n",
      "acts apostles\n",
      "differences long\n",
      "greek nt\n",
      "long recension\n",
      "readings included\n",
      "vaticanus siniaticus\n",
      "proof resurection\n",
      "christ captialist\n",
      " \n",
      "Concept 30:\n",
      "proof resurection\n",
      "christ captialist\n",
      "following christ captialist\n",
      "obedience gensis\n",
      "strict obedience\n",
      "strict obedience gensis\n",
      "following christ\n",
      "accordance allow\n",
      "accordance allow witch\n",
      "accordance allow witch live\n",
      " \n",
      "Concept 31:\n",
      "soc religion\n",
      "active liberals\n",
      "active liberals catholics\n",
      "active liberals catholics new\n",
      "agers athiests\n",
      "agers athiests someone\n",
      "agers athiests someone might\n",
      "apparent primarily\n",
      "apparent primarily active\n",
      "apparent primarily active liberals\n",
      " \n",
      "Concept 32:\n",
      "evidence senses\n",
      "science reason\n",
      "arrogance christians\n",
      "prove anything\n",
      "physical sphere\n",
      "monash melbourne\n",
      "physical senses\n",
      "good reason valid\n",
      "good reason valid system\n",
      "reason valid\n",
      " \n",
      "Concept 33:\n",
      "never achieve\n",
      "goal never\n",
      "goal never achieve\n",
      "achieve know\n",
      "achieve know saved\n",
      "achieve know saved faith\n",
      "anything think james\n",
      "anything think james tells\n",
      "anyways christians\n",
      "anyways christians know\n",
      " \n",
      "Concept 34:\n",
      "ezekiel 18\n",
      "enter heaven\n",
      "share guilt\n",
      "cannot enter heaven\n",
      "cannot enter\n",
      "sanctifying grace\n",
      "parents responsible\n",
      "jesus come\n",
      "let look\n",
      "teach children\n",
      " \n",
      "Concept 35:\n",
      "ezekiel 18\n",
      "environmentalism paganism\n",
      "_bashing_ paganism\n",
      "_bashing_ paganism figuring\n",
      "_bashing_ paganism figuring present\n",
      "answer pagans\n",
      "answer pagans lot\n",
      "answer pagans lot right\n",
      "bit less\n",
      "bit less effort\n",
      " \n",
      "Concept 36:\n",
      "enter heaven\n",
      "cannot enter heaven\n",
      "cannot enter\n",
      "environmentalism paganism\n",
      "artificial intelligence\n",
      "artificial intelligence programs\n",
      "artificial intelligence programs phone\n",
      "intelligence programs\n",
      "intelligence programs phone\n",
      "programs phone\n",
      " \n",
      "Concept 37:\n",
      "south africa\n",
      "enter heaven\n",
      "virgin mary\n",
      "cannot enter heaven\n",
      "cannot enter\n",
      "question virgin\n",
      "question virgin mary\n",
      "jesus come\n",
      "asking jesus\n",
      "asking jesus come\n",
      " \n",
      "Concept 38:\n",
      "christian practices\n",
      "parallel mormon\n",
      "mormon ceremonies\n",
      "mormon temples\n",
      "practices parallel\n",
      "early christian\n",
      "aspect christian\n",
      "aspect christian worship\n",
      "aspect christian worship involve\n",
      "biblical reason\n",
      " \n",
      "Concept 39:\n",
      "become atheists\n",
      "people become\n",
      "people become atheists\n",
      "believe god\n",
      "atheist prayer\n",
      "proven wrong\n",
      "darin johnson\n",
      "sex christianity\n",
      "new christian\n",
      "gay christians\n",
      " \n",
      "Concept 40:\n",
      "virgin mary\n",
      "question virgin\n",
      "question virgin mary\n",
      "although bodily\n",
      "although bodily assumption\n",
      "although bodily assumption basis\n",
      "assumption basis\n",
      "assumption basis bible\n",
      "assumption basis bible carl\n",
      "basis bible\n",
      " \n",
      "Concept 41:\n",
      "darin johnson\n",
      "sex christianity\n",
      "gay christians\n",
      "churches remind\n",
      "gay churches\n",
      "gay churches remind\n",
      "christians sex\n",
      "christians sex christianity\n",
      "avoid stumbling\n",
      "avoid stumbling block\n",
      " \n",
      "Concept 42:\n",
      "new christian\n",
      "like ask\n",
      "would like ask\n",
      "especially christianity\n",
      "israeli government\n",
      "questions new\n",
      "questions new christian\n",
      "steven hoskins\n",
      "christianity nothing\n",
      "christianity nothing drug\n",
      " \n",
      "Concept 43:\n",
      "spiritual needs\n",
      "revealed truth\n",
      "new christian\n",
      "luke account\n",
      "like ask\n",
      "would like ask\n",
      "question authority\n",
      "god truth\n",
      "questions would\n",
      "israeli government\n",
      " \n",
      "Concept 44:\n",
      "christian trait\n",
      "temper christian\n",
      "temper christian trait\n",
      "foolish foolish\n",
      "luke account\n",
      "losing temper\n",
      "losing temper christian\n",
      "losing temper christian trait\n",
      "acrid angry\n",
      "acrid angry sarcastic\n",
      " \n",
      "Concept 45:\n",
      "israeli government\n",
      "although israeli\n",
      "although israeli government\n",
      "although israeli government give\n",
      "appears nothing\n",
      "appears nothing stands\n",
      "appears nothing stands way\n",
      "brothers sisters time\n",
      "brothers sisters time hand\n",
      "days although\n",
      " \n",
      "Concept 46:\n",
      "luke account\n",
      "death penalty\n",
      "believing resurrection\n",
      "christian ever\n",
      "god shaped\n",
      "resurrection one\n",
      "belief resurrection\n",
      "become atheists\n",
      "cgsvax claremont\n",
      "cgsvax claremont writes\n",
      " \n",
      "Concept 47:\n",
      "death penalty\n",
      "god one set\n",
      "god one set rules\n",
      "one set\n",
      "one set rules\n",
      "set rules\n",
      "capital punishment\n",
      "god one\n",
      "ceremonial moral\n",
      "moral laws\n",
      " \n",
      "Concept 48:\n",
      "christian trait\n",
      "temper christian\n",
      "temper christian trait\n",
      "foolish foolish\n",
      "losing temper\n",
      "losing temper christian\n",
      "losing temper christian trait\n",
      "acrid angry\n",
      "acrid angry sarcastic\n",
      "angry sarcastic\n",
      " \n",
      "Concept 49:\n",
      "death penalty\n",
      "capital punishment\n",
      "death penalty revenge\n",
      "penalty revenge\n",
      "try refute\n",
      "keep peace\n",
      "non violent\n",
      "non violent provisions\n",
      "violent provisions\n",
      "catholic church\n",
      " \n",
      "Concept 50:\n",
      "capital punishment\n",
      "death penalty\n",
      "virgin mary\n",
      "biblical support\n",
      "roman catholic\n",
      "god shaped\n",
      "cell church\n",
      "catholic church\n",
      "immaculate conception\n",
      "assumption mary\n",
      " \n",
      "Concept 51:\n",
      "death penalty\n",
      "capital punishment\n",
      "death penalty revenge\n",
      "penalty revenge\n",
      "try refute\n",
      "others danger\n",
      "warning others\n",
      "warning others danger\n",
      "people island\n",
      "know daddy\n",
      " \n",
      "Concept 52:\n",
      "catholic liturgy\n",
      "palm sunday\n",
      "new things\n",
      "quality catholic\n",
      "quality catholic liturgy\n",
      "liturgy committee\n",
      "knows happening\n",
      "one knows happening\n",
      "ohio state\n",
      "one knows\n",
      " \n",
      "Concept 53:\n",
      "capital punishment\n",
      "death penalty\n",
      "keep peace\n",
      "non violent\n",
      "non violent provisions\n",
      "violent provisions\n",
      "god one set\n",
      "god one set rules\n",
      "one set\n",
      "one set rules\n",
      " \n",
      "Concept 54:\n",
      "spiritual needs\n",
      "northern research\n",
      "12 step\n",
      "recovery programs\n",
      "pa ques\n",
      "find people\n",
      "busy opinion\n",
      "busy opinion opinions\n",
      "busy opinion opinions oh\n",
      "canada philosophies\n",
      " \n",
      "Concept 55:\n",
      "dreams oobes\n",
      "body incidents\n",
      "dreams body\n",
      "dreams body incidents\n",
      "different moral\n",
      "ethics apply\n",
      "morally responsible\n",
      "david goggin\n",
      "morality applies\n",
      "moral laws\n",
      " \n",
      "Concept 56:\n",
      "believe predestination\n",
      "eternal marriage\n",
      "given marriage\n",
      "marry given\n",
      "marry given marriage\n",
      "cell church\n",
      "neither marry\n",
      "neither marry given\n",
      "neither marry given marriage\n",
      "david hammerslag\n",
      " \n",
      "Concept 57:\n",
      "children born\n",
      "born wedlock\n",
      "parents responsible\n",
      "68 25\n",
      "children born wedlock\n",
      "mormon beliefs\n",
      "teach children\n",
      "lds church\n",
      "share guilt\n",
      "children sins\n",
      " \n",
      "Concept 58:\n",
      "eternal marriage\n",
      "given marriage\n",
      "marry given\n",
      "marry given marriage\n",
      "neither marry\n",
      "neither marry given\n",
      "neither marry given marriage\n",
      "david hammerslag\n",
      "given marriage luke\n",
      "marriage luke\n",
      " \n",
      "Concept 59:\n",
      "dreams oobes\n",
      "body incidents\n",
      "dreams body\n",
      "dreams body incidents\n",
      "become atheists\n",
      "ethics apply\n",
      "john baptist\n",
      "repeated lives\n",
      "death penalty\n",
      "people become\n",
      " \n",
      "Concept 60:\n",
      "much later\n",
      "david wagner\n",
      "church added\n",
      "deutero canonical\n",
      "canonical books\n",
      "spiritual quality\n",
      "whoah whoah\n",
      "church history\n",
      "books added\n",
      "added books\n",
      " \n",
      "Concept 61:\n",
      "cell church\n",
      "much later\n",
      "church added\n",
      "david wagner\n",
      "canonical books\n",
      "second coming\n",
      "deutero canonical\n",
      "cell church discussion\n",
      "church discussion\n",
      "spiritual quality\n",
      " \n",
      "Concept 62:\n",
      "repeated lives\n",
      "john baptist\n",
      "new testament\n",
      "south africa\n",
      "pa ques\n",
      "elijah come\n",
      "put hell\n",
      "pregnancy rates\n",
      "christianity repeated\n",
      "christianity repeated lives\n",
      " \n",
      "Concept 63:\n",
      "brain washed\n",
      "indoctrinated parents\n",
      "become christian\n",
      "become christian indoctrinated\n",
      "become christian indoctrinated parents\n",
      "christian indoctrinated\n",
      "christian indoctrinated parents\n",
      "christian indoctrinated parents probably\n",
      "claim brain\n",
      "claim brain washed\n",
      " \n",
      "Concept 64:\n",
      "capital punishment\n",
      "god shaped\n",
      "keep peace\n",
      "god shaped hole\n",
      "shaped hole\n",
      "non violent\n",
      "non violent provisions\n",
      "violent provisions\n",
      "matthew huntbach\n",
      "death penalty military\n",
      " \n",
      "Concept 65:\n",
      "gifted one\n",
      "physical body\n",
      "jewish proselytism\n",
      "rich man\n",
      "repeated lives\n",
      "devil angels\n",
      "mark schnitzius\n",
      "glorifying god\n",
      "hell something\n",
      "john baptist\n",
      " \n",
      "Concept 66:\n",
      "gifted one\n",
      "glorifying god\n",
      "meaning importance\n",
      "second coming\n",
      "speak tounges\n",
      "life meaning\n",
      "god rather\n",
      "believe gifted\n",
      "believe gifted ones\n",
      "believe gifted ones glorifying\n",
      " \n",
      "Concept 67:\n",
      "god shaped\n",
      "cell church\n",
      "god shaped hole\n",
      "shaped hole\n",
      "second coming\n",
      "certain without\n",
      "shadow doubt\n",
      "sayeth lord\n",
      "thus sayeth\n",
      "thus sayeth lord\n",
      " \n",
      "Concept 68:\n",
      "god shaped\n",
      "much deleted\n",
      "god shaped hole\n",
      "shaped hole\n",
      "certain without\n",
      "shadow doubt\n",
      "sayeth lord\n",
      "thus sayeth\n",
      "thus sayeth lord\n",
      "something effect\n",
      " \n",
      "Concept 69:\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "husband wife\n",
      "jewish proselytism\n",
      "wife husband\n",
      "prayers hindered\n",
      "capital punishment\n",
      "sin block\n",
      "block prayers\n",
      " \n",
      "Concept 70:\n",
      "gifted one\n",
      "true religion\n",
      "answer question\n",
      "us lord\n",
      "spiritual needs\n",
      "think sad\n",
      "claim son\n",
      "claim son god\n",
      "claim son god prophet\n",
      "even claim\n",
      " \n",
      "Concept 71:\n",
      "gifted one\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "become atheists\n",
      "paul conditt\n",
      "much later\n",
      "believe god\n",
      "michigan hospitals\n",
      "people become\n",
      " \n",
      "Concept 72:\n",
      "second coming\n",
      "mark 13\n",
      "blood transfusion\n",
      "south africa\n",
      "david koresh\n",
      "david koresh second\n",
      "david koresh second coming\n",
      "koresh second\n",
      "koresh second coming\n",
      "jewish proselytism\n",
      " \n",
      "Concept 73:\n",
      "14 19\n",
      "gifted one\n",
      "south africa\n",
      "ten commandments\n",
      "weak faith\n",
      "jesus say\n",
      "expression mercy\n",
      "second coming\n",
      "first day\n",
      "jesus 14\n",
      " \n",
      "Concept 74:\n",
      "second coming\n",
      "mark 13\n",
      "pregnancy rates\n",
      "god shaped\n",
      "sex education\n",
      "abstinence education\n",
      "jewish proselytism\n",
      "non liberal\n",
      "physical body\n",
      "abstinence related\n",
      " \n",
      "Concept 75:\n",
      "cell church\n",
      "pregnancy rates\n",
      "blood transfusion\n",
      "physical body\n",
      "sex education\n",
      "abstinence education\n",
      "non liberal\n",
      "cell church discussion\n",
      "church discussion\n",
      "jesus christ\n",
      " \n",
      "Concept 76:\n",
      "holy spirit\n",
      "father son\n",
      "every language\n",
      "immaculate conception\n",
      "cell church\n",
      "son holy spirit\n",
      "son holy\n",
      "father son holy\n",
      "father son holy spirit\n",
      "bible available\n",
      " \n",
      "Concept 77:\n",
      "pregnancy rates\n",
      "abstinence education\n",
      "second coming\n",
      "non liberal\n",
      "sex education\n",
      "holy spirit\n",
      "online bible\n",
      "baritone range\n",
      "baritone range jesus\n",
      "baritone range jesus chants\n",
      " \n",
      "Concept 78:\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "god shaped\n",
      "god shaped hole\n",
      "shaped hole\n",
      "edgar pearlstein\n",
      "supreme court\n",
      "anyone verify\n",
      "anyone verify falsify\n",
      " \n",
      "Concept 79:\n",
      "physical body\n",
      "online bible\n",
      "us ability\n",
      "never heard\n",
      "cardinal ximenez\n",
      "holy spirit\n",
      "ability create\n",
      "threatened science\n",
      "life sexual\n",
      "life sexual relations\n",
      " \n",
      "Concept 80:\n",
      "private revelation\n",
      "much deleted\n",
      "kulikauskas home\n",
      "husband wife\n",
      "eternity hell hell\n",
      "hell hell\n",
      "existed prior\n",
      "alleged private\n",
      "alleged private revelation\n",
      "personal view\n",
      " \n",
      "Concept 81:\n",
      "god shaped\n",
      "second coming\n",
      "question authority\n",
      "husband wife\n",
      "wife husband\n",
      "god shaped hole\n",
      "shaped hole\n",
      "mark 13\n",
      "remain christian\n",
      "would remain\n",
      " \n",
      "Concept 82:\n",
      "online bible\n",
      "never heard\n",
      "bible software\n",
      "online bible software\n",
      "people new\n",
      "people new testament\n",
      "cell church\n",
      "bible study\n",
      "free decide\n",
      "new testament\n",
      " \n",
      "Concept 83:\n",
      "cell church\n",
      "dead sea\n",
      "dead sea scrolls\n",
      "sea scrolls\n",
      "coptic church\n",
      "us ability\n",
      "cell church discussion\n",
      "church discussion\n",
      "sex education\n",
      "could choose\n",
      " \n",
      "Concept 84:\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "jesus christ\n",
      "blood transfusion\n",
      "edgar pearlstein\n",
      "state government\n",
      "sheila patterson\n",
      "christ died\n",
      "every language\n",
      " \n",
      "Concept 85:\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "south africa\n",
      "pregnancy rates\n",
      "words book\n",
      "sex education\n",
      "abstinence education\n",
      "father son\n",
      "non liberal\n",
      " \n",
      "Concept 86:\n",
      "jesus name\n",
      "believe going\n",
      "free decide\n",
      "god shaped\n",
      "believe heaven\n",
      "heaven going\n",
      "going heaven\n",
      "marriage ceremony\n",
      "holy spirit\n",
      "resurrection sunday\n",
      " \n",
      "Concept 87:\n",
      "online bible\n",
      "never heard\n",
      "father son\n",
      "holy spirit\n",
      "bible software\n",
      "online bible software\n",
      "people new\n",
      "people new testament\n",
      "exist three\n",
      "exist three forms\n",
      " \n",
      "Concept 88:\n",
      "definition christianity\n",
      "dead sea\n",
      "would disappear\n",
      "dead sea scrolls\n",
      "sea scrolls\n",
      "belief jesus\n",
      "original poster\n",
      "possibility error\n",
      "anything laboratory\n",
      "anything laboratory rejecting\n",
      " \n",
      "Concept 89:\n",
      "dead sea\n",
      "second coming\n",
      "16 peter\n",
      "16 peter warns\n",
      "16 peter warns scriptures\n",
      "also read peter\n",
      "also read peter 16\n",
      "hard understand learned\n",
      "often hard understand\n",
      "often hard understand learned\n",
      " \n",
      "Concept 90:\n",
      "paul conditt\n",
      "question authority\n",
      "catholic church\n",
      "spiritual needs\n",
      "mark 13\n",
      "never heard\n",
      "online bible\n",
      "holy spirit\n",
      "second coming\n",
      "father son\n",
      " \n",
      "Concept 91:\n",
      "expression mercy\n",
      "catholic church\n",
      "definition christianity\n",
      "resurrection sunday\n",
      "belief jesus\n",
      "daily verse\n",
      "jewish proselytism\n",
      "book called\n",
      "words book\n",
      "find source\n",
      " \n",
      "Concept 92:\n",
      "spiritual needs\n",
      "private interpretation\n",
      "cell church\n",
      "expression mercy\n",
      "hard understand\n",
      "jacob esau\n",
      "bible unsuitable\n",
      "bible unsuitable new\n",
      "bible unsuitable new christians\n",
      "unsuitable new\n",
      " \n",
      "Concept 93:\n",
      "cell church\n",
      "free decide\n",
      "spiritual needs\n",
      "churches free\n",
      "private revelation\n",
      "true heaven\n",
      "churches free decide\n",
      "jesus christ\n",
      "genesis 15\n",
      "sunday law\n",
      " \n",
      "Concept 94:\n",
      "online bible\n",
      "never heard\n",
      "bible software\n",
      "online bible software\n",
      "people new\n",
      "people new testament\n",
      "unity god\n",
      "new testament\n",
      "marriage ceremony\n",
      "saying god\n",
      " \n",
      "Concept 95:\n",
      "private interpretation\n",
      "holy spirit\n",
      "peter 20\n",
      "father son\n",
      "definition christianity\n",
      "remain christian\n",
      "would remain\n",
      "would remain christian\n",
      "jesus christ\n",
      "christian practices\n",
      " \n",
      "Concept 96:\n",
      "every language\n",
      "jewish proselytism\n",
      "bible available\n",
      "cell church\n",
      "south africa\n",
      "nearly every\n",
      "every language missionaries\n",
      "language missionaries\n",
      "nearly every language\n",
      "nearly every language missionaries\n",
      " \n",
      "Concept 97:\n",
      "cell church\n",
      "question god\n",
      "jesus name\n",
      "hears prayers\n",
      "god hear\n",
      "god hear prayers\n",
      "hear prayers\n",
      "lord want\n",
      "19 centuries\n",
      "19 centuries ago\n",
      " \n",
      "Concept 98:\n",
      "immaculate conception\n",
      "private interpretation\n",
      "also read\n",
      "original sin\n",
      "16 peter\n",
      "16 peter warns\n",
      "16 peter warns scriptures\n",
      "also read peter\n",
      "also read peter 16\n",
      "hard understand learned\n",
      " \n",
      "Concept 99:\n",
      "homosexuality issues christiani\n",
      "issues christiani\n",
      "holy spirit\n",
      "homosexuality issues\n",
      "natural disaster\n",
      "interpret verses\n",
      "homosexuyality wrong\n",
      "homosexuyality wrong please\n",
      "homosexuyality wrong please tell\n",
      "interpreted way read\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Convert the SVD results from numerical representation, back to their appropriate word text form.\n",
    "# Iterates over the enumeration of matrix components, for each: zips the terms to components, sorts them, then prints. \n",
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print(\"Concept %d:\" % i )\n",
    "    for term in sortedTerms:\n",
    "        print(term[0])\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Results: Interpration Of Extracted Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**  \n",
    "In order to produce the above output, it took several attempts of fine-tuning the stopset list, vectorization parameters, and SVD parameters then re-running the model. During this process I found that stopset word selection can be tricky, because only those terms which repeat the most across the entire corpus should be excluded. If one unknowingly removes a term which is sparsely found in the corpus, then the efficiency of the model is reduced negatively, impacting both performance and the output of concepts. \n",
    "\n",
    "After trying a handful of different variations, I found the following parameters produce the most meaningful extraction of concepts:  \n",
    "- TfidfVectorizer(stop_words=stopset,use_idf=True, **ngram_range=(2, 4**)\n",
    "- TruncatedSVD(n_components=100, **n_iter=5**)\n",
    "\n",
    "Other configurations tested include ngram_range(1,3), (2,2), (2,3), (2,5), (3,3), and (1,4). For ngrams < 2 the results lacked substance and returned only very simple concepts such as: God, sin, hate, and love. As ngrams_range was adjusted the resulting concepts became much more intricate and meaningful. I also ran a few different configurations with different values for n_iter (epochs), and noticed that this significantly affected the runtime efficiency of the model for any values ~n_iter > 30. I tested n_iter=100, while it took well over 3 minutes to complete execution, the resulting concepts did not appear to have improved much, if at all. \n",
    "\n",
    "The exclusion words were updated several times as well with each test ran, and mostly what I found was that removing certain terms, about 4-5 at a time, then re-testing the model proved successful in the long-run. Specifically the output concepts were checked for terms which appeared out of place, and just 'odd', and then added to the stopset.\n",
    "\n",
    "An important observation made, was that of certain numbers that repeated as concept output. This was super tricky to filter for, as some were extremely significant actually representing bible verses that fit perfectly to the concept (ex: lev 18 22), while others were junk such as the following three numbers:'706','542','0358', which is actually the telephone number for the A.I. department at Georgia Tech! (_If you see a number produced as part of a concept, Google that number to find the bible verse. It proves to be very significant._)  \n",
    "\n",
    "**Interesting Findings**  \n",
    "Christianity is a topic that I am not personally very familiar with, which is in part why I chose it for this study. I wanted to see if I could extract concepts that were very clear to even an observer who is unknowledgeable on  the topic such as myself. \n",
    "\n",
    "I performed some research on a few of the more interesting concepts and ended up with some pretty awesome discoveries:\n",
    "\n",
    "- Ideological Manipulation ([Wikipedia](https://en.wikipedia.org/wiki/Dominant_ideology)):\n",
    ">  \"Social control exercised and effected by means of the _ideological manipulation_ of aspects of the common culture of a society — religion and politics, culture and economy, etc. — to explain and justify the status quo to the political advantage of the dominant (ruling) class...\"  \n",
    "\n",
    "- James G. Watt ([Wikipedia](https://en.wikipedia.org/wiki/James_G._Watt)):\n",
    "> \"James Gaius Watt (born January 31, 1938) served as U.S. Secretary of the Interior from 1981 to 1983. Often described as \"anti-environmentalist\", he was one of Ronald Reagan's most controversial cabinet appointments.\"\n",
    ">\n",
    "> \"In 1995, Watt was indicted on 25 counts of felony perjury and obstruction of justice by a federal grand jury, accused of making false statements before the grand jury investigating influence peddling at the Department of Housing and Urban Development, which he had lobbied in the 1980s\"\n",
    "\n",
    "- Speaking In Tongues ([Wikipedia](https://en.wikipedia.org/wiki/Glossolalia)):\n",
    "> \"Glossolalia or speaking in tongues, according to linguists, is the fluid vocalizing of speech-like syllables that lack any readily comprehended meaning, in some cases as part of religious practice in which it is believed to be a divine language unknown to the speaker.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
