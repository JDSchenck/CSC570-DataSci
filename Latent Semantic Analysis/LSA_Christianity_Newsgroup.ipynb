{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis:\n",
    "## _Data Mining for Meaningful Concepts In Christianity Newsgroups_\n",
    "---\n",
    "\n",
    "Prepared By: Jason Schenck  \n",
    "Date: February 6th 2017  \n",
    "CSC-570 Data Science Essentials\n",
    "\n",
    "\n",
    "<br>\n",
    "<big>Table Of Contents</big>\n",
    "\n",
    "---\n",
    "* **[1 Introduction][Introduction]**\n",
    "   * [1.1][1.1] _Purpose & Data Source_\n",
    "   * [1.2][1.2] _What is a \"Latent Semantic Analysis\"?_\n",
    "   * [1.3][1.3] _Terminology Defined_\n",
    "   * [1.4][1.4] _Process/Procedure & Methodology_\n",
    "\n",
    "\n",
    "* **[2 Data Preparation][Data Preparation]**\n",
    "   * [2.1][2.1] _Data Retrieval_\n",
    "   * [2.2][2.2] _Data Inspection_\n",
    "   * [2.3][2.3] _Defining 'stopwords'_\n",
    "\n",
    "\n",
    "* **[3 Latent Semantic Analysis (LSA)][Latent Semantic Analysis (LSA)]**\n",
    "   * [3.1][3.1] _TF-IDF Vectorization_\n",
    "   * [3.2][3.2] _SVD Modeling with Scikit-Learn_\n",
    "\n",
    "\n",
    "* **[4 Results: Interpration Of Extracted Concepts][Results: Interpration Of Extracted Concepts]**\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "[Introduction]: #1-Introduction\n",
    "[1.1]: #1.1-Purpose-&-Data-Source\n",
    "[1.2]: #1.2-What-is-a-\"Latent-Semantic-Analysis\"?\n",
    "[1.3]: #1.3-Terminology-Defined\n",
    "[1.4]: #1.4-Process/Procedure-&-Methodology\n",
    "[Data Preparation]: #2-Data-Preparation\n",
    "[2.1]: #2.1-Data-Retrieval\n",
    "[2.2]: #2.2-Data-Inspection\n",
    "[2.3]: #2.3-Defining-'stopwords'\n",
    "[Latent Semantic Analysis (LSA)]: #3-Latent-Semantic-Analysis-(LSA)\n",
    "[3.1]: #3.1-TF-IDF-Vectorization\n",
    "[3.2]: #3.2-SVD-Modeling-with-Scikit-Learn\n",
    "[Results: Interpration Of Extracted Concepts]: #4-Results:-Interpration-Of-Extracted-Concepts\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<html>\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Data Source</b><a href=\"http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#\">\"Twenty Newsgroups\", Provided By:<b> Scikit-Learn</b></b></a>\n",
    "</div>\n",
    "</html>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Purpose & Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis I will be performing data mining in an effort to extract a series of meaningful and significant concepts from a public dataset of newsgroup postings on the topic of Christianity.\n",
    "\n",
    "The dataset, titled \"Twenty Newsgroups\" and is officially described as follows:\n",
    ">\"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\"\n",
    "\n",
    "A newsgroup is an online public forum for discussion on a particular topic. The topic that I will be extracting data from will be \"Christianity\" (_soc.religion.christian_). I'm very curious to see what the results of this analysis will be, and in concluding intend to share my opinion on them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 What is a \"Latent Semantic Analysis\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Latent Semantic Analysis (LSA)_ is a technique commonly used in the field of Natural Language Processing (NLP). As a computer scientist, when performing NLP we are concerned with the interactions that that exist between computers and human language. A great portion of this field focuses on the analysis of the relationship between multiple words in a document of text contained in a collection of related documents. This is known as the subfield of _Natural Language Understanding_ and can be thought of more simply as \"teaching computers how to read\". \n",
    "\n",
    "LSA is more formally defined by [_\"An Introduction to Latent Semantic Analysis\" by Landauer, Foltz, & Laham_](http://lsa.colorado.edu/papers/dp1.LSAintro.pdf)\n",
    ">\"Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the\n",
    "contextual-usage meaning of words by statistical computations applied to a large corpus of\n",
    "text (Landauer and Dumais, 1997). The underlying idea is that the aggregate of all the word\n",
    "contexts in which a given word does and does not appear provides a set of mutual\n",
    "constraints that largely determines the similarity of meaning of words and sets of words to\n",
    "each other.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Terminology Defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a vast list of new terminoloy defined by the field of NLP. Below I will briefly define those of significance to LSA which will be used extensively throughout this analysis.\n",
    "\n",
    "* **Bag Of Words (BOW)** - An abstraction model in NLP where we consider each document of text to simply be a \"bag of words\" in the literal sense, such that grammar and conceptual meaning is ignored.\n",
    "* **Term Frequency–Inverse Document Frequency (TF-IDF)** - A mathematical calculation for scoring the importance of a word in a document or a collection. This score value is based on _Zipf's Law_ of power distributions.\n",
    "* **Term** - A single word found in a document of text.\n",
    "* **Document** - A single collection of terms. Defined by the LSA study. In this case, each discussion post by a user will be a document.\n",
    "* **Corpus** - A single collection of related documents.\n",
    "* **Concept** - The final output of an LSA is a list of concepts. These are words, or multiple words together, which were found to have the highest significance across our corpus. They are called concepts, because they represent a meaningful 'conceptualization' that has been extracted from the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Process/Procedure & Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In brief, I will summarize a list of 7 steps representing the overall process required to perform an LSA:\n",
    "\n",
    "1. Collect/Retrieve a dataset containing text of interest. \n",
    "2. Define which text in the dataset will be represented as documents (sentences, discussion board poasts, news articles, ?)\n",
    "3. Using the BOW model, parse by document and store words in a BOW where each bag is a document. Ending result should be a collection of documents of terms.\n",
    "4. Clean the data by removing any non-alphanumeric characters such as HTML or XML tagging. Next, remove words that have very high frequency of repetition across the corpus, but with little to no significance. Due to the nature of 'TF-IDF' which relies on the _inverse_ frequency of significant terms across the corpus, this part of the process is not a straightforward one. Instead, by trial and error remove words with caution and sparingly, then re-test the model. This means steps 1-7 are completed, however you then must test and repeat this step possibly several times until the desired output is achieved. \n",
    "5. Perform TF-IDF Vectorization. This scores the words as terms for each document and across the corpus.\n",
    "6. Matrix decomposition using the SVD algorithm.\n",
    "7. Output a list of concepts extracted. \n",
    "\n",
    "Now we can begin our prepartions for LSA, starting with step 1, importing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports, and dataset download via sk-learn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import re\n",
    "\n",
    "categories = ['soc.religion.christian']\n",
    "dataset = fetch_20newsgroups(subset='all',shuffle=True, random_state=42, categories=categories)\n",
    "corpus = dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many documents (forum posts) are in the dataset\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: sciysg@nusunix1.nus.sg (Yung Shing Gene)\\nSubject: Mission Aviation Fellowship\\nOrganization: National University of Singapore\\nLines: 3\\n\\nHi,\\n\\tDoes anyone know anything about this group and what they\\ndo? Any info would be appreciated. Thanks!\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first document\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Print the first 10 documents to inspect the data\\nfor x in range(0,12):\\n    print(corpus[x])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# * Uncomment this block to inspect a sample of 10 documents *\n",
    "\"\"\"\n",
    "# Print the first 10 documents to inspect the data\n",
    "for x in range(0,12):\n",
    "    print(corpus[x])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**  \n",
    "It appears our data is in plain text with no tagging. However, each post starts with a heading which I've noticed is also variable across the corpus. For example some posts start with a header containing \"From\", \"Subject\", and \"Organization\" while others do not. The following headers are present across the corpus:  \n",
    "* **From:** [ _email@emailaddress.com_ ]\n",
    "* **Subject:** [ _topic_ ]\n",
    "* **Reply-To:** [ _email@emailaddress.com_ ]\n",
    "* **Organization:** [ _Organization Name_ ]\n",
    "* **Lines:** [ _# Lines of post_ ]\n",
    "\n",
    "Also, it appears that a post can be from either a public individual or a member of an organization. In either case, posts can also be both new posts or replies to other's posts. Every header ends with \"Lines:\" which tells us the number of lines of text contained in the post message itself.\n",
    "\n",
    "\n",
    "Post content looks like it could be problemsome for LSA if I don't carefully define the stopset of exclusion words. I found that this part of the process consisted of stopset defining and repetitive model testing in order to fine-tune the results.   \n",
    "\n",
    "One thing that I know we are going to want to exclude regardless are e-mail addresses because these items appear across the entire corpus, and therefore will decrease model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: \n",
      "Subject: tongues (read me!)\n",
      "Lines: 8\n",
      "\n",
      "Persons interested in the tongues question are are invited to\n",
      "peruse an essay of mine, obtainable by sending the message\n",
      " GET TONGUES NOTRANS\n",
      " to  or to\n",
      "    \n",
      "\n",
      " Yours,\n",
      " James Kiefer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using regex, find and remove all e-mail addresses in all documents across the entire corpus\n",
    "corpus = [re.sub(r'(\\s)(\\S+\\@\\S+)(\\s)', r'\\1\\3', corpus[x]) for x in range(len(corpus))]\n",
    "\n",
    "# Check it\n",
    "print(corpus[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from: \n",
      "subject: tongues (read me!)\n",
      "lines: 8\n",
      "\n",
      "persons interested in the tongues question are are invited to\n",
      "peruse an essay of mine, obtainable by sending the message\n",
      " get tongues notrans\n",
      " to  or to\n",
      "    \n",
      "\n",
      " yours,\n",
      " james kiefer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert all text to lower-case\n",
    "postDocs = [x.lower() for x in corpus]\n",
    "\n",
    "# Check it\n",
    "print(postDocs[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Defining 'stopwords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have removed all of the email addresses and formatted the text to lower-case, I will define the stopset. \n",
    "\n",
    "A 'stopset' is a list of 'stopwords' which will be excluded from analysis automatically by scikit-learn's vectorization algorithm. For this LSA, I'm going to use a combination of two pre-built lists for the first attempt: a stopset provided by _Natural Language Toolkit(NTLK)_, and one that I found online called the _Terrier stopset_.\n",
    "\n",
    "In order to combine these two, we store them in a 'set' datastructure and perform a 'union' between them removing duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jasonschenck/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import NTLK stopset\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this cell to add new exclusion words to the stopset before and/or after model testing.\n",
    "# Note: Most of the words below were added over the course of numerous output testing efforts.\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "stopset.update(['anyone','exactly','say','ever','article','another','lot','mercury','san','christiansen','dozier','athens','josh','0001','jose','lois',\n",
    "                'perry','department','editorial','etc','would','ago','0358','542','706','30602','nasa','langley','consider',\n",
    "                'phone','bell','nova','gmi','khan0095','budd','28','bud','nj','wkuvx1','bitnet','easteee','holt','gatech',\n",
    "                'carol','howard','len','hampton','va','cs','terrance','acad1','sahs','uth','randerso','larc','gov','whitesbsd',\n",
    "                'nextwork','trol','eeap','apr','r2d2','vbv','n4tmi','wbt','wycliffe','david','paul','ata','hfsi','uk','fidonet',\n",
    "                'jeff','fenholt','indiana','fisher','microsystems','creps','alvin','netcom','andrew','fil','revdak','jr','velasco',\n",
    "                'virgilio','ac','za','hayesstw','risc1','ucs','lee','nicholas','mandock','randal','overacker','larry','bernard',\n",
    "                'elizabeth','dean','seanna','unisa','rose','bryan','bnr','jayne','heath','scott','michael','llo','acs','vela','atterlep',\n",
    "                'lines','petch','carlson','caralv','subject','university','georgia','aisun3','reply-to','organization','hulman','hayes','steve',\n",
    "                'mcovingt','ai','ca','covington','bigelow','eugene','tek','gvg47','chuck','gvg','com','uga','bernadette','rutgers',\n",
    "                'edu','quot','spacing','text','line','none','sans',\n",
    "                'line','title','word', 'neue','johnsd2','rpi','mls','panix','ebay','group','freenet','mark','carleton',\n",
    "                'ncr','cso','uxa','uiuc','bjorn','elsegundoca','mit','koberg','gt7122b','oo','la','microsoft','kuhub','cc','ukans',\n",
    "                'codex','fnal','marka','csd','sapienza','lady','posting','rolfe','joe','jon','tom','fred','ling','siew','wee','matt5',\n",
    "                'lest','bill','wager','oakland','rochester','alan','steele','therefore','todd','aaron','bryce','a888','sledd','stan',\n",
    "                'pretoria','392','commentary','writes','cox','paz','vic','fax','713','703','3729','827','murray','dale','gary','reply','mail','gerry','tx',\n",
    "                'shall','245','shell','box','univ','aa888','probably','traer','bruce','__','___','601','22102','708','632','trei','eggert','amateur','radio','company','houston','lincoln','21','though',\n",
    "                '408','241','9760','02173','617','244','st','203','617','981','2575',])\n",
    "\n",
    "# Potential bible verse references, originally added and then removed from stopset\n",
    "# '44','31','10','11','31','14','21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the Terrier stopset from file, union with existing stopset\n",
    "terrierstopset = open('terrierstopset.txt', 'r').read()\n",
    "stopset = set(stopset).union(set(terrierstopset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this process, I'll be using the TfidfVectorizer() function from the scikit-learn library. This is the part of the LSA that actually converts the words of text that we have collected in to numerical representations by assigning them TF-IDF scores. \n",
    "> _ The TF-IDF score of a word 'w' is:_  \n",
    "> \n",
    "> $$tf(w) * idf(w)$$\n",
    ">\n",
    "> _where: $$tf(w) =\\frac{\\text{number of times a word appears in the doc}}{\\text{total number of words in the doc}}$$_ \n",
    ">\n",
    "> and : $$idf(w)=  \\left\\{log\\frac{\\text{number of documents}}{\\text{number of documents that contain the word w}}\\right\\}$$\n",
    "\n",
    "When we vectorize, we are essentially defining a lexical analyzer that is built into scikit-learn and therefore must specify some important parameters:  \n",
    "\n",
    "* **stopwords:** set the param to var stopset  \n",
    "<br>\n",
    "* **use idf:** always set to true for LSA  \n",
    "<br>\n",
    "* **ngram range:** 'grams' are words, and the ngram_range specifies to the analyzer the minimum(1) to the maximum(N) grams to consider for contextual relationships. I originally started this analysis with ngram_range=(1,3), however found through serveral rounds of testing and fine-tuning that (2,5) tends to produce the most optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the vectorizer model\n",
    "vectorizer = TfidfVectorizer(stop_words=stopset, use_idf=True, ngram_range=(2, 3),smooth_idf=True)\n",
    "\n",
    "# Fit the corpus data\n",
    "X = vectorizer.fit_transform(postDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 266743)\t0.204570131169\n",
      "  (0, 216282)\t0.204570131169\n",
      "  (0, 97598)\t0.204570131169\n",
      "  (0, 155388)\t0.204570131169\n",
      "  (0, 24742)\t0.204570131169\n",
      "  (0, 90498)\t0.204570131169\n",
      "  (0, 160182)\t0.178581506154\n",
      "  (0, 219410)\t0.204570131169\n",
      "  (0, 112298)\t0.178581506154\n",
      "  (0, 131067)\t0.173410345509\n",
      "  (0, 17873)\t0.204570131169\n",
      "  (0, 121142)\t0.204570131169\n",
      "  (0, 19348)\t0.178581506154\n",
      "  (0, 266744)\t0.204570131169\n",
      "  (0, 216283)\t0.204570131169\n",
      "  (0, 97599)\t0.204570131169\n",
      "  (0, 155389)\t0.204570131169\n",
      "  (0, 24743)\t0.204570131169\n",
      "  (0, 90499)\t0.204570131169\n",
      "  (0, 160187)\t0.204570131169\n",
      "  (0, 219411)\t0.204570131169\n",
      "  (0, 112299)\t0.193069981397\n",
      "  (0, 131069)\t0.204570131169\n",
      "  (0, 17874)\t0.204570131169\n",
      "  (0, 121143)\t0.204570131169\n"
     ]
    }
   ],
   "source": [
    "# Tada! This is now the output of the first document in the corpus, in sparse IDF matrix form.\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(997, 266904)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The current shape is (documents, terms)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 SVD Modeling with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single Value Decomposition** (SVD) is the process of taking our corpus of matrices (X), and performing _matrix decomposition_ such that:\n",
    "\n",
    "<big>$$X \\approx USV^{T}$$</big>\n",
    "\n",
    "where...\n",
    "\n",
    "* **X** = Original corpus matrix\n",
    "* **m** = Number of documents contained in X\n",
    "* **n** = Number of terms\n",
    "<br>\n",
    "\n",
    " \n",
    "**_X is decomposed into three matricies called U, S, and T with k-value such that..._**  \n",
    "\n",
    "\n",
    "\n",
    ">* **k** = Number of concepts we want to mine for\n",
    ">\n",
    ">\n",
    ">* **U** = An {'_m x k_'} matrix.  \n",
    ">  * _Rows_ = Documents\n",
    ">  * _Columns_ = Concepts\n",
    ">* **S** = A {'_k x k_'} diagonal matrix. \n",
    ">  * _Elements_ =  Variation captured from each concept.\n",
    ">* **V** = An {'_n x k_'} matrix.\n",
    ">  * _Rows_ = Terms\n",
    ">  * _Columns_ = Concepts\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an advanced mathematical procedure involving linear algebra which will decompose our matrix X into three U,S,& V. The entire process is built-in to scikit-learn as an engine model, all we must do is define the model specifications and let it do the work for us. \n",
    "\n",
    "[**scikit-learn**](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) provides the following documentation on this function:  \n",
    "> \"Dimensionality reduction using truncated SVD (aka LSA).\n",
    "This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently.\n",
    "In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).\n",
    "This estimator supports two algorithms: a fast randomized SVD solver, and a “naive” algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the TruncatedSVD model\n",
    "\n",
    "# Params: n_components=100 for LSA per sk-learn doc, n_iter=5 (default, and should be adjusted during testing) \n",
    "lsa = TruncatedSVD(n_components=100, n_iter=5)\n",
    "\n",
    "# Fit the model\n",
    "lsa.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.18269713e-06,   2.18269714e-06,   2.26244627e-05, ...,\n",
       "         2.05282924e-05,   8.24302588e-05,   8.24302588e-05])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After decomposition, 'lsa.components_[]' represents matrix V'\n",
    "lsa.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "priest priest\n",
      "immaculate conception\n",
      "four years\n",
      "answer priest\n",
      "told priest\n",
      "years old\n",
      "case doctrine\n",
      "apparition deemed\n",
      "apparition deemed true\n",
      "appeared girl\n",
      " \n",
      "Concept 1:\n",
      "grass valley\n",
      "daily verse grass\n",
      "verse grass\n",
      "verse grass valley\n",
      "grass valley grass\n",
      "valley grass\n",
      "valley grass valley\n",
      "daily verse\n",
      "grass valley whoever\n",
      "valley whoever\n",
      " \n",
      "Concept 2:\n",
      "secretary interior\n",
      "appointee james\n",
      "appointee james watt\n",
      "christian think\n",
      "christian think secretary\n",
      "days last\n",
      "days last generation\n",
      "deforestation since\n",
      "deforestation since living\n",
      "discussion rather late\n",
      " \n",
      "Concept 3:\n",
      "married god\n",
      "god eyes\n",
      "married god eyes\n",
      "two people\n",
      "people married god\n",
      "people married\n",
      "two people married\n",
      "become married god\n",
      "couple become\n",
      "couple become married\n",
      " \n",
      "Concept 4:\n",
      "hate sin\n",
      "original sin\n",
      "eternal death\n",
      "god shaped\n",
      "god shaped hole\n",
      "shaped hole\n",
      "spiritual needs\n",
      "commands us\n",
      "love sinner\n",
      "normal humanity\n",
      " \n",
      "Concept 5:\n",
      "hate sin\n",
      "commands us\n",
      "love sinner\n",
      "sin love\n",
      "hate sin love\n",
      "sin love sinner\n",
      "deal sin\n",
      "consistent christianity\n",
      "consistent christianity think\n",
      "question whether statement\n",
      " \n",
      "Concept 6:\n",
      "eternal death\n",
      "hate sin\n",
      "christians hell\n",
      "atheists hell\n",
      "atheists believe\n",
      "bible problem\n",
      "bible problem view\n",
      "bible talks fires\n",
      "interpreters bible\n",
      "interpreters bible problem\n",
      " \n",
      "Concept 7:\n",
      "eternal death\n",
      "christianity compatible\n",
      "homosexuality issues\n",
      "issues christianity\n",
      "christianity compatible check\n",
      "compatible check\n",
      "homosexuality issues christianity\n",
      "gay christians\n",
      "18 22\n",
      "lev 18 22\n",
      " \n",
      "Concept 8:\n",
      "original sin\n",
      "doctrine original\n",
      "doctrine original sin\n",
      "enter heaven\n",
      "one enter\n",
      "born water\n",
      "born water spirit\n",
      "water spirit\n",
      "capable comprehending\n",
      "babies supposed\n",
      " \n",
      "Concept 9:\n",
      "absolute truth\n",
      "scripture truths\n",
      "truths absolutes\n",
      "absolutes answer\n",
      "truths absolutes answer\n",
      "absolutes scripture\n",
      "always true\n",
      "contradiction terms\n",
      "arrogance christians\n",
      "truth absolute\n",
      " \n",
      "Concept 10:\n",
      "normal humanity\n",
      "mary assumption\n",
      "appears places\n",
      "appears places mary\n",
      "beyond sanctification\n",
      "beyond sanctification normal\n",
      "greeted mary\n",
      "mary beyond\n",
      "places mary\n",
      "places mary beyond\n",
      " \n",
      "Concept 11:\n",
      "kicked heaven\n",
      "heaven biblical\n",
      "kicked heaven biblical\n",
      "satan kicked\n",
      "satan kicked heaven\n",
      "god authority\n",
      "angel god kicked\n",
      "authority problem\n",
      "authority problem cannot\n",
      "bible bible originate\n",
      " \n",
      "Concept 12:\n",
      "cultural interference\n",
      "ideological manipulation\n",
      "ideological manipulation cultural\n",
      "manipulation cultural\n",
      "manipulation cultural interference\n",
      "soc religion\n",
      "number translators\n",
      "concerned recent\n",
      "concerned recent sil\n",
      "denounced right\n",
      " \n",
      "Concept 13:\n",
      "body jesus\n",
      "body stolen\n",
      "dead body\n",
      "11 51\n",
      "luke 11\n",
      "luke 11 51\n",
      "really rise\n",
      "jewish roman\n",
      "men women\n",
      "10 31 explained\n",
      " \n",
      "Concept 14:\n",
      "murphy law\n",
      "body jesus\n",
      "body stolen\n",
      "jewish roman\n",
      "really rise\n",
      "authorities gained\n",
      "authorities gained discredited\n",
      "body jesus even\n",
      "dead body jesus\n",
      "discredited christians\n",
      " \n",
      "Concept 15:\n",
      "saved faith\n",
      "lukewarm christian\n",
      "faith alone\n",
      "faith without\n",
      "faith deeds\n",
      "faith without deeds\n",
      "without deeds\n",
      "battling problem\n",
      "battling problem know\n",
      "believing enough\n",
      " \n",
      "Concept 16:\n",
      "murphy law\n",
      "according purpose\n",
      "according purpose murphy\n",
      "amplifications murphy\n",
      "amplifications murphy law\n",
      "anything go\n",
      "anything go wrong\n",
      "appropriate humor\n",
      "appropriate humor contradicted\n",
      "called according\n",
      " \n",
      "Concept 17:\n",
      "knows everything\n",
      "catholic doctrine predestination\n",
      "god knows everything\n",
      "knows everything knows\n",
      "since god knows\n",
      "doctrine predestination\n",
      "everything knows\n",
      "since god\n",
      "god knows\n",
      "catholic doctrine\n",
      " \n",
      "Concept 18:\n",
      "saved faith\n",
      "never committed\n",
      "sin whole\n",
      "sin whole life\n",
      "lukewarm christian\n",
      "whole life\n",
      "faith without\n",
      "faith alone\n",
      "fully human\n",
      "faith deeds\n",
      " \n",
      "Concept 19:\n",
      "black sabbath\n",
      "may wrong\n",
      "hell_2 black\n",
      "hell_2 black sabbath\n",
      "may wrong part\n",
      "part black\n",
      "part black sabbath\n",
      "wrong part\n",
      "wrong part black\n",
      "black sabbath major\n",
      " \n",
      "Concept 20:\n",
      "go hell\n",
      "god judge\n",
      "go heaven\n",
      "believe christian god\n",
      "believe christian\n",
      "christian god\n",
      "going hell\n",
      "ask opinions\n",
      "assume lumped\n",
      "assume lumped category\n",
      " \n",
      "Concept 21:\n",
      "exist must\n",
      "enduring values\n",
      "views christianity\n",
      "assume god\n",
      "universe exist\n",
      "atheist views\n",
      "atheist views christianity\n",
      "christianity accepting\n",
      "christianity accepting jeesus\n",
      "views christianity accepting\n",
      " \n",
      "Concept 22:\n",
      "spirit filled\n",
      "congregations christians\n",
      "filled believers\n",
      "spirit filled believers\n",
      "visit congregations\n",
      "visit congregations christians\n",
      "maybe trying\n",
      "christians happen\n",
      "christians happen homosexuals\n",
      "congregations christians happen\n",
      " \n",
      "Concept 23:\n",
      "gifted one\n",
      "satanic tounges\n",
      "spirit talking\n",
      "witness real\n",
      "modern day\n",
      "speaking tongues\n",
      "different language\n",
      "angelic tongue\n",
      "god punished\n",
      "audience understand\n",
      " \n",
      "Concept 24:\n",
      "knew rules\n",
      "medieval period\n",
      "soc religion\n",
      "aquinas day\n",
      "ancient books\n",
      "former atheists\n",
      "10th cent\n",
      "10th cent aquinas\n",
      "anxious see\n",
      "anxious see cases\n",
      " \n",
      "Concept 25:\n",
      "mason beaten\n",
      "parents mason\n",
      "parents mason beaten\n",
      "christian parents\n",
      "fundamentalist christian\n",
      "fundamentalist christian parents\n",
      "enter heaven\n",
      "strict fundamentalist\n",
      "original sin\n",
      "sj reference\n",
      " \n",
      "Concept 26:\n",
      "mason beaten\n",
      "parents mason\n",
      "parents mason beaten\n",
      "christian parents\n",
      "fundamentalist christian\n",
      "fundamentalist christian parents\n",
      "strict fundamentalist\n",
      "sj reference\n",
      "beaten child\n",
      "beaten face\n",
      " \n",
      "Concept 27:\n",
      "work god\n",
      "genocide work\n",
      "genocide work god\n",
      "serbian genocide\n",
      "serbian genocide work\n",
      "god hmm\n",
      "serbs work\n",
      "serbs work god\n",
      "work god hmm\n",
      "cleansing serbs\n",
      " \n",
      "Concept 28:\n",
      "research center\n",
      "affirmation mcc\n",
      "affirmation mcc churches\n",
      "check dignity\n",
      "check dignity integrity\n",
      "christians find pray\n",
      "churches affirmation\n",
      "churches affirmation mcc\n",
      "churches meet\n",
      "churches meet gay\n",
      " \n",
      "Concept 29:\n",
      "south africa\n",
      "virgin mary\n",
      "arrogance christians\n",
      "blood transfusion\n",
      "missiology south\n",
      "missiology south africa\n",
      "africa internet\n",
      "africa south\n",
      "africa south africa\n",
      "south africa internet\n",
      " \n",
      "Concept 30:\n",
      "soc religion\n",
      "active liberals\n",
      "active liberals catholics\n",
      "agers athiests\n",
      "agers athiests someone\n",
      "apparent primarily\n",
      "apparent primarily active\n",
      "athiests someone\n",
      "athiests someone might\n",
      "become apparent\n",
      " \n",
      "Concept 31:\n",
      "proof resurection\n",
      "ezekiel 18\n",
      "christ captialist\n",
      "following christ captialist\n",
      "obedience gensis\n",
      "strict obedience\n",
      "strict obedience gensis\n",
      "following christ\n",
      "religious wars\n",
      "accordance allow\n",
      " \n",
      "Concept 32:\n",
      "english translation\n",
      "acts apostles\n",
      "differences long\n",
      "greek nt\n",
      "long recension\n",
      "readings included\n",
      "vaticanus siniaticus\n",
      "recension acts\n",
      "variants nt\n",
      "1440 variant\n",
      " \n",
      "Concept 33:\n",
      "never achieve\n",
      "goal never\n",
      "goal never achieve\n",
      "ezekiel 18\n",
      "english translation\n",
      "achieve know\n",
      "achieve know saved\n",
      "anything think james\n",
      "anyways christians\n",
      "anyways christians know\n",
      " \n",
      "Concept 34:\n",
      "ezekiel 18\n",
      "environmentalism paganism\n",
      "share guilt\n",
      "_bashing_ paganism\n",
      "_bashing_ paganism figuring\n",
      "answer pagans\n",
      "answer pagans right\n",
      "bit less\n",
      "bit less effort\n",
      "christ answer pagans\n",
      " \n",
      "Concept 35:\n",
      "become atheists\n",
      "people become\n",
      "people become atheists\n",
      "evidence senses\n",
      "science reason\n",
      "fair god\n",
      "prove anything\n",
      "physical sphere\n",
      "never achieve\n",
      "physical senses\n",
      " \n",
      "Concept 36:\n",
      "virgin mary\n",
      "never achieve\n",
      "question virgin\n",
      "question virgin mary\n",
      "goal never\n",
      "goal never achieve\n",
      "environmentalism paganism\n",
      "_bashing_ paganism\n",
      "_bashing_ paganism figuring\n",
      "answer pagans\n",
      " \n",
      "Concept 37:\n",
      "become atheists\n",
      "people become\n",
      "people become atheists\n",
      "atheist prayer\n",
      "proven wrong\n",
      "michigan hospitals\n",
      "maxwell muir\n",
      "christian practices\n",
      "parallel mormon\n",
      "atheism says\n",
      " \n",
      "Concept 38:\n",
      "enter heaven\n",
      "cannot enter heaven\n",
      "cannot enter\n",
      "become atheists\n",
      "people become\n",
      "people become atheists\n",
      "jesus come\n",
      "asking jesus\n",
      "asking jesus come\n",
      "come heart\n",
      " \n",
      "Concept 39:\n",
      "virgin mary\n",
      "question virgin\n",
      "question virgin mary\n",
      "although bodily\n",
      "although bodily assumption\n",
      "assumption basis\n",
      "assumption basis bible\n",
      "basis bible\n",
      "basis bible carl\n",
      "bible carl\n",
      " \n",
      "Concept 40:\n",
      "christian practices\n",
      "parallel mormon\n",
      "mormon ceremonies\n",
      "practices parallel\n",
      "early christian\n",
      "mormon temples\n",
      "christian practices parallel\n",
      "early christian practices\n",
      "mormon temple ceremonies\n",
      "parallel mormon temple\n",
      " \n",
      "Concept 41:\n",
      "especially christianity\n",
      "christianity nothing drug\n",
      "christians inject\n",
      "drugs escape\n",
      "drugs escape reality\n",
      "escape reality\n",
      "especially christianity nothing\n",
      "live high\n",
      "nothing drug\n",
      "people use drugs\n",
      " \n",
      "Concept 42:\n",
      "cell church\n",
      "new christian\n",
      "like ask\n",
      "questions new\n",
      "questions new christian\n",
      "steven hoskins\n",
      "ask recommend\n",
      "ask recommend good\n",
      "good reading\n",
      "good reading list\n",
      " \n",
      "Concept 43:\n",
      "death penalty\n",
      "capital punishment\n",
      "christian trait\n",
      "temper christian\n",
      "temper christian trait\n",
      "foolish foolish\n",
      "keep peace\n",
      "losing temper\n",
      "losing temper christian\n",
      "revealed truth\n",
      " \n",
      "Concept 44:\n",
      "christian trait\n",
      "temper christian\n",
      "temper christian trait\n",
      "foolish foolish\n",
      "losing temper\n",
      "losing temper christian\n",
      "acrid angry\n",
      "acrid angry sarcastic\n",
      "angry sarcastic\n",
      "replies acrid\n",
      " \n",
      "Concept 45:\n",
      "israeli government\n",
      "although israeli\n",
      "although israeli government\n",
      "appears nothing\n",
      "appears nothing stands\n",
      "brothers sisters time\n",
      "days although\n",
      "days although israeli\n",
      "drawing near look\n",
      "give permission start\n",
      " \n",
      "Concept 46:\n",
      "death penalty\n",
      "capital punishment\n",
      "keep peace\n",
      "death penalty revenge\n",
      "penalty revenge\n",
      "try refute\n",
      "non violent\n",
      "non violent provisions\n",
      "violent provisions\n",
      "luke account\n",
      " \n",
      "Concept 47:\n",
      "luke account\n",
      "really rise\n",
      "believing resurrection\n",
      "resurrection one\n",
      "belief resurrection\n",
      "account highly\n",
      "account highly suspect\n",
      "argument total\n",
      "argument total complete\n",
      "basic problem argument\n",
      " \n",
      "Concept 48:\n",
      "capital punishment\n",
      "darin johnson\n",
      "sex christianity\n",
      "gay christians\n",
      "churches remind\n",
      "gay churches\n",
      "gay churches remind\n",
      "christians sex\n",
      "christians sex christianity\n",
      "cell church\n",
      " \n",
      "Concept 49:\n",
      "new testament\n",
      "homosexual intercourse\n",
      "god one set\n",
      "one set\n",
      "one set rules\n",
      "set rules\n",
      "pa ques\n",
      "god one\n",
      "eternal marriage\n",
      "14 19\n",
      " \n",
      "Concept 50:\n",
      "spiritual needs\n",
      "northern research\n",
      "12 step\n",
      "recovery programs\n",
      "catholic liturgy\n",
      "capital punishment\n",
      "palm sunday\n",
      "peoples spiritual\n",
      "peoples spiritual needs\n",
      "abuse recovery\n",
      " \n",
      "Concept 51:\n",
      "spiritual needs\n",
      "eternal marriage\n",
      "given marriage\n",
      "marry given\n",
      "marry given marriage\n",
      "neither marry\n",
      "neither marry given\n",
      "given marriage luke\n",
      "marriage luke\n",
      "chapt 20\n",
      " \n",
      "Concept 52:\n",
      "cell church\n",
      "god one set\n",
      "one set\n",
      "one set rules\n",
      "set rules\n",
      "god one\n",
      "ceremonial moral\n",
      "moral laws\n",
      "dreams oobes\n",
      "14 19\n",
      " \n",
      "Concept 53:\n",
      "cell church\n",
      "catholic liturgy\n",
      "palm sunday\n",
      "cell church discussion\n",
      "church discussion\n",
      "new things\n",
      "cell churches\n",
      "second coming\n",
      "quality catholic\n",
      "quality catholic liturgy\n",
      " \n",
      "Concept 54:\n",
      "cell church\n",
      "spiritual needs\n",
      "cell church discussion\n",
      "church discussion\n",
      "cell churches\n",
      "northern research\n",
      "12 step\n",
      "joseph cell\n",
      "joseph cell church\n",
      "beginning discussion\n",
      " \n",
      "Concept 55:\n",
      "dreams oobes\n",
      "body incidents\n",
      "dreams body\n",
      "dreams body incidents\n",
      "ethics apply\n",
      "different moral\n",
      "death penalty\n",
      "morally responsible\n",
      "morality applies\n",
      "normal dreams\n",
      " \n",
      "Concept 56:\n",
      "spiritual needs\n",
      "virgin mary\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "catholic doctrine\n",
      "biblical support\n",
      "john baptist\n",
      "repeated lives\n",
      "1950 certainly\n",
      " \n",
      "Concept 57:\n",
      "cell church\n",
      "pa ques\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "easter name\n",
      "easter name new\n",
      "name new testament\n",
      "new testament double\n",
      "testament double\n",
      " \n",
      "Concept 58:\n",
      "dreams oobes\n",
      "body incidents\n",
      "dreams body\n",
      "dreams body incidents\n",
      "different moral\n",
      "ethics apply\n",
      "morally responsible\n",
      "morality applies\n",
      "god shaped\n",
      "normal dreams\n",
      " \n",
      "Concept 59:\n",
      "second coming\n",
      "repeated lives\n",
      "john baptist\n",
      "elijah come\n",
      "christianity repeated\n",
      "christianity repeated lives\n",
      "apostles ask\n",
      "apostles ask pharisees\n",
      "apostles perceived\n",
      "ask pharisees\n",
      " \n",
      "Concept 60:\n",
      "source existence\n",
      "capital punishment\n",
      "second coming\n",
      "repeated lives\n",
      "things exist\n",
      "14 19\n",
      "elijah come\n",
      "john baptist\n",
      "jason smith\n",
      "oulu fi\n",
      " \n",
      "Concept 61:\n",
      "second coming\n",
      "much later\n",
      "dreams oobes\n",
      "koresh second\n",
      "koresh second coming\n",
      "become atheists\n",
      "church added\n",
      "could choose\n",
      "believed god\n",
      "deutero canonical\n",
      " \n",
      "Concept 62:\n",
      "much later\n",
      "god shaped\n",
      "catholic church\n",
      "church added\n",
      "canonical books\n",
      "deutero canonical\n",
      "spiritual quality\n",
      "church history\n",
      "books added\n",
      "whoah whoah\n",
      " \n",
      "Concept 63:\n",
      "south africa\n",
      "eternity hell hell\n",
      "eternity hell\n",
      "hell theory\n",
      "annihilation however\n",
      "annihilation however minority\n",
      "denominations know\n",
      "denominations know hold\n",
      "descriptions lake\n",
      "descriptions lake fire\n",
      " \n",
      "Concept 64:\n",
      "second coming\n",
      "blood transfusion\n",
      "capital punishment\n",
      "brain washed\n",
      "indoctrinated parents\n",
      "become christian\n",
      "believe predestination\n",
      "become christian indoctrinated\n",
      "christian indoctrinated\n",
      "christian indoctrinated parents\n",
      " \n",
      "Concept 65:\n",
      "eternity hell hell\n",
      "eternity hell\n",
      "cell church\n",
      "hell theory\n",
      "hell hell\n",
      "annihilation however\n",
      "annihilation however minority\n",
      "denominations know\n",
      "denominations know hold\n",
      "descriptions lake\n",
      " \n",
      "Concept 66:\n",
      "put hell\n",
      "blood transfusion\n",
      "much later\n",
      "27 babies\n",
      "27 babies born\n",
      "babies born state\n",
      "born state\n",
      "born state die\n",
      "cuf god\n",
      "cuf god put\n",
      " \n",
      "Concept 67:\n",
      "cell church\n",
      "pregnancy rates\n",
      "online bible\n",
      "gifted one\n",
      "never heard\n",
      "abstinence education\n",
      "non liberal\n",
      "sex education\n",
      "blood transfusion\n",
      "true heaven\n",
      " \n",
      "Concept 68:\n",
      "second coming\n",
      "immaculate conception\n",
      "cell church\n",
      "branch davidians\n",
      "claim son\n",
      "claim son god\n",
      "even claim\n",
      "even claim son\n",
      "faith mere\n",
      "faith mere man\n",
      " \n",
      "Concept 69:\n",
      "pregnancy rates\n",
      "abstinence education\n",
      "sex education\n",
      "non liberal\n",
      "holy spirit\n",
      "father son\n",
      "daily verse\n",
      "abstinence related\n",
      "abstinence related curricula\n",
      "related curricula\n",
      " \n",
      "Concept 70:\n",
      "expression mercy\n",
      "spiritual needs\n",
      "able find source\n",
      "agony one\n",
      "agony one achieve\n",
      "among others able\n",
      "amount agony\n",
      "amount agony one\n",
      "amount separation\n",
      "amount separation hence\n",
      " \n",
      "Concept 71:\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "michigan hospitals\n",
      "supreme court\n",
      "edgar pearlstein\n",
      "online bible\n",
      "cases involving\n",
      "cases involving religion\n",
      "come court\n",
      " \n",
      "Concept 72:\n",
      "god shaped\n",
      "cell church\n",
      "pregnancy rates\n",
      "husband wife\n",
      "god shaped hole\n",
      "shaped hole\n",
      "sex education\n",
      "abstinence education\n",
      "wife husband\n",
      "natural disaster\n",
      " \n",
      "Concept 73:\n",
      "14 19\n",
      "ten commandments\n",
      "definition religion\n",
      "legal definition\n",
      "legal definition religion\n",
      "weak faith\n",
      "first day\n",
      "seventh day\n",
      "natural disaster\n",
      "christian pauline\n",
      " \n",
      "Concept 74:\n",
      "expression mercy\n",
      "blood transfusion\n",
      "artificial intelligence\n",
      "artificial intelligence programs\n",
      "intelligence programs\n",
      "find source\n",
      "jewish proselytism\n",
      "meaning importance\n",
      "source even\n",
      "able find source\n",
      " \n",
      "Concept 75:\n",
      "god shaped\n",
      "second coming\n",
      "god shaped hole\n",
      "shaped hole\n",
      "put hell\n",
      "private interpretation\n",
      "free decide\n",
      "every language\n",
      "27 babies\n",
      "27 babies born\n",
      " \n",
      "Concept 76:\n",
      "physical body\n",
      "rich man\n",
      "death penalty revenge\n",
      "penalty revenge\n",
      "try refute\n",
      "believe going\n",
      "death penalty\n",
      "new revelation\n",
      "thought god\n",
      "god told\n",
      " \n",
      "Concept 77:\n",
      "immaculate conception\n",
      "expression mercy\n",
      "jewish proselytism\n",
      "also read\n",
      "16 peter\n",
      "16 peter warns\n",
      "also read peter\n",
      "hard understand learned\n",
      "often hard understand\n",
      "peter 16\n",
      " \n",
      "Concept 78:\n",
      "certain without\n",
      "shadow doubt\n",
      "sayeth lord\n",
      "thus sayeth\n",
      "thus sayeth lord\n",
      "genesis 15\n",
      "second coming\n",
      "immaculate conception\n",
      "every language\n",
      "bruise heel\n",
      " \n",
      "Concept 79:\n",
      "second coming\n",
      "immaculate conception\n",
      "genesis 15\n",
      "also read\n",
      "meaning importance\n",
      "private revelation\n",
      "new york\n",
      "billion years\n",
      "war hell\n",
      "us ability\n",
      " \n",
      "Concept 80:\n",
      "pregnancy rates\n",
      "abstinence education\n",
      "sex education\n",
      "non liberal\n",
      "blood transfusion\n",
      "physical body\n",
      "abstinence related\n",
      "abstinence related curricula\n",
      "related curricula\n",
      "genesis 15\n",
      " \n",
      "Concept 81:\n",
      "dead sea\n",
      "words book\n",
      "dead sea scrolls\n",
      "sea scrolls\n",
      "private interpretation\n",
      "jesus christ\n",
      "almost certainly\n",
      "new thought deuterocanonicals\n",
      "thought deuterocanonicals\n",
      "physical body\n",
      " \n",
      "Concept 82:\n",
      "holy spirit\n",
      "father son\n",
      "crystal clear\n",
      "people aids\n",
      "god shaped\n",
      "fundamental principles\n",
      "son holy\n",
      "son holy spirit\n",
      "father son holy\n",
      "ted kalivoda\n",
      " \n",
      "Concept 83:\n",
      "sheila patterson\n",
      "war hell\n",
      "drew circle\n",
      "free decide\n",
      "al temple\n",
      "al temple think\n",
      "anger part\n",
      "anger part human\n",
      "changers et\n",
      "changers et al\n",
      " \n",
      "Concept 84:\n",
      "dead sea\n",
      "online bible\n",
      "every language\n",
      "dead sea scrolls\n",
      "sea scrolls\n",
      "never heard\n",
      "bible available\n",
      "south africa\n",
      "crystal clear\n",
      "bible software\n",
      " \n",
      "Concept 85:\n",
      "natural disaster\n",
      "disaster judgement\n",
      "natural disaster judgement\n",
      "portland earthquake\n",
      "pregnancy rates\n",
      "book jeremiah\n",
      "abstinence education\n",
      "quick look\n",
      "sex education\n",
      "new zealand\n",
      " \n",
      "Concept 86:\n",
      "also read\n",
      "16 peter\n",
      "16 peter warns\n",
      "also read peter\n",
      "hard understand learned\n",
      "often hard understand\n",
      "peter 16\n",
      "peter 16 peter\n",
      "peter warns\n",
      "peter warns scriptures\n",
      " \n",
      "Concept 87:\n",
      "genesis 15\n",
      "bruise heel\n",
      "crush head\n",
      "crush head bruise\n",
      "head bruise\n",
      "head bruise heel\n",
      "jesus christ\n",
      "also read\n",
      "full grace\n",
      "clearly masculine\n",
      " \n",
      "Concept 88:\n",
      "every language\n",
      "immaculate conception\n",
      "according folly\n",
      "answer fool\n",
      "answer fool according\n",
      "fool according\n",
      "fool according folly\n",
      "rick granberry\n",
      "bible available\n",
      "exist three\n",
      " \n",
      "Concept 89:\n",
      "rich man\n",
      "ethnic cleansing\n",
      "devil angels\n",
      "eternal fire\n",
      "crystal clear\n",
      "capital punishment\n",
      "matthew 25\n",
      "thought god\n",
      "fundamental principles\n",
      "book called\n",
      " \n",
      "Concept 90:\n",
      "second coming\n",
      "free decide\n",
      "definition christianity\n",
      "16 peter\n",
      "16 peter warns\n",
      "also read peter\n",
      "hard understand learned\n",
      "often hard understand\n",
      "peter 16\n",
      "peter 16 peter\n",
      " \n",
      "Concept 91:\n",
      "private interpretation\n",
      "holy spirit\n",
      "true heaven\n",
      "peter 20\n",
      "_christianity crisis_ hank\n",
      "book feelings\n",
      "crisis_ hank\n",
      "crisis_ hank hanegraaff\n",
      "important book\n",
      "important book feelings\n",
      " \n",
      "Concept 92:\n",
      "dead sea\n",
      "question authority\n",
      "dead sea scrolls\n",
      "sea scrolls\n",
      "words book\n",
      "bible specifically\n",
      "resurrection sunday\n",
      "cell church\n",
      "pagan goddess\n",
      "genesis 15\n",
      " \n",
      "Concept 93:\n",
      "online bible\n",
      "never heard\n",
      "private revelation\n",
      "bible software\n",
      "online bible software\n",
      "people new\n",
      "people new testament\n",
      "dead sea\n",
      "blood transfusion\n",
      "true heaven\n",
      " \n",
      "Concept 94:\n",
      "god love\n",
      "immaculate conception\n",
      "jersey institute\n",
      "jersey institute technology\n",
      "new jersey institute\n",
      "new jersey\n",
      "online bible\n",
      "according folly\n",
      "answer fool\n",
      "answer fool according\n",
      " \n",
      "Concept 95:\n",
      "private interpretation\n",
      "hank hanegraaff\n",
      "_christianity crisis_ hank\n",
      "book feelings\n",
      "crisis_ hank\n",
      "crisis_ hank hanegraaff\n",
      "important book\n",
      "important book feelings\n",
      "read important\n",
      "read important book\n",
      " \n",
      "Concept 96:\n",
      "definition christianity\n",
      "homosexuality issues christiani\n",
      "issues christiani\n",
      "eternal life\n",
      "catholic church\n",
      "immaculate conception\n",
      "dead sea\n",
      "see way\n",
      "believe going\n",
      "interpret verses\n",
      " \n",
      "Concept 97:\n",
      "jesus name\n",
      "private interpretation\n",
      "peter 20\n",
      "prayer jesus\n",
      "prayer jesus name\n",
      "seventh day\n",
      "ted kalivoda\n",
      "book called\n",
      "interpretation god\n",
      "jacob esau\n",
      " \n",
      "Concept 98:\n",
      "holy spirit\n",
      "father son\n",
      "genesis 15\n",
      "son holy\n",
      "son holy spirit\n",
      "bruise heel\n",
      "crush head\n",
      "crush head bruise\n",
      "head bruise\n",
      "head bruise heel\n",
      " \n",
      "Concept 99:\n",
      "jesus christ\n",
      "sheila patterson\n",
      "every language\n",
      "jesus name\n",
      "absolute binding\n",
      "absolute binding others\n",
      "beliefs true\n",
      "beliefs true absolute\n",
      "binding others\n",
      "specific beliefs\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Convert the SVD results from numerical representation, back to their appropriate word text form.\n",
    "# Iterates over the enumeration of matrix components, for each: zips the terms to components, sorts them, then prints. \n",
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print(\"Concept %d:\" % i )\n",
    "    for term in sortedTerms:\n",
    "        print(term[0])\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Results: Interpration Of Extracted Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**  \n",
    "In order to produce the above output, it took several attempts of fine-tuning the stopset list, vectorization parameters, and SVD parameters then re-running the model. During this process I found that stopset word selection can be tricky, because only those terms which repeat the most across the entire corpus should be excluded. If one unknowingly removes a term which is sparsely found in the corpus, then the efficiency of the model is reduced negatively, impacting both performance and the output of concepts. \n",
    "\n",
    "After trying a handful of different variations, I found the following parameters produce the most meaningful extraction of concepts:  \n",
    "- TfidfVectorizer(stop_words=stopset,use_idf=True, **ngram_range=(2, 5**)\n",
    "- TruncatedSVD(n_components=100, **n_iter=5**)\n",
    "\n",
    "Other configurations tested include ngram_range(1,3), (2,2), (3,3), and (1,4). For ngrams < 2 the results lacked substance and returned only very simple concepts such as: God, sin, hate, and love. As the ngrams_range was increased the resulting concepts became much more intricate and meaningful. I also ran a few different configurations with different values for n_iter (epochs), and noticed that this significantly affected the runtime efficiency of the model for any values ~n_iter > 30. I tested n_iter=100, while it took well over 3 minutes to complete execution, the resulting concepts did not appear to have improved much, if at all. \n",
    "\n",
    "The exclusion words were updated several times as well with each test ran, and mostly what I found was that removing certain terms, about 4-5 at a time, then re-testing the model proved successful in the long-run. Specifically the output concepts were checked for terms which appeared out of place, and just 'odd', and then added to the stopset.\n",
    "\n",
    "An important observation made, was that of certain numbers that repeated as concept output. This was super tricky to filter for, as some were extremely significant actually representing bible verses that fit perfectly to the concept (ex: lev 18 22), while others were junk such as the following three numbers:'706','542','0358', which is actually the telephone number for the A.I. department at Georgia Tech! (_If you see a number produced as part of a concept, Google that number to find the bible verse. It proves to be very significant._)  \n",
    "\n",
    "**Interesting Findings**  \n",
    "Christianity is a topic that I am not personally very familiar with, which is in part why I chose it for this study. I wanted to see if I could extract concepts that were very clear to even an observer who is unknowledgeable on  the topic such as myself. \n",
    "\n",
    "I performed some research on a few of the more interesting concepts and ended up with some pretty awesome discoveries:\n",
    "\n",
    "- Ideological Manipulation ([Wikipedia](https://en.wikipedia.org/wiki/Dominant_ideology)):\n",
    ">  \"Social control exercised and effected by means of the _ideological manipulation_ of aspects of the common culture of a society — religion and politics, culture and economy, etc. — to explain and justify the status quo to the political advantage of the dominant (ruling) class...\"  \n",
    "\n",
    "- James G. Watt ([Wikipedia](https://en.wikipedia.org/wiki/James_G._Watt)):\n",
    "> \"James Gaius Watt (born January 31, 1938) served as U.S. Secretary of the Interior from 1981 to 1983. Often described as \"anti-environmentalist\", he was one of Ronald Reagan's most controversial cabinet appointments.\"\n",
    ">\n",
    "> \"In 1995, Watt was indicted on 25 counts of felony perjury and obstruction of justice by a federal grand jury, accused of making false statements before the grand jury investigating influence peddling at the Department of Housing and Urban Development, which he had lobbied in the 1980s\"\n",
    "\n",
    "- Speaking In Tongues ([Wikipedia](https://en.wikipedia.org/wiki/Glossolalia)):\n",
    "> \"Glossolalia or speaking in tongues, according to linguists, is the fluid vocalizing of speech-like syllables that lack any readily comprehended meaning, in some cases as part of religious practice in which it is believed to be a divine language unknown to the speaker.\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
