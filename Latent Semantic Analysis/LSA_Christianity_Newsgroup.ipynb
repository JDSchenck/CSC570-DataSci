{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis:\n",
    "## _Data Mining for Meaningful Concepts In Christianity Newsgroups_\n",
    "---\n",
    "\n",
    "Prepared By: Jason Schenck  \n",
    "Date: February 6th 2017  \n",
    "CSC-570 Data Science Essentials\n",
    "\n",
    "\n",
    "<br>\n",
    "<big>Table Of Contents</big>\n",
    "\n",
    "---\n",
    "* **[1 Introduction][Introduction]**\n",
    "   * [1.1][1.1] _Purpose & Data Source_\n",
    "   * [1.2][1.2] _What is a \"Latent Semantic Analysis\"(LSA)?_\n",
    "   * [1.3][1.3] _Terminology Defined_\n",
    "   * [1.4][1.4] _Process/Procedure & Methodology_\n",
    "\n",
    "\n",
    "* **[2 Data Preparation][Data Preparation]**\n",
    "   * [2.1][2.1] _Data Retrieval_\n",
    "   * [2.2][2.2] _Data Inspection_\n",
    "   * [2.3][2.3] _Defining 'stopwords'_\n",
    "\n",
    "\n",
    "* **[3 TF-IDF Vectorization][TF-IDF Vectorization]**\n",
    "   * [3.1][3.1] _Overview: Vectorizing_\n",
    "   * [3.2][3.2] _TF-IDF Vectorization with Scikit-Learn_\n",
    "     * [3.2.1][3.2.1] Function & Syntax Documentation\n",
    "     * [3.2.2][3.2.2] Parameters\n",
    "\n",
    "\n",
    "* **[4 Lexical Semantic Analysis (LSA)][Lexical Semantic Analysis (LSA)]**\n",
    "   * [4.1][4.1] _Overview: Theory & Practice_\n",
    "   * [4.2][4.2] _Mathematics: Singular Value Decomposition (SVD)_\n",
    "   * [4.3][4.3] _SVD Modeling with Scikit-Learn_\n",
    "     * [4.3.1][4.3.1] Function & Syntax Documentation\n",
    "     * [4.3.2][4.3.2] Parameters\n",
    "   * [4.4][4.4] _Producing A Meaningful Output Of Concepts_\n",
    "     * [4.4.1][4.4.1] TruncatedSVD() Output\n",
    "     * [4.4.2][4.4.2] Converting Document Matrices to Concepts\n",
    "\n",
    "\n",
    "* **[5 Results: Interpration Of Extracted Concepts][Results: Interpration Of Extracted Concepts]**\n",
    "    * [5.1][5.1] _Output_\n",
    "    * [5.2][5.2] _Observations & Opinions_\n",
    "\n",
    "\n",
    "     \n",
    "[Introduction]: #1-Introduction\n",
    "[1.1]: #1.1-Purpose-&-Data-Source\n",
    "[1.2]: #1.2-What-is-a-\"Latent-Semantic-Analysis\"(LSA)?\n",
    "[1.3]: #1.3-Terminology-Defined\n",
    "[1.4]: #1.4-Process/Procedure-&-Methodology\n",
    "[Data Preparation]: #2-Data-Preparation\n",
    "[2.1]: #2.1-Data-Retrieval\n",
    "[2.2]: #2.2-Data-Inspection\n",
    "[2.3]: #2.3-Defining-'stopwords'\n",
    "[TF-IDF Vectorization]: #3-TF-IDF-Vectorization\n",
    "[3.1]: #3.1-Overiview:-Vectorizing\n",
    "[3.2]: #3.2-TF-IDF-Vectorization-with-Scikit-Learn\n",
    "[3.2.1]: #3.2.1-Function-&-Syntax-Documentation\n",
    "[3.2.2]: #3.2.2-Parameters\n",
    "[Lexical Semantic Analysis (LSA)]: #4-4-Lexical-Semantic-Analysis-(LSA)\n",
    "[4.1]: #4.1-Overview:-Theory-&-Practice\n",
    "[4.2]: #4.2-Mathematics:-Singular-Value-Decomposition-(SVD)\n",
    "[4.3]: #4.3-SVD-Modeling-with-Scikit-Learn\n",
    "\n",
    "[4.3.1]: #4.3.1-Function-&-Syntax-Documentation\n",
    "[4.3.2]: #4.3.2-Parameters\n",
    "\n",
    "[4.4]: #4.4-Producing-A-Meaningful-Output-Of-Concepts\n",
    "\n",
    "[4.4.1]: #4.4.1-TruncatedSVD()-Output\n",
    "[4.4.2]: #4.4.2-Converting-Document-Matrices-to-Concepts\n",
    "\n",
    "[Results: Interpration Of Extracted Concepts]: #5-Results:-Interpration-Of-Extracted-Concepts\n",
    "[5.1]: #5.1-Output\n",
    "[5.2]: #5.2-Observations-&-Opinions\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Data Source</b> [\"Twenty Newsgroups\", Provided By: Scikit-Learn](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#)\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Purpose & Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis I will be performing data mining in an effort to extract a series of meaningful and significant concepts from a public dataset of newsgroup postings on the topic of Christianity.\n",
    "\n",
    "The dataset, titled \"Twenty Newsgroups\" and is officially described as follows:\n",
    ">\"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\"\n",
    "\n",
    "A newsgroup is an online public forum for discussion on a particular topic. The topic that I will be extracting data from will be \"Christianity\" (soc.religion.christian). I'm very curious to see what the results of this analysis will be, and in concluding intend to share my opinion on them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 What is a \"Latent Semantic Analysis\"(LSA)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Semantic Analysis (LSA) is a technique commonly used in the field of Natural Language Processing (NLP). As a computer scientist, when performing NLP we are concerned with studying the interactions and between computers and human language. A great portion of this field focuses on the analysis of the relationship between multiple words in a document of text containing in a collection of documents. This is known as the subfield of Natural Language Understanding and can be thought of more simply as teaching computers how to read. \n",
    "\n",
    "LSA is more formally defined by [\"An Introduction to Latent Semantic Analysis\" by Landauer, Foltz, & Laham](http://lsa.colorado.edu/papers/dp1.LSAintro.pdf)\n",
    ">\"Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the\n",
    "contextual-usage meaning of words by statistical computations applied to a large corpus of\n",
    "text (Landauer and Dumais, 1997). The underlying idea is that the aggregate of all the word\n",
    "contexts in which a given word does and does not appear provides a set of mutual\n",
    "constraints that largely determines the similarity of meaning of words and sets of words to\n",
    "each other.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Terminology Defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a vast list of new terminoloy defined by the field of NLP. Below I will briefly define those of significance to LSA that I will be using regularly throughout this analysis.\n",
    "\n",
    "* **Word** - A single English word in text.\n",
    "* **Bag Of Words (BOW)** - An abstraction model in NLP where we consider each document of text to simply be a \"bag of words\" in the literal sense, such that grammar and conceptual meaning is ignored.\n",
    "* **Term Frequency–Inverse Document Frequency (TF-IDF)** - A mathematical calculation for scoring the importance of a word in a document or a collection. \n",
    "* **Term** - A single word found in a document of text.\n",
    "* **Document** - A single collection of terms.\n",
    "* **Corpus** - A single collection documents.\n",
    "* **Concept** - The final output of and LSA is a list of concepts. These are words, or multiple words together, which were found to have the highest significance across our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Process/Procedure & Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In brief, I will summarize a list of 7 steps representing the overall process required to perform an LSA:\n",
    "\n",
    "1. Collect/Retrieve a dataset containing text of interest. \n",
    "2. Define which text in the dataset will be represented as documents (sentences, discussion board poasts, news articles, ?)\n",
    "3. Using the BOW model, parse by document and store words in a bag of words where each bag is a document. Ending result should be a collection of documents of tokenized words.\n",
    "4. Clean the data. Remove as many unneccessary words and characters as possible.\n",
    "5. Perform TF-IDF Vectorization. This scores the words as terms for each document and across the document collection as a whole. \n",
    "6. Matrix decomposition using the Singular Value Decomposition algorithm.\n",
    "7. Output a list of concepts extracted. \n",
    "\n",
    "Now we can begin our prepartions for LSA, starting with step 1, importing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports, and dataset download via sk-learn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import re\n",
    "\n",
    "categories = ['soc.religion.christian']\n",
    "dataset = fetch_20newsgroups(subset='all',shuffle=True, random_state=42, categories=categories)\n",
    "corpus = dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc in corpus:\n",
    "    doc = str(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: sciysg@nusunix1.nus.sg (Yung Shing Gene)\\nSubject: Mission Aviation Fellowship\\nOrganization: National University of Singapore\\nLines: 3\\n\\nHi,\\n\\tDoes anyone know anything about this group and what they\\ndo? Any info would be appreciated. Thanks!\\n'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postDocs = [x for x in corpus]\n",
    "postDocs = [x.lower() for x in postDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from: sciysg@nusunix1.nus.sg (yung shing gene)\\nsubject: mission aviation fellowship\\norganization: national university of singapore\\nlines: 3\\n\\nhi,\\n\\tdoes anyone know anything about this group and what they\\ndo? any info would be appreciated. thanks!\\n'"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postDocs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-269-d7aecff9e73a>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-269-d7aecff9e73a>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    #email_address = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', doc)\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for doc in postDocs:\n",
    "    #email_address = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Defining 'stopwords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Overiview: Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 TF-IDF Vectorization with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 Function & Syntax Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Lexical Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Overview: Theory & Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Mathematics: Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 SVD Modeling with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.1 Function & Syntax Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2 Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Producing A Meaningful Output Of Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.1 TruncatedSVD() Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.2 Converting Document Matrices to Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Results: Interpration Of Extracted Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Observations & Opinions"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
