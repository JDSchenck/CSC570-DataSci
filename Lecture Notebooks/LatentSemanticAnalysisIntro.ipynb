{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis\n",
    "\n",
    "**Latent Semantic Analysis** is a more advanced and powerful strategy useful for interpreting and analyzing textual/language based data. In order to complete an LSA, there are a few steps of preparation work on our data that must be completed prior to actually beginning the analysis. \n",
    "\n",
    "First, I will be utilizing the _BeautifulSoup_ library for parsing an XML file containing a decent number of real student forum posts from my Data Science course's discussion board to a corpus of documents. Then I will use  _scikit-learn_ to streamline the TF-IDF process by **vectorizing** directly from each document to a sparse matrix of TFIDF features. \n",
    "\n",
    "Once I have my corpus in the form of a collection of TF-IDF matrices, then I will perform an LSA on the dataset which will result in the extraction of significant **concepts** from our textual data that will be easily interpreted for any further study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing & Cleaning with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jasonschenck/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading stopwords should they not be present\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifuSoup is a very efficient tool for text parsing and cleanup, it's very flexible and easy to use as well. In this case I need to read in an XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse XML file dq forum posts\n",
    "posts = open('raw_forum_posts.dat', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<text></text>, <text>&lt;p&gt;Data Science is about analyzing relevant data to obtain patterns of information in order to help achieve a goal. The main focus of the data analysis is the goal rather then the methodology on how it will achieved. This allows for creative thinking and allowing for the optimal solution or model to be found wihtout the constraint of a specific methodology.&lt;/p&gt;</text>, <text>&lt;p&gt;At it's fundamental core, I believe that data science is ultimately a way to allow people and businesses to compound and work with their observations (essentially, that's what data is--- a recorded observation) and gain as much of an understanding of their context and problems as they possibly can, and use information they can generate to make deeper insights, greater decisions, and ultimately, shift paradigms and propel humanity forward. &lt;/p&gt;\n",
      "&lt;p&gt;As mentioned in the lecture, ultimately what this data will be used for is solving humanities problems more efficiently than ever before. &lt;/p&gt;\n",
      "&lt;p&gt;With the greater power this increase in data gives humanity though comes an increased responsibility---- just as easily as bigger data can help, in the case of people who are underinformed in data science techniques and aren't aware that they may not be using suitable practices, the additional data and sophisticated techniques will just allow a poor practitioner to get themselves and others in trouble faster than before, and at greater magnitudes.&lt;br&gt;&lt;br&gt;Ultimately, however, I think this increase of data and the development of computing tools and methodologies to help process it faster and better will be one of the biggest benefits humanity has seen yet--- it may take a few years to get there, but this is truly an exciting time to be involved in data science. &lt;/p&gt;\n",
      "&lt;br&gt;</text>, <text>&lt;p&gt;Having worked in technology, particularly on the web building websites, running large scale websites and managing engineers, I have found data science to be some of what we did before it had a name. &lt;/p&gt;\n",
      "&lt;p&gt;When I was the GM at a daily newspaper we had millions of users each month looking at 30+ million pages a month. In general you can get lost in the data and what is actionable. But we focused on the actions we could take. That got us into analytics like Omniture. But it wasn't enough. We started doing A/B testing but with a focus on actions. Then we would get lost in the content users wanted to and building models around that. Then we grew into trying to machine learn about users on the fly, store info, make suggestions and build emails around modeling. Ever since then I've been hooked on data because it doesn't lie. Yes, you might find many red herrings but ultimately you can find concrete actionable information within the data that doesn't mislead you like opinions. &lt;/p&gt;\n",
      "&lt;p&gt;I still work online but more in an agency setting but also with publications. Agriculture is our niche and this is an industry that is growing massively in big data from farming/planting to weather. The goal is making farming far more predictible. &lt;/p&gt;</text>, <text>&lt;p&gt;I work as a financial analyst and as a result I have to sift through a lot of data to be able to make decisions and advice management. Ordinarily, the stock market is full of so much information which requires good understanding before you anyone can invest hard earned money. I believe this is where data science comes handy. Data Science is therefore a science of eliminating unwanted data (Noise) and using good data to make accurate decisions or conclusions about the future.&lt;/p&gt;</text>, <text>&lt;p&gt;Data science is the science of using data in order to provide the means to answers questions using large amounts of data in the most efficient and correct way possible.  I think an important distinction here is that in true data science you aren't just giving the answer, you are providing a way to answer questions and maybe even bring up new questions.  In my current role, I know I am not using data scientist techniques.  I find myself being asked specific questions like &amp;quot;how many contacts did we get from Canada last year&amp;quot;.  They don't want a dashboard, they don't want a streamlined process that would allow them to answer this same question on another country.  A data scientist would set up the data so that these questions could be asked later in a different and efficient way.  A data scientist may even start by asking  &amp;quot;what are we really solving?  Do we really gain insight by knowing the number of contacts from Canada?  Or is there a deeper issue we are trying to solve.  Perhaps what is behind a perceived increase in contacts in Canada?&amp;quot;.   &lt;/p&gt;</text>, <text>&lt;p&gt;In my opinion data science in lamen terms has to do with 'trying to make sense of it all'. There is so much data available these days that it is difficult to make good use of this data so that organizations can make better decisions. But the beneficiaries are individual users as well, such as with applications in the health space. The advent of new repository technologies has expanded the scope of how we can implement and use data, so knowledge of now these new technologies work is important in order to apply data science initiatives effectively. &lt;/p&gt;</text>, <text>&lt;p&gt;Data Science in my understanding is a process of extracting knowledge from any data to help in obtaining wisdom through the knowledge gained and ultimately solve any challenges/improve upon anything we do.&lt;br&gt;&lt;br&gt;With so much overexplosion of data in the systems, it is just natural that the evolution of data science has taken place.  As it goes by the famous saying &amp;quot;In god we trust, all others bring data&amp;quot;, the insights gained through data would be highly valued/irrefutable in today's complex business environment.  Data Science in my opinion will not be limited to Statistics, machine learning, software techniques, understanding business to which it is applied to, but will soon evolve to complex study using the strengths of other areas of science.&lt;/p&gt;</text>, <text>&lt;p&gt;Hello Folks,&lt;/p&gt;\n",
      "&lt;p&gt;My name is Tejaswi Nainala. I'm glad to j&lt;span style=\"color: rgb(51,51,51);font-family: &amp;quot;Helvetica Neue&amp;quot; , Helvetica , Arial , sans-serif;font-size: 14.0px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: 21.0px;text-indent: 0.0px;text-transform: none;white-space: normal;word-spacing: 0.0px;\"&gt;oin the data revolution with all of you. Companies are searching for data scientists.  We will be i&lt;span style=\"color: rgb(51,51,51);font-family: &amp;quot;Helvetica Neue&amp;quot; , Helvetica , Arial , sans-serif;font-size: 14.0px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: 21.0px;text-indent: 0.0px;text-transform: none;white-space: normal;word-spacing: 0.0px;\"&gt;ntroduced to the basics of data science and left armed with practical experience extracting value from big data.&lt;span class=\"Apple-converted-space\"&gt; &lt;/span&gt;&lt;/span&gt; &lt;/span&gt;&lt;/p&gt;\n",
      "&lt;p&gt;&lt;span style=\"color: rgb(51,51,51);font-family: &amp;quot;Helvetica Neue&amp;quot; , Helvetica , Arial , sans-serif;font-size: 14.0px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: 21.0px;text-indent: 0.0px;text-transform: none;white-space: normal;word-spacing: 0.0px;\"&gt;&lt;span style=\"color: rgb(51,51,51);font-family: &amp;quot;Helvetica Neue&amp;quot; , Helvetica , Arial , sans-serif;font-size: 14.0px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: 21.0px;text-indent: 0.0px;text-transform: none;white-space: normal;word-spacing: 0.0px;\"&gt;&lt;span class=\"Apple-converted-space\"&gt;In my views &lt;span style=\"font-family: arial , helvetica , sans-serif;font-size: small;\"&gt; d&lt;span style=\"color: rgb(1,1,47);font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: 20.0px;text-indent: 0.0px;text-transform: none;white-space: normal;word-spacing: 0.0px;\"&gt;ata science  is simply about moving people and/or systems between current and new technologies and between beginner and expert skills. It involves Organising data, packaging data and delivering data.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;</text>, <text>&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;\n",
      "&lt;p&gt;&lt;span style=\"font-family: Verdana;color: rgb(51,51,51);\"&gt;I read your comments and agree with most of them, but I just tried to define the data science with broad view. Data science is a kind of business to find the desire of people. Why do we try to learn data science or big data? I think these procedures have one common to find the best way to satisfy people's need, to solve problems, or to predict the new trend for preparing future. From these purpose, the concept of Data science can be developed to the academic approach, such as Hadoop or analyzing a technical issue. &lt;/span&gt;&lt;/p&gt;\n",
      "&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;</text>, <text>&lt;p&gt;From what I have heard / read so far on data science, it seems very similar to archaeology.  Digging and sifting through countless amounts of data trying to find something out of it all.  Whether it be a few gems like key points of data when used in a science setting or finding relationships between the different &amp;quot;dig sites&amp;quot; to find patterns in it all.  The difference being though in actual archaeology, we only have 1 earth with 1 history to dig through, where as in the computer world, we practically have a new history (data set) generated on a new planet (company / organization) every single day.  It is up to the data scientist to find efficient ways to dig through these ever increasing piles of sand before we eventually become buried in it.&lt;/p&gt;</text>, <text>&lt;p&gt;Hi,&lt;/p&gt;\n",
      "&lt;p&gt;With vast amounts of data now available, companies in almost every industry are focused on exploiting data for competitive advantage. Data science is where you make use of a large amout of data in order to make better dicision, solve business problems, building data products, and design data model. According to drewconway, data science is a combination of hacking skills, math &amp;amp; statistics knowledge, and substantive expertise.&lt;/p&gt;\n",
      "&lt;p&gt;Onsiri&lt;/p&gt;\n",
      "&lt;br&gt;\n",
      "&lt;br&gt;</text>, <text>&lt;p&gt;I belive that, Data Science is all about finding problems in a set of data and building efficient model to get information out of that, by applying(or making use of) different science disciplines either in a smart way or trying several times.  &lt;/p&gt;\n",
      "&lt;p&gt;Interested in learning more about Data science in this course.&lt;/p&gt;\n",
      "&lt;br&gt;\n",
      "&lt;p&gt;Thanks &lt;/p&gt;\n",
      "&lt;p&gt;Pavithra&lt;/p&gt;</text>, <text>&lt;p&gt;I do not have any experience in the field so it is hard to come up with my own definition of data science. However based on the video by Josh Wills and the other responses by the students it seems that data science is the study of utilizing data to form a substantive model. Another student, Violeta, noted that data scientists focus more on utilizing all the data available to them and do not spend a lot of time on forming ways to collect the data. I thought this was interesting but I think as we enter the field it may be important to take a different mental approach than this. While a data scientist may spend more time analyzing and cleaning data sets as Josh noted they should also have a solid understanding of the ways in which the data was collect because this will improve results in the future. By doing so we can provide feedback to those who are developing tools to collect data and help them shape their systems to provide more useful data sets.&lt;/p&gt;\n",
      "&lt;p&gt;A simple analogy that comes to mind is lego builders verse lego designers. A data scientist is similar to a child playing with a large box of assorted legos and piecing together different parts to create something new. Once the child has experience building a different sort of creations they may be able to design a simple lego that will allow them to greatly increase the number of designs they can build. The child doesn't have to know how the pieces are made or the type of plastic that is used but can still provide feedback on pieces to be designed in the future that would be useful to them as a builder.&lt;/p&gt;</text>, <text>&lt;p&gt;From what I've been able to learn about data science so far, I would define it as the art of finding good answers to questions using relevant data and appropriate scientific technique.&lt;/p&gt;</text>, <text>&lt;p&gt;Hi&lt;/p&gt;\n",
      "&lt;p&gt;Data Science is all about analyzing a problem and finding data intensive ways to solve that problem. It is our ability to make intelligent decisions with data to make better decisions by creative thinking.&lt;/p&gt;\n",
      "&lt;p&gt;Thank you&lt;/p&gt;\n",
      "&lt;p&gt;Akhilesh&lt;/p&gt;</text>, <text>&lt;p&gt;Hello everyone,&lt;/p&gt;\n",
      "&lt;p&gt;According to me Data Science is a field that concentrates on the tools and the different procedures that are used for the analysis of large amounts of data.These procedures could be from any other field.It plots mainly on different practices that are used to store large amounts of data.&lt;/p&gt;\n",
      "&lt;p&gt;Since I am new to this field.This is all I could figure out.I will explore more in the coming days and will come up with more valuable information&lt;/p&gt;\n",
      "&lt;p&gt;Thank you&lt;/p&gt;</text>, <text>&lt;p&gt;Hello everyone,&lt;/p&gt;\n",
      "&lt;p&gt;According to me Data Science is a field that concentrates on the tools and the different procedures that are used for the analysis of large amounts of data.These procedures could be from any other field.It plots mainly on different practices that are used to store large amounts of data.&lt;/p&gt;\n",
      "&lt;p&gt;Since I am new to this field.This is all I could figure out.I will explore more in the coming days and will come up with more valuable information&lt;/p&gt;\n",
      "&lt;p&gt;Thank you&lt;/p&gt;</text>, <text>&lt;p&gt;&lt;span style=\"font-size: 16.0pt;font-family: &amp;quot;Times New Roman&amp;quot;;\"&gt;Hello everyone,&lt;/span&gt;&lt;/p&gt;\n",
      "&lt;p&gt;&lt;span style=\"font-size: 16.0pt;font-family: &amp;quot;Times New Roman&amp;quot;;\"&gt; &lt;/span&gt;&lt;/p&gt;\n",
      "&lt;p&gt;&lt;span style=\"font-size: 16.0pt;font-family: &amp;quot;Times New Roman&amp;quot;;\"&gt;In my understanding, data science is the integration of methods from statistics, computer science, and other fields for gaining insights from data. In practice, data science encompasses an iterative process of data harvesting, cleaning, analysis and visualization, and implementation. &lt;/span&gt;&lt;/p&gt;\n",
      "&lt;p&gt;&lt;span style=\"font-size: 16.0pt;font-family: &amp;quot;Times New Roman&amp;quot;;\"&gt;Ultimately, this interdisciplinary and cross-functional field leads to decisions that move an organization forward, whether the object of the decision is product design, a proposed investment, or business strategy.&lt;/span&gt;&lt;/p&gt;\n",
      "&lt;p&gt;&lt;span style=\"font-size: 16.0pt;font-family: &amp;quot;Times New Roman&amp;quot;;\"&gt; &lt;/span&gt;&lt;/p&gt;\n",
      "&lt;p&gt;&lt;span style=\"font-size: 16.0pt;font-family: &amp;quot;Times New Roman&amp;quot;;\"&gt;Thanks.&lt;/span&gt;&lt;/p&gt;\n",
      "&lt;br&gt;</text>, <text>&lt;p&gt;Data science, in my words is science using which 'relevant' data can be extracted from a superset of data. Data can be huge and finding the relevant data that will answer the question that we are trying to answer can be cumbersome. Data science helps in extracting that answer by using domain knowledge, if any, computational skills and statistical science to get to a statistically significant confidence level for that answer. This seems part science and part art. The science part involves collecting and computing the data. The art part involves analyzing that data to come up with a correct answer. Overall, the gist of data science is to find statistically signifant answer to the problem we are trying to solve. &lt;/p&gt;</text>, <text>&lt;p&gt;I'm very new to the concept of data science. Based on the readings I've completed and the videos we've just seen, I have a few thoughts. Since it's in the title, data obviously plays an important role. A somewhat circular definition of data seems to be: information that is organized in a significant way to the person(s) accessing it, often in electronic format. Any science is usually the ability to explain, understand, and predict.&lt;/p&gt;\n",
      "&lt;p&gt;It sounds like exactly what Josh Willis was describing: Data Science may be described as the ability to explain, understand, and predict information that is organized in a significant way to the person(s) accessing it.&lt;/p&gt;\n",
      "&lt;p&gt;The most interesting thing that I found from the videos is that data science is often done on &amp;quot;unpredictable&amp;quot; sets of data, be it economic, psychological, or otherwise. I bet this makes model creation (the ability to predict) pretty difficult! &lt;/p&gt;</text>, <text>&lt;p&gt;From the first video with Josh Wills, data science is a huge field that involves software programming and mathemtical analyses to process and find useful trends in data for businesses and industries. From the second video about cheeseburgers, data science can be a neat tool to give insight about the digital world around us. It seems that data science is a field that inevitably arose from a digital age where most information is recorded and from the need to mine the data for facts/trends/correlations for profit/consumer-exploitation/knowledge.&lt;/p&gt;</text>, <text>&lt;p&gt;Hello,&lt;/p&gt;\n",
      "&lt;p&gt;According to my perspective,&lt;/p&gt;\n",
      "&lt;p&gt;The process or the way of finding the data, its sources and form of representation from which a resource is built and &lt;/p&gt;\n",
      "&lt;p&gt;is used by many business fields to get a competitive edge over others. This analysis of data using various scientific &lt;/p&gt;\n",
      "&lt;p&gt;methods is called &lt;strong&gt;Data Science&lt;/strong&gt;.&lt;/p&gt;\n",
      "&lt;p&gt;These data science methods effect in many domains,especially from the business perspective&lt;/p&gt;\n",
      "&lt;p&gt;that are very valuable for IT strategies.&lt;/p&gt;\n",
      "&lt;br&gt;\n",
      "&lt;p&gt;Thanks,&lt;/p&gt;\n",
      "&lt;p&gt;Indraja.&lt;/p&gt;</text>, <text>&lt;p&gt;I am new to Data Science and all I can figure out a definition for this is that, Data Science is all about analyzing the existing data/information and finding out an efficient solution with creativity to achieve the ultimate goal.  &lt;/p&gt;</text>, <text>&lt;p&gt;In my opinion, data science is a general term for extracting knowledge from a given dataset. A data scientist is similar to a research scientist where extensive analyses are performed on the data collected with the hope of drawing meaningful conclusions that would contribute to the field of study. It is often common to use statistics and data processing software in science-related research, but it is not a requirement in some fields such as theoretical physics where formulated models are not as data-driven. On the other hand, the use of statistics and data analysis tools appears to be a must for the field of data science, as it attempts to validate hypotheses through empirical data.&lt;/p&gt;\n",
      "&lt;p&gt;In addition, as data becomes more and more abundant and easily accessible, data science can be applied to social studies, business decisions making, or almost in any fields of studies.&lt;/p&gt;\n",
      "&lt;br&gt;</text>, <text>&lt;p&gt;Hello ,&lt;/p&gt;\n",
      "&lt;p&gt; Data Science Essentials is been linked with the Big Data and &lt;span style=\"color: rgb(0,0,0);font-family: Arial , Helvetica , sans-serif;font-size: 12.0px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: normal;text-indent: 0.0px;text-transform: none;white-space: normal;word-spacing: 0.0px;\"&gt;&lt;span class=\"Apple-converted-space\"&gt; it &lt;/span&gt;is the process of gathering and measuring information on variables of interest.&lt;/span&gt;&lt;/p&gt;\n",
      "&lt;p&gt;&lt;span style=\"color: rgb(0,0,0);font-family: Arial , Helvetica , sans-serif;font-size: 12.0px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: normal;text-indent: 0.0px;text-transform: none;white-space: normal;word-spacing: 0.0px;\"&gt;It tells about where did the information came from,the ways it is represented and how the collected data can be used into useful resource .&lt;/span&gt;&lt;/p&gt;\n",
      "&lt;p&gt;&lt;span style=\"color: rgb(0,0,0);font-family: Arial , Helvetica , sans-serif;font-size: 12.0px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: normal;text-indent: 0.0px;text-transform: none;white-space: normal;word-spacing: 0.0px;\"&gt;&lt;/span&gt;&lt;/p&gt;\n",
      "&lt;p&gt;&lt;span style=\"color: rgb(0,0,0);font-family: Arial , Helvetica , sans-serif;font-size: 12.0px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: normal;text-indent: 0.0px;text-transform: none;white-space: normal;word-spacing: 0.0px;\"&gt;Thanking you ,&lt;/span&gt;&lt;/p&gt;\n",
      "&lt;p&gt;&lt;span style=\"color: rgb(0,0,0);font-family: Arial , Helvetica , sans-serif;font-size: 12.0px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: normal;text-indent: 0.0px;text-transform: none;white-space: normal;word-spacing: 0.0px;\"&gt;Manideep Maddipatla.&lt;/span&gt;&lt;/p&gt;</text>, <text>&lt;p&gt;Data scientist is a work title for a representative or business intelligent (BI) experts who excels at analyzing data, especially large amounts of data, to help a business pick up a competitive edge.&lt;/p&gt;</text>, <text>&lt;p&gt;Hello Everyone!&lt;br&gt;&lt;br&gt;From what I could infer from the videos, Data Science is a study of Information around us in order to find answers to  the most complex questions or sometimes get specific answers to the silliest of questions. Having worked as an Oracle Apps DBA I know for a fact that these days alot of research is going on in the data science field and technologies like Big data and Hadoop are gaining prominence. A more technical answer to what data science is - computing statistics in order to relay useful information.&lt;/p&gt;\n",
      "&lt;p&gt;Regards,&lt;/p&gt;\n",
      "&lt;p&gt;Muheeb&lt;/p&gt;</text>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Generate the corpus with some list comprehension, iterates for all documents all text and appends docs to postDocs.\\n# Also, now the data will no longer be just text, so new var name postDocs \\npostDocs = [x.text for x in postTxt]\\npostDocs.pop(0)\\npostDocs = [x.lower() for x in postDocs]\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In XML, the raw text content is arranged and separated by tagging. Below I will extract only data from text tags\n",
    "# First, instantiate beautiful 'soup' with params (data_file,'file type') Note var name must be soup!\n",
    "soup = BeautifulSoup(posts, 'lxml')\n",
    "\n",
    "# Filter the soup for only the values found between the <text> tags, rename the variable for ease of reading.\n",
    "postTxt = soup.findAll('text')\n",
    "print(postTxt)\n",
    "\n",
    "# Generate the corpus with some list comprehension, iterates for all documents all text and appends docs to postDocs.\n",
    "# Also, now the data will no longer be just text, so new var name postDocs \n",
    "postDocs = [x.text for x in postTxt]\n",
    "postDocs.pop(0)\n",
    "postDocs = [x.lower() for x in postDocs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning time. But no worries, Python and scikit-learn make this easy. All we need to do is to define a list of **stopwords** called a **stopset** then let scikit-learn know about it as a parameter, and it will automatically just remove them as it processes the vectorization. \n",
    "\n",
    "Stopwords are going to be the kind of words that will have no conceptual meaning from the textual analysis. For example the words \"the\", \"0px\", \"rgb\", etc. can be removed as all they will do is slow the process down and make it more inaccurate in the longrun.\n",
    "\n",
    "For this example, I used a provided set of stopwords that was downloaded from the **ntlk** library. A few additional HTML syntax additions were manually added, but for the most part it should do the trick. After the list is defined, just need to call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))\n",
    "stopset.update(['lt','p','/p','br','amp','quot','field','font','normal','span','0px','rgb','style','51', \n",
    "                'spacing','text','helvetica','size','family', 'space', 'arial', 'height', 'indent', 'letter'\n",
    "                'line','none','sans','serif','transform','line','variant','weight','times', 'new','strong', 'video',\n",
    "                'title','white','word','letter', 'roman','0pt','16','color','12','14','21', 'neue', 'apple', 'class',  ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizing with Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>data science is about analyzing relevant data to obtain patterns of information in order to help achieve a goal. the main focus of the data analysis is the goal rather then the methodology on how it will achieved. this allows for creative thinking and allowing for the optimal solution or model to be found wihtout the constraint of a specific methodology.</p>'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before...\n",
    "postDocs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we vectorize, we are essentially defining a lexical analyzer that is built into scikit-learn and therefore must specify some important parameters:  \n",
    "\n",
    "* **stopwords:** set the param to var stopset  \n",
    "<br>\n",
    "* **use idf:** true or false [will want this to be set to true in most cases]  \n",
    "<br>\n",
    "* **ngram range:** 'grams' are essentially words, and the ngram_range specifies to the analyzer the minimum(1) to the maximum(3) grams to consider for contextual relationships and significance. For example, in this case we are going to use 'ngram_range=(1,3)' which means \"analyze at minimum one word, but also analyze for pairings of two words repeating, or even up to 3 words in a relationship across our corpus. The larger the range the more possible concepts we will be able to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the vectorizer model -- TfidfVectorizer(set stopwords = ?, use idf = true, num grams range = ?)\n",
    "vectorizer = TfidfVectorizer(stop_words=stopset,use_idf=True, ngram_range=(1, 3))\n",
    "\n",
    "# Fit the corpus data to the vectorizer model\n",
    "X = vectorizer.fit_transform(postDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3341 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 89 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice the output here..\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 641)\t0.0899008417366\n",
      "  (0, 2471)\t0.0310567745199\n",
      "  (0, 160)\t0.0675084290336\n",
      "  (0, 2400)\t0.0882799340268\n",
      "  (0, 2026)\t0.10905143902\n",
      "  (0, 2140)\t0.0969008875155\n",
      "  (0, 1575)\t0.0529592419308\n",
      "  (0, 2071)\t0.0761293825223\n",
      "  (0, 1459)\t0.0761293825223\n",
      "  (0, 47)\t0.0969008875155\n",
      "  (0, 1376)\t0.176559868054\n",
      "  (0, 1801)\t0.10905143902\n",
      "  (0, 1248)\t0.0882799340268\n",
      "  (0, 143)\t0.071509957232\n",
      "  (0, 2365)\t0.10905143902\n",
      "  (0, 1902)\t0.21810287804\n",
      "  (0, 52)\t0.10905143902\n",
      "  (0, 108)\t0.10905143902\n",
      "  (0, 617)\t0.0969008875155\n",
      "  (0, 2965)\t0.0969008875155\n",
      "  (0, 105)\t0.10905143902\n",
      "  (0, 2065)\t0.10905143902\n",
      "  (0, 2741)\t0.0969008875155\n",
      "  (0, 1930)\t0.0761293825223\n",
      "  (0, 1282)\t0.0882799340268\n",
      "  :\t:\n",
      "  (0, 2028)\t0.10905143902\n",
      "  (0, 2144)\t0.10905143902\n",
      "  (0, 1587)\t0.10905143902\n",
      "  (0, 2077)\t0.10905143902\n",
      "  (0, 1461)\t0.10905143902\n",
      "  (0, 49)\t0.10905143902\n",
      "  (0, 1378)\t0.10905143902\n",
      "  (0, 1803)\t0.10905143902\n",
      "  (0, 1252)\t0.10905143902\n",
      "  (0, 649)\t0.10905143902\n",
      "  (0, 147)\t0.10905143902\n",
      "  (0, 1382)\t0.10905143902\n",
      "  (0, 2367)\t0.10905143902\n",
      "  (0, 1904)\t0.10905143902\n",
      "  (0, 54)\t0.10905143902\n",
      "  (0, 110)\t0.10905143902\n",
      "  (0, 619)\t0.10905143902\n",
      "  (0, 2967)\t0.10905143902\n",
      "  (0, 107)\t0.10905143902\n",
      "  (0, 2067)\t0.10905143902\n",
      "  (0, 2745)\t0.10905143902\n",
      "  (0, 1938)\t0.10905143902\n",
      "  (0, 1288)\t0.10905143902\n",
      "  (0, 3275)\t0.10905143902\n",
      "  (0, 552)\t0.10905143902\n"
     ]
    }
   ],
   "source": [
    "# Tada! This is now the output of the first document in the corpus, in sparse idf matrix form.\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA is the process of taking our corpus of matrices (X), and performing **matrix decomposition** such that:\n",
    "\n",
    "<big>$$X \\approx USV^{T}$$</big>\n",
    "\n",
    "where...\n",
    "\n",
    "* **X** = original corpus of matrices\n",
    "* **m** = # of matrices, or documents, contained in X\n",
    "* **n** = # of terms  \n",
    "<br>\n",
    "\n",
    ">**The Process:**  \n",
    ">- X is decomposed into three matricies called U, S, and T with k-value such that...  \n",
    "\n",
    "<br>\n",
    "\n",
    "* **k** = # of concepts we want to keep during analysis\n",
    "\n",
    "\n",
    "and...\n",
    "\n",
    "* **U** will be a **m x k** matrix.  \n",
    " * _Rows_ --> Documents\n",
    " * _Columns_ --> Concepts\n",
    "* **S** will be a **k x k** diagonal matrix. \n",
    " * _Elements_ --> the amount of _variation_ captured from each concept.\n",
    "* **V** will be a **n x k** matrix.\n",
    " * _Rows_ --> Terms\n",
    " * _Columns_ --> Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 3341)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The current shape is (documents, terms)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an advanced mathematical procedure involving linear algebra which will decompose our matrix X into three U,S,& V. The entire process is built-in to scikit-learn as an engine model, all we must do is define the model specifications and let it do the work for us. \n",
    "\n",
    "[**scikit-learn**](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) provides the following documentation on this function:  \n",
    "> \"Dimensionality reduction using truncated SVD (aka LSA).\n",
    "This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently.\n",
    "In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).\n",
    "This estimator supports two algorithms: a fast randomized SVD solver, and a “naive” algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=27, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Begin by defining the TruncatedSVD model (num rows/docs?, how many passes over the data (epochs)? )\n",
    "#Note: n_iter defaults to 5 if not passed, and 1 if using partial_fit\n",
    "lsa = TruncatedSVD(n_components=27, n_iter=5)\n",
    "\n",
    "# Fit the model\n",
    "lsa.fit(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Post-SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-SVD 'lsa' will be a collection of the 3 matrices above, where matrix V has been transposed from through the decomposition of X -> U,S & --> V[]   (Number of Terms x Extracted Concepts).  \n",
    "\n",
    "**Concepts** are the the reason we peformed this LSA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00568167,  0.00568167,  0.00568167, ...,  0.00438096,\n",
       "        0.00438096,  0.00438096])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After decomposition, 'lsa.components_[]' represents matrix V'\n",
    "lsa.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "data\n",
      "procedures\n",
      "large amounts\n",
      "large amounts data\n",
      "science\n",
      "amounts\n",
      "amounts data\n",
      "different\n",
      "could\n",
      "large\n",
      " \n",
      "Concept 1:\n",
      "procedures\n",
      "large amounts\n",
      "large amounts data\n",
      "could\n",
      "amounts\n",
      "amounts data\n",
      "large\n",
      "used\n",
      "according data\n",
      "according data science\n",
      " \n",
      "Concept 2:\n",
      "make\n",
      "decisions\n",
      "make better\n",
      "problem\n",
      "better\n",
      "data science analyzing\n",
      "science analyzing\n",
      "better decisions\n",
      "make better decisions\n",
      "hi\n",
      " \n",
      "Concept 3:\n",
      "goal\n",
      "data science analyzing\n",
      "science analyzing\n",
      "achieve\n",
      "solution\n",
      "methodology\n",
      "relevant\n",
      "relevant data\n",
      "answer\n",
      "finding\n",
      " \n",
      "Concept 4:\n",
      "information\n",
      "converted\n",
      "big\n",
      "big data\n",
      "big data converted\n",
      "data converted\n",
      "useful\n",
      "goal\n",
      "came\n",
      "came ways\n",
      " \n",
      "Concept 5:\n",
      "business\n",
      "methods\n",
      "competitive edge\n",
      "edge\n",
      "especially\n",
      "perspective\n",
      "goal\n",
      "analyzing\n",
      "competitive\n",
      "achieve\n",
      " \n",
      "Concept 6:\n",
      "converted\n",
      "hello\n",
      "big data converted\n",
      "data converted\n",
      "resource\n",
      "relevant\n",
      "relevant data\n",
      "art\n",
      "scientific\n",
      "big\n",
      " \n",
      "Concept 7:\n",
      "converted\n",
      "dig\n",
      "users\n",
      "competitive\n",
      "data scientist\n",
      "scientist\n",
      "building\n",
      "amounts\n",
      "amounts data\n",
      "find\n",
      " \n",
      "Concept 8:\n",
      "may\n",
      "data scientist\n",
      "scientist\n",
      "provide\n",
      "child\n",
      "collect\n",
      "lego\n",
      "experience\n",
      "data help\n",
      "help\n",
      " \n",
      "Concept 9:\n",
      "way\n",
      "people\n",
      "predict\n",
      "problems\n",
      "finding\n",
      "perspective\n",
      "thanks\n",
      "define\n",
      "learn data\n",
      "learn data science\n",
      " \n",
      "Concept 10:\n",
      "predict\n",
      "ability\n",
      "videos\n",
      "ability explain\n",
      "ability explain understand\n",
      "accessing\n",
      "explain\n",
      "explain understand\n",
      "explain understand predict\n",
      "information organized\n",
      " \n",
      "Concept 11:\n",
      "users\n",
      "actionable\n",
      "actions\n",
      "farming\n",
      "get lost\n",
      "lost\n",
      "month\n",
      "websites\n",
      "around\n",
      "ultimately\n",
      " \n",
      "Concept 12:\n",
      "learning\n",
      "set data\n",
      "applying\n",
      "applying making\n",
      "applying making use\n",
      "belive\n",
      "belive data\n",
      "belive data science\n",
      "building efficient\n",
      "building efficient model\n",
      " \n",
      "Concept 13:\n",
      "people\n",
      "need\n",
      "define\n",
      "learn data\n",
      "learn data science\n",
      "learn\n",
      "problems\n",
      "knowledge\n",
      "gained\n",
      "digital\n",
      " \n",
      "Concept 14:\n",
      "digital\n",
      "trends\n",
      "perspective\n",
      "many\n",
      "age\n",
      "age information\n",
      "age information recorded\n",
      "analyses process\n",
      "analyses process find\n",
      "arose\n",
      " \n",
      "Concept 15:\n",
      "good\n",
      "statistics data\n",
      "studies\n",
      "use statistics\n",
      "use statistics data\n",
      "conclusions\n",
      "able\n",
      "research\n",
      "would\n",
      "fields\n",
      " \n",
      "Concept 16:\n",
      "information\n",
      "data used\n",
      "complex\n",
      "came\n",
      "came ways\n",
      "came ways represented\n",
      "collected data\n",
      "collected data used\n",
      "converted process\n",
      "converted process gathering\n",
      " \n",
      "Concept 17:\n",
      "part\n",
      "answer\n",
      "ability\n",
      "significant\n",
      "gained\n",
      "accessing\n",
      "explain\n",
      "explain understand\n",
      "explain understand predict\n",
      "information organized\n",
      " \n",
      "Concept 18:\n",
      "statistics data\n",
      "studies\n",
      "use statistics\n",
      "use statistics data\n",
      "fields\n",
      "greater\n",
      "humanity\n",
      "use\n",
      "easily\n",
      "collected\n",
      " \n",
      "Concept 19:\n",
      "ways\n",
      "gained\n",
      "substantive\n",
      "child\n",
      "collect\n",
      "lego\n",
      "design\n",
      "able learn data\n",
      "appropriate\n",
      "appropriate scientific\n",
      " \n",
      "Concept 20:\n",
      "use data\n",
      "technologies\n",
      "perspective\n",
      "many\n",
      "methods\n",
      "better decisions\n",
      "make better decisions\n",
      "scientific\n",
      "finding\n",
      "advent\n",
      " \n",
      "Concept 21:\n",
      "perspective\n",
      "methodology\n",
      "order\n",
      "according\n",
      "every\n",
      "dig\n",
      "scientific\n",
      "companies\n",
      "good\n",
      "according drewconway\n",
      " \n",
      "Concept 22:\n",
      "achieve ultimate\n",
      "achieve ultimate goal\n",
      "analyzing existing\n",
      "analyzing existing data\n",
      "creativity\n",
      "creativity achieve\n",
      "creativity achieve ultimate\n",
      "data information\n",
      "data information finding\n",
      "data science figure\n",
      " \n",
      "Concept 23:\n",
      "use data\n",
      "good\n",
      "work\n",
      "amounts data help\n",
      "analyzing data especially\n",
      "bi\n",
      "bi experts\n",
      "bi experts excels\n",
      "business intelligent\n",
      "business intelligent bi\n",
      " \n",
      "Concept 24:\n",
      "part\n",
      "answer\n",
      "part involves\n",
      "science part\n",
      "statistically\n",
      "trying\n",
      "use data\n",
      "science\n",
      "analyzing data come\n",
      "answer cumbersome\n",
      " \n",
      "Concept 25:\n",
      "canada\n",
      "contacts\n",
      "asked\n",
      "contacts canada\n",
      "even\n",
      "really\n",
      "using data\n",
      "want\n",
      "way\n",
      "questions\n",
      " \n",
      "Concept 26:\n",
      "20 ata\n",
      "abundant\n",
      "come\n",
      "different\n",
      "since\n",
      "able make\n",
      "abundant easily\n",
      "ability explain understand\n",
      "30\n",
      "figure\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print(\"Concept %d:\" % i )\n",
    "    for term in sortedTerms:\n",
    "        print(term[0])\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.68167255e-03,   5.68167255e-03,   5.68167255e-03, ...,\n",
       "          4.38095782e-03,   4.38095782e-03,   4.38095782e-03],\n",
       "       [ -1.01002149e-02,  -1.01002149e-02,  -1.01002149e-02, ...,\n",
       "         -7.49848859e-03,  -7.49848859e-03,  -7.49848859e-03],\n",
       "       [ -2.37990997e-03,  -2.37990997e-03,  -2.37990997e-03, ...,\n",
       "          1.86152381e-03,   1.86152381e-03,   1.86152381e-03],\n",
       "       ..., \n",
       "       [ -1.88577217e-02,  -1.88577217e-02,  -1.88577217e-02, ...,\n",
       "         -3.18150966e-03,  -3.18150966e-03,  -3.18150966e-03],\n",
       "       [ -4.78584275e-05,  -4.78584275e-05,  -4.78584275e-05, ...,\n",
       "         -1.21575209e-02,  -1.21575209e-02,  -1.21575209e-02],\n",
       "       [ -6.91126091e-01,   4.24611441e-01,   8.54192316e-03, ...,\n",
       "          7.58532068e-04,   7.58532068e-04,  -5.36711691e-04]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
