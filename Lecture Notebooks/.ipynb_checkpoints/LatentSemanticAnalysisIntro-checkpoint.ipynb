{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis\n",
    "\n",
    "**Latent Semantic Analysis** is a more advanced and powerful strategy useful for interpreting and analyzing textual/language based data. In order to complete an LSA, there are a few steps of preparation work on our data that must be completed prior to actually beginning the analysis. \n",
    "\n",
    "First, I will be utilizing the _BeautifulSoup_ library for parsing an XML file containing a decent number of real student forum posts from my Data Science course's discussion board to a corpus of documents. Then I will use  _scikit-learn_ to streamline the TF-IDF process by **vectorizing** directly from each document to a sparse matrix of TFIDF features. \n",
    "\n",
    "Once I have my corpus in the form of a collection of TF-IDF matrices, then I will perform an LSA on the dataset which will result in the extraction of significant **concepts** from our textual data that will be easily interpreted for any further study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing & Cleaning with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jasonschenck/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading stopwords should they not be present\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifuSoup is a very efficient tool for text parsing and cleanup, it's very flexible and easy to use as well. In this case I need to read in an XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse XML file dq forum posts\n",
    "posts = open('raw_forum_posts.dat', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In XML, the raw text content is arranged and separated by tagging. Below I will extract only data from text tags\n",
    "# First, instantiate beautiful 'soup' with params (data_file,'file type') Note var name must be soup!\n",
    "soup = BeautifulSoup(posts, 'lxml')\n",
    "\n",
    "# Filter the soup for only the values found between the <text> tags, rename the variable for ease of reading.\n",
    "postTxt = soup.findAll('text')\n",
    "\n",
    "# Generate the corpus with some list comprehension, iterates for all documents all text and appends docs to postDocs.\n",
    "# Also, now the data will no longer be just text, so new var name postDocs \n",
    "postDocs = [x.text for x in postTxt]\n",
    "postDocs.pop(0)\n",
    "postDocs = [x.lower() for x in postDocs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning time. But no worries, Python and scikit-learn make this easy. All we need to do is to define a list of **stopwords** called a **stopset** then let scikit-learn know about it as a parameter, and it will automatically just remove them as it processes the vectorization. \n",
    "\n",
    "Stopwords are going to be the kind of words that will have no conceptual meaning from the textual analysis. For example the words \"the\", \"0px\", \"rgb\", etc. can be removed as all they will do is slow the process down and make it more inaccurate in the longrun.\n",
    "\n",
    "For this example, I used a provided set of stopwords that was downloaded from the **ntlk** library. A few additional HTML syntax additions were manually added, but for the most part it should do the trick. After the list is defined, just need to call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))\n",
    "stopset.update(['lt','p','/p','br','amp','quot','field','font','normal','span','0px','rgb','style','51', \n",
    "                'spacing','text','helvetica','size','family', 'space', 'arial', 'height', 'indent', 'letter'\n",
    "                'line','none','sans','serif','transform','line','variant','weight','times', 'new','strong', 'video',\n",
    "                'title','white','word','letter', 'roman','0pt','16','color','12','14','21', 'neue', 'apple', 'class',  ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizing with Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>data science is about analyzing relevant data to obtain patterns of information in order to help achieve a goal. the main focus of the data analysis is the goal rather then the methodology on how it will achieved. this allows for creative thinking and allowing for the optimal solution or model to be found wihtout the constraint of a specific methodology.</p>'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before...\n",
    "postDocs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we vectorize, we are essentially defining a lexical analyzer that is built into scikit-learn and therefore must specify some important parameters:  \n",
    "\n",
    "* **stopwords:** set the param to var stopset  \n",
    "<br>\n",
    "* **use idf:** true or false [will want this to be set to true in most cases]  \n",
    "<br>\n",
    "* **ngram range:** 'grams' are essentially words, and the ngram_range specifies to the analyzer the minimum(1) to the maximum(3) grams to consider for contextual relationships and significance. For example, in this case we are going to use 'ngram_range=(1,3)' which means \"analyze at minimum one word, but also analyze for pairings of two words repeating, or even up to 3 words in a relationship across our corpus. The larger the range the more possible concepts we will be able to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the vectorizer model -- TfidfVectorizer(set stopwords = ?, use idf = true, num grams range = ?)\n",
    "vectorizer = TfidfVectorizer(stop_words=stopset,use_idf=True, ngram_range=(1, 3))\n",
    "\n",
    "# Fit the corpus data to the vectorizer model\n",
    "X = vectorizer.fit_transform(postDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3341 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 89 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice the output here..\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 641)\t0.0899008417366\n",
      "  (0, 2471)\t0.0310567745199\n",
      "  (0, 160)\t0.0675084290336\n",
      "  (0, 2400)\t0.0882799340268\n",
      "  (0, 2026)\t0.10905143902\n",
      "  (0, 2140)\t0.0969008875155\n",
      "  (0, 1575)\t0.0529592419308\n",
      "  (0, 2071)\t0.0761293825223\n",
      "  (0, 1459)\t0.0761293825223\n",
      "  (0, 47)\t0.0969008875155\n",
      "  (0, 1376)\t0.176559868054\n",
      "  (0, 1801)\t0.10905143902\n",
      "  (0, 1248)\t0.0882799340268\n",
      "  (0, 143)\t0.071509957232\n",
      "  (0, 2365)\t0.10905143902\n",
      "  (0, 1902)\t0.21810287804\n",
      "  (0, 52)\t0.10905143902\n",
      "  (0, 108)\t0.10905143902\n",
      "  (0, 617)\t0.0969008875155\n",
      "  (0, 2965)\t0.0969008875155\n",
      "  (0, 105)\t0.10905143902\n",
      "  (0, 2065)\t0.10905143902\n",
      "  (0, 2741)\t0.0969008875155\n",
      "  (0, 1930)\t0.0761293825223\n",
      "  (0, 1282)\t0.0882799340268\n",
      "  :\t:\n",
      "  (0, 2028)\t0.10905143902\n",
      "  (0, 2144)\t0.10905143902\n",
      "  (0, 1587)\t0.10905143902\n",
      "  (0, 2077)\t0.10905143902\n",
      "  (0, 1461)\t0.10905143902\n",
      "  (0, 49)\t0.10905143902\n",
      "  (0, 1378)\t0.10905143902\n",
      "  (0, 1803)\t0.10905143902\n",
      "  (0, 1252)\t0.10905143902\n",
      "  (0, 649)\t0.10905143902\n",
      "  (0, 147)\t0.10905143902\n",
      "  (0, 1382)\t0.10905143902\n",
      "  (0, 2367)\t0.10905143902\n",
      "  (0, 1904)\t0.10905143902\n",
      "  (0, 54)\t0.10905143902\n",
      "  (0, 110)\t0.10905143902\n",
      "  (0, 619)\t0.10905143902\n",
      "  (0, 2967)\t0.10905143902\n",
      "  (0, 107)\t0.10905143902\n",
      "  (0, 2067)\t0.10905143902\n",
      "  (0, 2745)\t0.10905143902\n",
      "  (0, 1938)\t0.10905143902\n",
      "  (0, 1288)\t0.10905143902\n",
      "  (0, 3275)\t0.10905143902\n",
      "  (0, 552)\t0.10905143902\n"
     ]
    }
   ],
   "source": [
    "# Tada! This is now the output of the first document in the corpus, in sparse idf matrix form.\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Semantic Analysis (LSA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA is the process of taking our corpus of matrices (X), and performing **matrix decomposition** such that:\n",
    "\n",
    "<big>$$X \\approx USV^{T}$$</big>\n",
    "\n",
    "where...\n",
    "\n",
    "* **X** = original corpus of matrices\n",
    "* **m** = # of matrices, or documents, contained in X\n",
    "* **n** = # of terms  \n",
    "<br>\n",
    "\n",
    ">**The Process:**  \n",
    ">- X is decomposed into three matricies called U, S, and T with k-value such that...  \n",
    "\n",
    "<br>\n",
    "\n",
    "* **k** = # of concepts we want to keep during analysis\n",
    "\n",
    "\n",
    "and...\n",
    "\n",
    "* **U** will be a **m x k** matrix.  \n",
    " * _Rows_ --> Documents\n",
    " * _Columns_ --> Concepts\n",
    "* **S** will be a **k x k** diagonal matrix. \n",
    " * _Elements_ --> the amount of _variation_ captured from each concept.\n",
    "* **V** will be a **n x k** matrix.\n",
    " * _Rows_ --> Terms\n",
    " * _Columns_ --> Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 3341)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The current shape is (documents, terms)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an advanced mathematical procedure involving linear algebra which will decompose our matrix X into three U,S,& V. The entire process is built-in to scikit-learn as an engine model, all we must do is define the model specifications and let it do the work for us. \n",
    "\n",
    "[**scikit-learn**](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) provides the following documentation on this function:  \n",
    "> \"Dimensionality reduction using truncated SVD (aka LSA).\n",
    "This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently.\n",
    "In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).\n",
    "This estimator supports two algorithms: a fast randomized SVD solver, and a “naive” algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=27, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Begin by defining the TruncatedSVD model (num rows/docs?, how many passes over the data (epochs)? )\n",
    "#Note: n_iter defaults to 5 if not passed, and 1 if using partial_fit\n",
    "lsa = TruncatedSVD(n_components=27, n_iter=5)\n",
    "\n",
    "# Fit the model\n",
    "lsa.fit(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Post-SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-SVD 'lsa' will be a collection of the 3 matrices above, where matrix V has been transposed from through the decomposition of X -> U,S & --> V[]   (Number of Terms x Extracted Concepts).  \n",
    "\n",
    "**Concepts** are the the reason we peformed this LSA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00568167,  0.00568167,  0.00568167, ...,  0.00438096,\n",
       "        0.00438096,  0.00438096])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After decomposition, 'lsa.components_[]' represents matrix V'\n",
    "lsa.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "data\n",
      "procedures\n",
      "large amounts\n",
      "large amounts data\n",
      "science\n",
      "amounts\n",
      "amounts data\n",
      "different\n",
      "could\n",
      "large\n",
      " \n",
      "Concept 1:\n",
      "procedures\n",
      "large amounts\n",
      "large amounts data\n",
      "could\n",
      "amounts\n",
      "amounts data\n",
      "large\n",
      "used\n",
      "according data\n",
      "according data science\n",
      " \n",
      "Concept 2:\n",
      "make\n",
      "decisions\n",
      "make better\n",
      "problem\n",
      "better\n",
      "data science analyzing\n",
      "science analyzing\n",
      "better decisions\n",
      "make better decisions\n",
      "hi\n",
      " \n",
      "Concept 3:\n",
      "goal\n",
      "data science analyzing\n",
      "science analyzing\n",
      "achieve\n",
      "solution\n",
      "methodology\n",
      "relevant\n",
      "relevant data\n",
      "answer\n",
      "finding\n",
      " \n",
      "Concept 4:\n",
      "information\n",
      "converted\n",
      "big\n",
      "big data\n",
      "big data converted\n",
      "data converted\n",
      "useful\n",
      "goal\n",
      "came\n",
      "came ways\n",
      " \n",
      "Concept 5:\n",
      "business\n",
      "methods\n",
      "competitive edge\n",
      "edge\n",
      "especially\n",
      "perspective\n",
      "goal\n",
      "analyzing\n",
      "competitive\n",
      "achieve\n",
      " \n",
      "Concept 6:\n",
      "converted\n",
      "hello\n",
      "big data converted\n",
      "data converted\n",
      "resource\n",
      "relevant\n",
      "relevant data\n",
      "art\n",
      "scientific\n",
      "big\n",
      " \n",
      "Concept 7:\n",
      "converted\n",
      "dig\n",
      "users\n",
      "competitive\n",
      "data scientist\n",
      "scientist\n",
      "building\n",
      "amounts\n",
      "amounts data\n",
      "find\n",
      " \n",
      "Concept 8:\n",
      "may\n",
      "data scientist\n",
      "scientist\n",
      "provide\n",
      "child\n",
      "collect\n",
      "lego\n",
      "experience\n",
      "data help\n",
      "help\n",
      " \n",
      "Concept 9:\n",
      "way\n",
      "people\n",
      "predict\n",
      "problems\n",
      "finding\n",
      "perspective\n",
      "thanks\n",
      "define\n",
      "learn data\n",
      "learn data science\n",
      " \n",
      "Concept 10:\n",
      "predict\n",
      "ability\n",
      "videos\n",
      "ability explain\n",
      "ability explain understand\n",
      "accessing\n",
      "explain\n",
      "explain understand\n",
      "explain understand predict\n",
      "information organized\n",
      " \n",
      "Concept 11:\n",
      "users\n",
      "actionable\n",
      "actions\n",
      "farming\n",
      "get lost\n",
      "lost\n",
      "month\n",
      "websites\n",
      "around\n",
      "ultimately\n",
      " \n",
      "Concept 12:\n",
      "learning\n",
      "set data\n",
      "applying\n",
      "applying making\n",
      "applying making use\n",
      "belive\n",
      "belive data\n",
      "belive data science\n",
      "building efficient\n",
      "building efficient model\n",
      " \n",
      "Concept 13:\n",
      "people\n",
      "need\n",
      "define\n",
      "learn data\n",
      "learn data science\n",
      "learn\n",
      "problems\n",
      "knowledge\n",
      "gained\n",
      "digital\n",
      " \n",
      "Concept 14:\n",
      "digital\n",
      "trends\n",
      "perspective\n",
      "many\n",
      "age\n",
      "age information\n",
      "age information recorded\n",
      "analyses process\n",
      "analyses process find\n",
      "arose\n",
      " \n",
      "Concept 15:\n",
      "good\n",
      "statistics data\n",
      "studies\n",
      "use statistics\n",
      "use statistics data\n",
      "conclusions\n",
      "able\n",
      "research\n",
      "would\n",
      "fields\n",
      " \n",
      "Concept 16:\n",
      "information\n",
      "data used\n",
      "complex\n",
      "came\n",
      "came ways\n",
      "came ways represented\n",
      "collected data\n",
      "collected data used\n",
      "converted process\n",
      "converted process gathering\n",
      " \n",
      "Concept 17:\n",
      "part\n",
      "answer\n",
      "ability\n",
      "significant\n",
      "gained\n",
      "accessing\n",
      "explain\n",
      "explain understand\n",
      "explain understand predict\n",
      "information organized\n",
      " \n",
      "Concept 18:\n",
      "statistics data\n",
      "studies\n",
      "use statistics\n",
      "use statistics data\n",
      "fields\n",
      "greater\n",
      "humanity\n",
      "use\n",
      "easily\n",
      "collected\n",
      " \n",
      "Concept 19:\n",
      "ways\n",
      "gained\n",
      "substantive\n",
      "child\n",
      "collect\n",
      "lego\n",
      "design\n",
      "able learn data\n",
      "appropriate\n",
      "appropriate scientific\n",
      " \n",
      "Concept 20:\n",
      "use data\n",
      "technologies\n",
      "perspective\n",
      "many\n",
      "methods\n",
      "better decisions\n",
      "make better decisions\n",
      "scientific\n",
      "finding\n",
      "advent\n",
      " \n",
      "Concept 21:\n",
      "perspective\n",
      "methodology\n",
      "order\n",
      "according\n",
      "every\n",
      "dig\n",
      "scientific\n",
      "companies\n",
      "good\n",
      "according drewconway\n",
      " \n",
      "Concept 22:\n",
      "achieve ultimate\n",
      "achieve ultimate goal\n",
      "analyzing existing\n",
      "analyzing existing data\n",
      "creativity\n",
      "creativity achieve\n",
      "creativity achieve ultimate\n",
      "data information\n",
      "data information finding\n",
      "data science figure\n",
      " \n",
      "Concept 23:\n",
      "use data\n",
      "good\n",
      "work\n",
      "amounts data help\n",
      "analyzing data especially\n",
      "bi\n",
      "bi experts\n",
      "bi experts excels\n",
      "business intelligent\n",
      "business intelligent bi\n",
      " \n",
      "Concept 24:\n",
      "part\n",
      "answer\n",
      "part involves\n",
      "science part\n",
      "statistically\n",
      "trying\n",
      "use data\n",
      "science\n",
      "analyzing data come\n",
      "answer cumbersome\n",
      " \n",
      "Concept 25:\n",
      "canada\n",
      "contacts\n",
      "asked\n",
      "contacts canada\n",
      "even\n",
      "really\n",
      "using data\n",
      "want\n",
      "way\n",
      "questions\n",
      " \n",
      "Concept 26:\n",
      "20 ata\n",
      "abundant\n",
      "come\n",
      "different\n",
      "since\n",
      "able make\n",
      "abundant easily\n",
      "ability explain understand\n",
      "30\n",
      "figure\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print(\"Concept %d:\" % i )\n",
    "    for term in sortedTerms:\n",
    "        print(term[0])\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.68167255e-03,   5.68167255e-03,   5.68167255e-03, ...,\n",
       "          4.38095782e-03,   4.38095782e-03,   4.38095782e-03],\n",
       "       [ -1.01002149e-02,  -1.01002149e-02,  -1.01002149e-02, ...,\n",
       "         -7.49848859e-03,  -7.49848859e-03,  -7.49848859e-03],\n",
       "       [ -2.37990997e-03,  -2.37990997e-03,  -2.37990997e-03, ...,\n",
       "          1.86152381e-03,   1.86152381e-03,   1.86152381e-03],\n",
       "       ..., \n",
       "       [ -1.88577217e-02,  -1.88577217e-02,  -1.88577217e-02, ...,\n",
       "         -3.18150966e-03,  -3.18150966e-03,  -3.18150966e-03],\n",
       "       [ -4.78584275e-05,  -4.78584275e-05,  -4.78584275e-05, ...,\n",
       "         -1.21575209e-02,  -1.21575209e-02,  -1.21575209e-02],\n",
       "       [ -6.91126091e-01,   4.24611441e-01,   8.54192316e-03, ...,\n",
       "          7.58532068e-04,   7.58532068e-04,  -5.36711691e-04]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
