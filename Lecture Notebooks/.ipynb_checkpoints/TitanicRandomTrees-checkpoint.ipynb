{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitc Regression Analysis:\n",
    "## _An Example Model Predicting Survival Aboard the Titanic_\n",
    "---\n",
    "\n",
    "Prepared By: Jason Schenck  \n",
    "Date: February 20th 2017  \n",
    "CSC-570 Data Science Essentials\n",
    "\n",
    "\n",
    "<br>\n",
    "<big>Table Of Contents</big>\n",
    "\n",
    "---\n",
    "* [1 Introduction][Introduction]  \n",
    "\n",
    "\n",
    "* [2 Data Preparation][Data Preparation]\n",
    "\n",
    "\n",
    "* [3 Parameter Optimization][Parameter Optimization]\n",
    "\n",
    "\n",
    "* [4 Logistic Model Definition][Logistic Model Definition]\n",
    "\n",
    "\n",
    "* [5 Results: Model Scoring][Results: Model Scoring]\n",
    "\n",
    "---\n",
    "\n",
    "[Introduction]: #1.-Introduction\n",
    "[Data Preparation]: #2.-Data-Preparation\n",
    "[Parameter Optimization]: #3.-Parameter-Optimization\n",
    "[Logistic Model Definition]: #4.-Logistic-Model-Definition\n",
    "[Results: Model Scoring]: #5.-Results:-Model-Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, I performed an EDA (Exploratory Data Analysis) on the Titanic Survival dataset provided by Kaggle. In this analysis I will be continuing forward with Logistic Regression Analysis. I've implemented a custom function that will determine the best set of features to include as 'X' based on highest AUC.\n",
    "\n",
    "I'll begin first, by preparing the data for analysis which consists of the following steps:\n",
    "\n",
    "1. Choosing some set of independent features to work with, X\n",
    "2. Correcting all missing values\n",
    "3. One hot encoding for any categorical variables with respect to majority distribution from EDA.\n",
    "4. Scaling the data\n",
    "5. Splitting the data with a 20% holdout \n",
    "\n",
    "Once the data frame is properly created and scaled accordingly, A logistic model is defined and passed the training data to generate predictions. Finally, I will measure the accuracy of my model by examing the AUC (Area Under Curve) score generated and plot this inline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonschenck/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import itertools\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bba558c8a441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read in Titanic train data to dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jasonschenck/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jasonschenck/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jasonschenck/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jasonschenck/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jasonschenck/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Read in Titanic train data to dataframe\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print columns/features\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, we need a new dataframe for only the features for consideration set 'Z'\n",
    "Z = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Of all the available features, from previous EDA, I have decided to start by working with only the below features.\n",
    "# Sex, Age, Pclass, SibSp, Parch, Fare, Embarked, and y = Survived\n",
    "Z['Sex']=df['Sex']\n",
    "Z['Age']=df['Age']\n",
    "Z['Pclass']=df['Pclass']\n",
    "Z['SibSp']=df['SibSp']\n",
    "Z['Parch']=df['Parch']\n",
    "Z['Fare']=df['Fare']\n",
    "Z['Embarked']=df['Embarked']\n",
    "Z['Survived']=df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y = Survived , drop from set Z\n",
    "y = Z['Survived']\n",
    "Z = Z.drop(['Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the new DF\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Assumptions from _[previous EDA](https://nbviewer.jupyter.org/github/JDSchenck/CSC570-DataSci/blob/master/Titanic%20EDA/EDA-Titanic.ipynb)_:\n",
    "> - **Sex**: Categorical, binary, classes: 'male' or 'female', distribution male = ~65%, female = ~35%, No missing values.\n",
    "> - **Age**: Continuous, std dev = ~14.53, mean = ~30, _177 Missing values_\n",
    "> - **Pclass**: Categorical, three classes: '1', '2', or '3' , distribution 1 = ~24%, 2 = ~21%, 3 = ~55%, No missing values.\n",
    "> - **SibSp**: Continuous(small range of 0 to 8), mean = ~0.52 , mode = 0, std dev = ~1.1, _Var has bias_, No missing values.\n",
    "> - **Parch**: Continuous (small range of 0 to 6), mean = ~0.38, mode = 0, std dev = ~0.81, _Var has bias_, No missing values\n",
    "> - **Fare**: Continuous (range from 0 to ~512.33), mean = ~32.20, std dev = ~49.69, Value in currency Pounds, No missing values.\n",
    "> - **Embarked**: Categorical, three categories of ports ('S')outhampton, ('C')herbourg, and ('Q')ueenstown, distribution heavily to 'S', _2 missing values_.\n",
    "> - **Survived**: Categorical, binary 0 = no, 1 = yes for survived. Dependent variable (y) for predicton model. No missing values.\n",
    "\n",
    "Using the above information from EDA, I will start by handling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Age: 177 missing values -- Use mean age to fill in for the missing age values. Get exact mean.\n",
    "\n",
    "mean_age = Z.Age.mean()\n",
    "print(mean_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace all missing ages with the mean age.\n",
    "# Note: This is not the BEST possible strategy, but is better than dropping 177 observations completely.\n",
    "\n",
    "Z.Age = Z.Age.fillna(value=mean_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check for missing\n",
    "Z[Z.Age.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the Embarked missing values, recall _[from my EDA](https://nbviewer.jupyter.org/github/JDSchenck/CSC570-DataSci/blob/master/Titanic%20EDA/EDA-Titanic.ipynb#2.1.7-Embarked)_:\n",
    ">\"Two passengers with missing Embarked values. Both of which survived. **_But how can a passenger not have a port of embarkation?_** Interesting. Since there's only two, I performed a brief Google search for 'Icard, Miss. Amelie' and found something very interesting! \n",
    "\n",
    ">From the _[Encyclopedia Titanica](https://www.encyclopedia-titanica.org/titanic-survivor/amelia-icard.html)_:\n",
    ">\"Miss Rose Amélie Icard, 38, was born in Vaucluse, France on 31 October 1872, her father Marc Icard lived at Mafs á Murs (?).\n",
    "\n",
    ">She boarded the Titanic at Southampton as maid to Mrs George Nelson Stone. She travelled on Mrs Stone's ticket (#113572).\n",
    "\n",
    ">Mrs Stone and Miss Icard were rescued by the Carpathia in lifeboat 6.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Double check for the missing values.\n",
    "Z[Z.Embarked.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As verified, both of these passengers embarked from Southampton 'S', Set manually.\n",
    "Z.Embarked = Z.Embarked.fillna(value='S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensure corrected\n",
    "Z[Z.Embarked.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all missing values have been accounted for, I will perform one hot encoding on all categorical variables using the majority distribution data uncovered in my original EDA. I will try and keep only the fewest number of encoded categoricals such that they collectively represent >50% of our sample. Dropping all others help to prevent collinearity. \n",
    "\n",
    "I also plan to experiment by engineering a new feature, 'TravelingAlone', by combining the data of two existing ones: SibSp and Parch. While tuning the model, perhaps this may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encoding both categoricals SibSp and Parch\n",
    "sibsp_temp = pd.get_dummies(Z.SibSp)\n",
    "parch_temp = pd.get_dummies(Z.Parch)\n",
    "\n",
    "# Here's a peek at what was just generated for both sibsp and parch.\n",
    "print(\"--- sibsp_temp: result of get_dummies ---\")\n",
    "sibsp_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One hot encoding for SibSp, keeping only the classes 0 and 1 due to the distribution data from EDA.\n",
    "Z['SibSp0'] = sibsp_temp.ix[:,'0':'0']\n",
    "Z['SibSp1'] = sibsp_temp.ix[:,'1':'1']\n",
    "Z.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One hot encoding for Parch, keeping only the classes 0,1,and 2 due to the distribution data from EDA.\n",
    "Z['Parch0'] = parch_temp.ix[:,'0':'0']\n",
    "Z['Parch1'] = parch_temp.ix[:,'1':'1']\n",
    "Z['Parch2'] = parch_temp.ix[:,'2':'2']\n",
    "Z.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build new feature data for 'TravelingAlone' via iteration.\n",
    "# TravelingAlone = 1 --> Passenger was indeed alone on the ship\n",
    "# TravelingAlone = 0 --> Passenger was NOT alone, and with atleast 1 related person (parent,child,brother,etc.)\n",
    "\n",
    "TravelingAlone = list()\n",
    "value = 0\n",
    "\n",
    "# Iterate for all observations (891), determine and store the appropriate TravelingAlone feature value.\n",
    "while value < len(sibsp_temp):\n",
    "    if ((sibsp_temp.iloc[value,0] == 0) and (parch_temp.iloc[value,0] == 0)):\n",
    "        # Both 0, then travelingalone = 1\n",
    "        TravelingAlone.append(1)\n",
    "        \n",
    "    else:\n",
    "        # Not traveling alone, therefore travelingalone = 0\n",
    "        TravelingAlone.append(0)\n",
    "        \n",
    "    value+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Append the new feature to Z, and preview\n",
    "Z['TravelingAlone'] = TravelingAlone\n",
    "Z.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will use **_\"one hot encoding\"_** to convert the categoricals Sex, Pclass, and Embarked to numerical indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generator binary data table for Sex, dropping male.\n",
    "# Now, If: Sex = 1 then 'female' ; If: Sex = 0 then 'male' inferred. \n",
    "Z['Sex']=pd.get_dummies(Z.Sex)['female']\n",
    "Z.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Applying the same technique to Pclass, a bit more complex one hot encoding due to 3 categories\n",
    "# First create two new features Pclass1,Pclass3 from get_dummies, then drop Pclass\n",
    "# Now, If: Pclass1 & Pclass3 = 0, Pclass is 2 inferred. \n",
    "\n",
    "Z['Pclass1']=pd.get_dummies(Z.Pclass)[1]\n",
    "Z['Pclass3']=pd.get_dummies(Z.Pclass)[3]\n",
    "Z = Z.drop('Pclass', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking to ensure accuracy\n",
    "Z.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embarked: 3 categories ('S', 'C', and 'Q') therefore applying same tactic as used for Pclass.\n",
    "# Now, If: EmbarkedS & EmbarkedC = 0, Embarked is 'Q' inferred.\n",
    "Z['EmbarkedS']=pd.get_dummies(Z.Embarked)['S']\n",
    "Z['EmbarkedC']=pd.get_dummies(Z.Embarked)['C']\n",
    "Z = Z.drop('Embarked', 1)\n",
    "Z.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I am now ready to move into the optimization phase. To recap, I now have two sets of variables: Z and y. Where Z is the set of all the possible independent features for Logistic Regression, and y is the dependent variable Survived for prediction. All missing fields have been handled approiately, categoricals have all been set to numerical indicators using one hot encoding, and I have a new engineered feature 'TravelingAlone' that I created by combining the features 'SibSp' & 'Parch'. The next step is to optimize the best combination of features for X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parameter Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will generate a list of all possible combinations of features contained in Z. This will allow me to then try each of them as possible sets of X by iteratively scaling, predicting, and scoring for each subset and then selecting the subset of Z which results in the highest AUC score as my X set. I'm not sure whether or not this will prove useful, but I figured I'd give it a try and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "*[Author: Jason Schenck]*\n",
    "-- Manually determining variable importance. The below code is a pair of functions I've defined that             --\n",
    "-- together will crunch out all the possible combinations of features that exist against some defined model      --\n",
    "-- and return only the those with the highest importance in terms of some defined benchmark. This is unnecessary --\n",
    "-- with the implementation of 'model.feature_importances_' from RandomTreesRegressor that does it for us, and    --\n",
    "-- surely better :) \n",
    "\n",
    "\n",
    "# List of all features in Z, which will be candidates for X.\n",
    "possible_X_feats = list([\"Sex\",\"Age\",\"Fare\",\"TravelingAlone\",\"Pclass1\",\"Pclass3\",\"EmbarkedS\",\"EmbarkedC\",\"SibSp0\",\n",
    "                         \"SibSp1\",\"Parch0\",\"Parch1\",\"Parch2\"])\n",
    "\n",
    "# Generate all possible combinations for X\n",
    "possible_X_combos = []\n",
    "\n",
    "for i in range(2, len(possible_X_feats)+1):\n",
    "    possible_X_combos.extend(itertools.combinations(possible_X_feats, i))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "def pickBestX(possible_X_combos):\n",
    "    best_X_AUC = 0\n",
    "    best_X = list()\n",
    "    for combo in possible_X_combos:\n",
    "        \n",
    "        possible_X = scalePossibleX(combo)\n",
    "        \n",
    "        # Split into test/train this possible_X\n",
    "        X_train,X_test,y_train,y_test = train_test_split(possible_X,y,test_size=0.2,random_state=42)\n",
    "        \n",
    "        # Logistic Model -- params kept constant for function\n",
    "        model = LogisticRegression(penalty='l1', C=1)\n",
    "        \n",
    "        # Fit this possible_X to model\n",
    "        model.fit(X_train,y_train)\n",
    "        \n",
    "        # Get AUC\n",
    "        possible_X_AUC = roc_auc_score(y_test,model.predict(X_test))\n",
    "        \n",
    "        \n",
    "        # Check if highest AUC, update the record accordingly\n",
    "\n",
    "        if (possible_X_AUC > best_X_AUC):\n",
    "            best_X_AUC = possible_X_AUC\n",
    "            #best_X.clear()\n",
    "            best_X.append(combo)\n",
    "            best_X.append(best_X_AUC)\n",
    "            \n",
    "        \n",
    "    return best_X_AUC, best_X\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def scalePossibleX(combo):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    possible_X = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    for feature in combo:\n",
    "        possible_X[feature] = Z[feature]\n",
    "    \n",
    "    \n",
    "    # Scale the possible X\n",
    "    possible_X_scaled = scaler.fit_transform(possible_X)\n",
    "    \n",
    "    return possible_X_scaled\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find the best subset of Z for X which yields the highest AUC score\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Best Features and AUC Scores: \\n\")\n",
    "best_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Logistic Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define X\n",
    "X = pd.DataFrame()\n",
    "X['Sex'] = Z['Sex']\n",
    "X['Age'] = Z['Age']\n",
    "X['Pclass1'] = Z['Pclass1']\n",
    "X['Pclass3'] = Z['Pclass3']\n",
    "X['SibSp0'] = Z['SibSp0']\n",
    "X['SibSp1'] = Z['SibSp1']\n",
    "X['Parch2'] = Z['Parch2']\n",
    "\n",
    "# Scale X\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "# Logistic Model -- params tuned from sklearn docs\n",
    "model = LogisticRegression(penalty='l2', C=1, solver='newton-cg',fit_intercept=True,max_iter=100)\n",
    "        \n",
    "# Fit this possible_X to model\n",
    "model.fit(X_train,y_train)\n",
    "        \n",
    "# Test and print the AUC Score\n",
    "X_AUC = roc_auc_score(y_test,model.predict(X_test))\n",
    "print(\"Best Logistic AUC = %2.2f\" %X_AUC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Results: Model Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic Accuracy Score\n",
    "print(\"Logistic accuracy is %2.2f\" % accuracy_score(y_test,model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic AUC Score & Classification Report\n",
    "print(\"Logistic AUC = %2.2f\" % X_AUC)\n",
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % X_AUC)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
